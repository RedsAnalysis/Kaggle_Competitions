{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78073fe9-bda5-4ec3-9cf6-fcad7b7a083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Core Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "# --- Preprocessing & Feature Engineering ---\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# --- Modeling & Evaluation ---\n",
    "import lightgbm as lgb\n",
    "import optuna  # Import Optuna for hyperparameter tuning\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# --- Notebook Settings ---\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('seaborn-v0_8-deep')\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0566f9b-1e66-4101-98c6-94796f62d79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (18524, 9)\n",
      "Testing Data Shape: (6175, 8)\n"
     ]
    }
   ],
   "source": [
    "# --- Load the datasets ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File not found: {e}. Please ensure all CSV files are in the correct directory.\")\n",
    "    train_df, test_df = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# --- Initial Data Inspection ---\n",
    "if not train_df.empty:\n",
    "    print(\"Training Data Shape:\", train_df.shape)\n",
    "    print(\"Testing Data Shape:\", test_df.shape)\n",
    "    y = train_df['Personality'] # Define target variable early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6718b1-2a0d-434e-aada-36c3334da535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic feature engineering complete.\n",
      "Creating advanced features...\n",
      "Advanced feature engineering complete.\n",
      "\n",
      "Applying KNN Imputer and Scaling...\n",
      "\n",
      "Preprocessing complete.\n",
      "Final Processed Training Data Shape: (18524, 31)\n"
     ]
    }
   ],
   "source": [
    "if not train_df.empty:\n",
    "    # --- Combine for consistent processing ---\n",
    "    combined_df = pd.concat([train_df.drop('Personality', axis=1), test_df], ignore_index=True)\n",
    "\n",
    "    # --- 1. Basic Feature Engineering (from your notebook) ---\n",
    "    binary_cols = ['Stage_fear', 'Drained_after_socializing']\n",
    "    for col in binary_cols:\n",
    "        combined_df[col] = combined_df[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "    epsilon = 1e-6\n",
    "    combined_df['Social_Energy_Score'] = (combined_df['Social_event_attendance'] + combined_df['Going_outside']) / (combined_df['Time_spent_Alone'] + epsilon)\n",
    "    combined_df['Online_Offline_Ratio'] = combined_df['Post_frequency'] / (combined_df['Social_event_attendance'] + epsilon)\n",
    "    combined_df['Friends_to_Alone_Ratio'] = combined_df['Friends_circle_size'] / (combined_df['Time_spent_Alone'] + epsilon)\n",
    "    for col in ['Time_spent_Alone', 'Social_event_attendance']:\n",
    "        combined_df[f'{col}_sq'] = combined_df[col]**2\n",
    "        \n",
    "    print(\"Basic feature engineering complete.\")\n",
    "\n",
    "    # --- 2. Advanced Feature Engineering (NEW) ---\n",
    "    print(\"Creating advanced features...\")\n",
    "    \n",
    "    # Interaction between key behavioral traits\n",
    "    combined_df['Social_Engagement_Index'] = combined_df['Social_event_attendance'] * combined_df['Friends_circle_size']\n",
    "    combined_df['Alone_vs_Social_Time_Diff'] = combined_df['Social_event_attendance'] - combined_df['Time_spent_Alone']\n",
    "\n",
    "    # Binning a feature to capture non-linear effects\n",
    "    # We can group 'Friends_circle_size' into categories\n",
    "    combined_df['Friends_group_size'] = pd.cut(combined_df['Friends_circle_size'], \n",
    "                                               bins=[-1, 5, 10, 15, 20], \n",
    "                                               labels=[0, 1, 2, 3])\n",
    "\n",
    "    # --- 3. Missing Value Flags (from your notebook) ---\n",
    "    for col in combined_df.columns:\n",
    "        if combined_df[col].isnull().any():\n",
    "            combined_df[f'{col}_missing_flag'] = combined_df[col].isnull().astype(int)\n",
    "\n",
    "    print(\"Advanced feature engineering complete.\")\n",
    "    \n",
    "    # --- 4. Preprocessing (Imputation, Scaling, Target Encoding) ---\n",
    "    print(\"\\nApplying KNN Imputer and Scaling...\")\n",
    "    \n",
    "    # Fill NaN in new categorical feature before imputation\n",
    "    combined_df['Friends_group_size'] = combined_df['Friends_group_size'].cat.add_categories([-1]).fillna(-1)\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=7)\n",
    "    imputed_data = imputer.fit_transform(combined_df)\n",
    "    combined_df_imputed = pd.DataFrame(imputed_data, columns=combined_df.columns)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(combined_df_imputed)\n",
    "    combined_df_processed = pd.DataFrame(scaled_data, columns=combined_df_imputed.columns)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    X_processed = combined_df_processed.iloc[:len(train_df)]\n",
    "    X_test_processed = combined_df_processed.iloc[len(train_df):]\n",
    "\n",
    "    print(\"\\nPreprocessing complete.\")\n",
    "    print(\"Final Processed Training Data Shape:\", X_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7498c88a-0fee-40ce-8eab-cba72dc3c5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating predictions from the SIMPLE model (v2) ---\n",
      "Simple Model - Fold 1\n",
      "[LightGBM] [Info] Number of positive: 4342, number of negative: 12329\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2224\n",
      "[LightGBM] [Info] Number of data points in the train set: 16671, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260452 -> initscore=-1.043619\n",
      "[LightGBM] [Info] Start training from score -1.043619\n",
      "Simple Model - Fold 2\n",
      "[LightGBM] [Info] Number of positive: 4342, number of negative: 12329\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2219\n",
      "[LightGBM] [Info] Number of data points in the train set: 16671, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260452 -> initscore=-1.043619\n",
      "[LightGBM] [Info] Start training from score -1.043619\n",
      "Simple Model - Fold 3\n",
      "[LightGBM] [Info] Number of positive: 4342, number of negative: 12329\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2220\n",
      "[LightGBM] [Info] Number of data points in the train set: 16671, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260452 -> initscore=-1.043619\n",
      "[LightGBM] [Info] Start training from score -1.043619\n",
      "Simple Model - Fold 4\n",
      "[LightGBM] [Info] Number of positive: 4342, number of negative: 12329\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2219\n",
      "[LightGBM] [Info] Number of data points in the train set: 16671, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260452 -> initscore=-1.043619\n",
      "[LightGBM] [Info] Start training from score -1.043619\n",
      "Simple Model - Fold 5\n",
      "[LightGBM] [Info] Number of positive: 4343, number of negative: 12329\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2221\n",
      "[LightGBM] [Info] Number of data points in the train set: 16672, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260497 -> initscore=-1.043389\n",
      "[LightGBM] [Info] Start training from score -1.043389\n",
      "Simple Model - Fold 6\n",
      "[LightGBM] [Info] Number of positive: 4343, number of negative: 12329\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2214\n",
      "[LightGBM] [Info] Number of data points in the train set: 16672, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260497 -> initscore=-1.043389\n",
      "[LightGBM] [Info] Start training from score -1.043389\n",
      "Simple Model - Fold 7\n",
      "[LightGBM] [Info] Number of positive: 4343, number of negative: 12329\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2223\n",
      "[LightGBM] [Info] Number of data points in the train set: 16672, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260497 -> initscore=-1.043389\n",
      "[LightGBM] [Info] Start training from score -1.043389\n",
      "Simple Model - Fold 8\n",
      "[LightGBM] [Info] Number of positive: 4343, number of negative: 12329\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2220\n",
      "[LightGBM] [Info] Number of data points in the train set: 16672, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260497 -> initscore=-1.043389\n",
      "[LightGBM] [Info] Start training from score -1.043389\n",
      "Simple Model - Fold 9\n",
      "[LightGBM] [Info] Number of positive: 4343, number of negative: 12329\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2217\n",
      "[LightGBM] [Info] Number of data points in the train set: 16672, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260497 -> initscore=-1.043389\n",
      "[LightGBM] [Info] Start training from score -1.043389\n",
      "Simple Model - Fold 10\n",
      "[LightGBM] [Info] Number of positive: 4342, number of negative: 12330\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2218\n",
      "[LightGBM] [Info] Number of data points in the train set: 16672, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.260437 -> initscore=-1.043700\n",
      "[LightGBM] [Info] Start training from score -1.043700\n",
      "Predictions from simple model generated.\n"
     ]
    }
   ],
   "source": [
    "# --- All imports and data loading/preprocessing from your v2 notebook go here ---\n",
    "# (Assume X_processed, y_encoded, X_test_processed, and le are available)\n",
    "\n",
    "print(\"--- Generating predictions from the SIMPLE model (v2) ---\")\n",
    "\n",
    "# Model Configuration from your v2\n",
    "N_SPLITS = 10\n",
    "RANDOM_STATE = 42\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# We need to store the raw probability predictions\n",
    "simple_model_test_preds = np.zeros((len(X_test_processed),))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_processed, y_encoded)):\n",
    "    print(f\"Simple Model - Fold {fold+1}\")\n",
    "    X_train, y_train = X_processed.iloc[train_idx], y_encoded[train_idx]\n",
    "    X_val, y_val = X_processed.iloc[val_idx], y_encoded[val_idx]\n",
    "    \n",
    "    # Use the simple, default model\n",
    "    model = lgb.LGBMClassifier(random_state=RANDOM_STATE, n_estimators=1000, objective='binary')\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              eval_metric='accuracy',\n",
    "              callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    \n",
    "    # Get the probability of the positive class (e.g., 'Extrovert')\n",
    "    fold_test_preds = model.predict_proba(X_test_processed)[:, 1]\n",
    "    simple_model_test_preds += fold_test_preds / N_SPLITS\n",
    "\n",
    "print(\"Predictions from simple model generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5262f16-80a3-4296-bbd4-fd79fdb4967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-04 18:35:57,864] A new study created in memory with name: no-name-644e804e-f212-4d96-a055-4ec517f22172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting SAFER Optuna Optimization ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98801510f36437194a319e17f1929ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-04 18:35:58,448] Trial 0 finished with value: 0.9689592897345541 and parameters: {'learning_rate': 0.07257293206649229, 'num_leaves': 34, 'max_depth': 6, 'subsample': 0.8265339369170482, 'colsample_bytree': 0.7465163789156342, 'reg_alpha': 0.2959617089121309, 'reg_lambda': 0.9286362546400216, 'min_child_samples': 27}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:35:59,164] Trial 1 finished with value: 0.968635403094878 and parameters: {'learning_rate': 0.07584304546783488, 'num_leaves': 43, 'max_depth': 8, 'subsample': 0.7108152496680548, 'colsample_bytree': 0.9462653822435309, 'reg_alpha': 0.9328779406952297, 'reg_lambda': 0.4815295638788598, 'min_child_samples': 21}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:35:59,740] Trial 2 finished with value: 0.9688513275213287 and parameters: {'learning_rate': 0.05352092293308994, 'num_leaves': 21, 'max_depth': 9, 'subsample': 0.8341649387620014, 'colsample_bytree': 0.814355228035436, 'reg_alpha': 0.8536926539626496, 'reg_lambda': 0.9687228251244218, 'min_child_samples': 50}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:00,507] Trial 3 finished with value: 0.9687973318409832 and parameters: {'learning_rate': 0.05442519448993365, 'num_leaves': 41, 'max_depth': 9, 'subsample': 0.7310962275035678, 'colsample_bytree': 0.7101212299175084, 'reg_alpha': 0.9788162320053775, 'reg_lambda': 0.12332713454114258, 'min_child_samples': 22}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:01,858] Trial 4 finished with value: 0.9688513275213287 and parameters: {'learning_rate': 0.020078243289127273, 'num_leaves': 53, 'max_depth': 7, 'subsample': 0.9573896356074878, 'colsample_bytree': 0.8127624440340436, 'reg_alpha': 0.49086147377425615, 'reg_lambda': 0.35889389946459416, 'min_child_samples': 13}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:02,885] Trial 5 finished with value: 0.9685814074145324 and parameters: {'learning_rate': 0.024906895582337583, 'num_leaves': 59, 'max_depth': 7, 'subsample': 0.8282067485174933, 'colsample_bytree': 0.7702982842879157, 'reg_alpha': 0.5518465062514527, 'reg_lambda': 0.4991952232255842, 'min_child_samples': 38}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:03,844] Trial 6 finished with value: 0.9687973318409832 and parameters: {'learning_rate': 0.030545277761085107, 'num_leaves': 43, 'max_depth': 9, 'subsample': 0.8595694690705038, 'colsample_bytree': 0.8082301819070381, 'reg_alpha': 0.7308683479471634, 'reg_lambda': 0.9050391956769973, 'min_child_samples': 47}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:04,652] Trial 7 finished with value: 0.9687973318409832 and parameters: {'learning_rate': 0.052933365218956925, 'num_leaves': 50, 'max_depth': 8, 'subsample': 0.7266419208443122, 'colsample_bytree': 0.7986159345258483, 'reg_alpha': 0.7811860786780442, 'reg_lambda': 0.20348578165569517, 'min_child_samples': 26}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:05,166] Trial 8 finished with value: 0.9687433507343703 and parameters: {'learning_rate': 0.055388297246681456, 'num_leaves': 29, 'max_depth': 5, 'subsample': 0.7882030450261786, 'colsample_bytree': 0.958341851198937, 'reg_alpha': 0.4788204367219352, 'reg_lambda': 0.4763879582183217, 'min_child_samples': 49}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:06,166] Trial 9 finished with value: 0.9687433653081033 and parameters: {'learning_rate': 0.019184696466954206, 'num_leaves': 20, 'max_depth': 7, 'subsample': 0.8448034421511027, 'colsample_bytree': 0.9431987900808242, 'reg_alpha': 0.5609167784600063, 'reg_lambda': 0.8578857395446492, 'min_child_samples': 28}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:06,625] Trial 10 finished with value: 0.9689053086279413 and parameters: {'learning_rate': 0.07804490814070468, 'num_leaves': 32, 'max_depth': 5, 'subsample': 0.9250605017013684, 'colsample_bytree': 0.7201342917847414, 'reg_alpha': 0.1150719702267107, 'reg_lambda': 0.7409019420430883, 'min_child_samples': 36}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:07,072] Trial 11 finished with value: 0.9689592897345541 and parameters: {'learning_rate': 0.07956482533234788, 'num_leaves': 32, 'max_depth': 5, 'subsample': 0.9334286774108679, 'colsample_bytree': 0.7045424268875335, 'reg_alpha': 0.1019040167240342, 'reg_lambda': 0.7629874989120623, 'min_child_samples': 37}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:07,613] Trial 12 finished with value: 0.9686893842014905 and parameters: {'learning_rate': 0.06987395086822852, 'num_leaves': 32, 'max_depth': 6, 'subsample': 0.9978665591674272, 'colsample_bytree': 0.8885010138662598, 'reg_alpha': 0.12079311900183452, 'reg_lambda': 0.7108786117167825, 'min_child_samples': 36}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:08,149] Trial 13 finished with value: 0.968851298373863 and parameters: {'learning_rate': 0.06627364899119903, 'num_leaves': 27, 'max_depth': 6, 'subsample': 0.8968787202008274, 'colsample_bytree': 0.7477918460559119, 'reg_alpha': 0.2807242235307874, 'reg_lambda': 0.7113026527176082, 'min_child_samples': 41}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:08,867] Trial 14 finished with value: 0.9688513566687945 and parameters: {'learning_rate': 0.039783335142547824, 'num_leaves': 37, 'max_depth': 6, 'subsample': 0.780491623552261, 'colsample_bytree': 0.7001042171819042, 'reg_alpha': 0.33405070649631696, 'reg_lambda': 0.8159334108595506, 'min_child_samples': 32}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:09,369] Trial 15 finished with value: 0.9687973464147162 and parameters: {'learning_rate': 0.0669770000425935, 'num_leaves': 35, 'max_depth': 5, 'subsample': 0.8973630294945624, 'colsample_bytree': 0.8668770324219134, 'reg_alpha': 0.2703239716675087, 'reg_lambda': 0.6222655550908813, 'min_child_samples': 14}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:09,865] Trial 16 finished with value: 0.9687973318409832 and parameters: {'learning_rate': 0.07999178952640512, 'num_leaves': 26, 'max_depth': 6, 'subsample': 0.9632059070902482, 'colsample_bytree': 0.7522514507292476, 'reg_alpha': 0.19203105377512483, 'reg_lambda': 0.9758080017468567, 'min_child_samples': 42}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:10,715] Trial 17 finished with value: 0.9688513420950615 and parameters: {'learning_rate': 0.06281618746736514, 'num_leaves': 48, 'max_depth': 10, 'subsample': 0.7741112006954627, 'colsample_bytree': 0.999223816529435, 'reg_alpha': 0.38290838062692156, 'reg_lambda': 0.78659718398818, 'min_child_samples': 32}. Best is trial 0 with value: 0.9689592897345541.\n",
      "[I 2025-07-04 18:36:11,319] Trial 18 finished with value: 0.9690672519477793 and parameters: {'learning_rate': 0.042079317380294703, 'num_leaves': 37, 'max_depth': 5, 'subsample': 0.8760763588666587, 'colsample_bytree': 0.738202603754513, 'reg_alpha': 0.2043258238466737, 'reg_lambda': 0.6027323551371301, 'min_child_samples': 19}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:12,052] Trial 19 finished with value: 0.9685814074145324 and parameters: {'learning_rate': 0.03998212645509507, 'num_leaves': 37, 'max_depth': 6, 'subsample': 0.8807477537070357, 'colsample_bytree': 0.7647002722542985, 'reg_alpha': 0.42158718677444196, 'reg_lambda': 0.6098433988044049, 'min_child_samples': 18}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:13,518] Trial 20 finished with value: 0.9689593043082869 and parameters: {'learning_rate': 0.010907991220551472, 'num_leaves': 46, 'max_depth': 5, 'subsample': 0.8117652608551656, 'colsample_bytree': 0.8596102960760824, 'reg_alpha': 0.22424188052894706, 'reg_lambda': 0.3792937532796603, 'min_child_samples': 10}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:14,095] Trial 21 finished with value: 0.9690132854148995 and parameters: {'learning_rate': 0.011518939110229845, 'num_leaves': 45, 'max_depth': 5, 'subsample': 0.8122048959028504, 'colsample_bytree': 0.8572731788586295, 'reg_alpha': 0.21778547741337673, 'reg_lambda': 0.33648452864443357, 'min_child_samples': 10}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:15,496] Trial 22 finished with value: 0.9690132854148995 and parameters: {'learning_rate': 0.011879767737262636, 'num_leaves': 46, 'max_depth': 5, 'subsample': 0.7889165851976591, 'colsample_bytree': 0.898287898736772, 'reg_alpha': 0.22136732886103974, 'reg_lambda': 0.33873957073914956, 'min_child_samples': 10}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:16,995] Trial 23 finished with value: 0.9689593043082869 and parameters: {'learning_rate': 0.01092381425141003, 'num_leaves': 55, 'max_depth': 5, 'subsample': 0.7627984657232745, 'colsample_bytree': 0.9021382737655153, 'reg_alpha': 0.19160559000916838, 'reg_lambda': 0.3074024812270714, 'min_child_samples': 10}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:17,712] Trial 24 finished with value: 0.9689053086279413 and parameters: {'learning_rate': 0.032394059103002956, 'num_leaves': 46, 'max_depth': 5, 'subsample': 0.8105223464204393, 'colsample_bytree': 0.8402194077229169, 'reg_alpha': 0.640271331289651, 'reg_lambda': 0.2460768249685792, 'min_child_samples': 16}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:18,702] Trial 25 finished with value: 0.9688513275213287 and parameters: {'learning_rate': 0.01709676795672792, 'num_leaves': 40, 'max_depth': 5, 'subsample': 0.7571097569184736, 'colsample_bytree': 0.9101631062986306, 'reg_alpha': 0.3717983932454795, 'reg_lambda': 0.3806674382134517, 'min_child_samples': 20}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:19,667] Trial 26 finished with value: 0.9687973464147162 and parameters: {'learning_rate': 0.027260069321995976, 'num_leaves': 51, 'max_depth': 6, 'subsample': 0.8652387441376329, 'colsample_bytree': 0.8423786373832832, 'reg_alpha': 0.19871730498318105, 'reg_lambda': 0.587702495550724, 'min_child_samples': 13}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:20,563] Trial 27 finished with value: 0.9689053086279413 and parameters: {'learning_rate': 0.03779471799264432, 'num_leaves': 44, 'max_depth': 7, 'subsample': 0.8003092325785064, 'colsample_bytree': 0.9164475941007554, 'reg_alpha': 0.43566687105987373, 'reg_lambda': 0.41699709005820557, 'min_child_samples': 24}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:21,135] Trial 28 finished with value: 0.968851312947596 and parameters: {'learning_rate': 0.04649928574808233, 'num_leaves': 38, 'max_depth': 5, 'subsample': 0.8789495125571134, 'colsample_bytree': 0.8787222533736974, 'reg_alpha': 0.2538760650211016, 'reg_lambda': 0.25497955080312085, 'min_child_samples': 18}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:22,593] Trial 29 finished with value: 0.9686893696277578 and parameters: {'learning_rate': 0.015959033135616058, 'num_leaves': 57, 'max_depth': 6, 'subsample': 0.7449587000498222, 'colsample_bytree': 0.7817569963123823, 'reg_alpha': 0.35083751741221286, 'reg_lambda': 0.15507043327940512, 'min_child_samples': 10}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:23,634] Trial 30 finished with value: 0.9687973464147162 and parameters: {'learning_rate': 0.02386314446074081, 'num_leaves': 48, 'max_depth': 6, 'subsample': 0.8153801218789242, 'colsample_bytree': 0.8274290800329792, 'reg_alpha': 0.3141754886964623, 'reg_lambda': 0.6604727840686794, 'min_child_samples': 15}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:25,135] Trial 31 finished with value: 0.9690132854148995 and parameters: {'learning_rate': 0.010557604238777536, 'num_leaves': 45, 'max_depth': 5, 'subsample': 0.8016905750337221, 'colsample_bytree': 0.8616049127971908, 'reg_alpha': 0.22167276368309882, 'reg_lambda': 0.3050482770746369, 'min_child_samples': 10}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:26,409] Trial 32 finished with value: 0.9689053086279416 and parameters: {'learning_rate': 0.012758781833267768, 'num_leaves': 41, 'max_depth': 5, 'subsample': 0.7923117131298162, 'colsample_bytree': 0.8725625913183184, 'reg_alpha': 0.14437936204793533, 'reg_lambda': 0.5249691371561014, 'min_child_samples': 12}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:27,546] Trial 33 finished with value: 0.9688513275213287 and parameters: {'learning_rate': 0.015072807615044388, 'num_leaves': 46, 'max_depth': 5, 'subsample': 0.8420661786285498, 'colsample_bytree': 0.9303408860339943, 'reg_alpha': 0.17304691799737287, 'reg_lambda': 0.3034041210368567, 'min_child_samples': 17}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:28,772] Trial 34 finished with value: 0.968635403094878 and parameters: {'learning_rate': 0.021612153954634478, 'num_leaves': 40, 'max_depth': 8, 'subsample': 0.7077597235232649, 'colsample_bytree': 0.850028365817174, 'reg_alpha': 0.2440171176445735, 'reg_lambda': 0.45072076247623016, 'min_child_samples': 20}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:29,658] Trial 35 finished with value: 0.9689592897345541 and parameters: {'learning_rate': 0.03335135827348039, 'num_leaves': 44, 'max_depth': 6, 'subsample': 0.832049858996097, 'colsample_bytree': 0.8936890775988867, 'reg_alpha': 0.3005117134148824, 'reg_lambda': 0.3173916374852851, 'min_child_samples': 12}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:30,886] Trial 36 finished with value: 0.9688513275213287 and parameters: {'learning_rate': 0.027525846156535947, 'num_leaves': 51, 'max_depth': 10, 'subsample': 0.8211442708329852, 'colsample_bytree': 0.8278607233519485, 'reg_alpha': 0.15983569962036842, 'reg_lambda': 0.5524967747409555, 'min_child_samples': 22}. Best is trial 18 with value: 0.9690672519477793.\n",
      "[I 2025-07-04 18:36:32,079] Trial 37 finished with value: 0.9691212184806594 and parameters: {'learning_rate': 0.014204893193943989, 'num_leaves': 43, 'max_depth': 5, 'subsample': 0.747325210998792, 'colsample_bytree': 0.7896239912618939, 'reg_alpha': 0.2315125114419452, 'reg_lambda': 0.18086093684946056, 'min_child_samples': 15}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:32,856] Trial 38 finished with value: 0.9687973464147159 and parameters: {'learning_rate': 0.04673195801313962, 'num_leaves': 42, 'max_depth': 7, 'subsample': 0.7585048250326677, 'colsample_bytree': 0.7295677184272484, 'reg_alpha': 0.6385927837359161, 'reg_lambda': 0.13076712986637778, 'min_child_samples': 15}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:33,724] Trial 39 finished with value: 0.9689052940542083 and parameters: {'learning_rate': 0.021615005829310148, 'num_leaves': 39, 'max_depth': 5, 'subsample': 0.7329800036536271, 'colsample_bytree': 0.7890509211143291, 'reg_alpha': 0.9206817892564292, 'reg_lambda': 0.20303340175007334, 'min_child_samples': 18}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:35,279] Trial 40 finished with value: 0.9689052940542083 and parameters: {'learning_rate': 0.014732623622661185, 'num_leaves': 49, 'max_depth': 7, 'subsample': 0.8580287625233995, 'colsample_bytree': 0.7324619603011048, 'reg_alpha': 0.42026480763236146, 'reg_lambda': 0.19076653912326116, 'min_child_samples': 24}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:36,265] Trial 41 finished with value: 0.9689592897345539 and parameters: {'learning_rate': 0.01822955589263223, 'num_leaves': 53, 'max_depth': 5, 'subsample': 0.7691009247058627, 'colsample_bytree': 0.8279704569584295, 'reg_alpha': 0.22278883200610697, 'reg_lambda': 0.26082327859029014, 'min_child_samples': 12}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:37,763] Trial 42 finished with value: 0.9688513275213287 and parameters: {'learning_rate': 0.010196100091494152, 'num_leaves': 44, 'max_depth': 5, 'subsample': 0.7950376784908331, 'colsample_bytree': 0.8099777206096157, 'reg_alpha': 0.3063740998738595, 'reg_lambda': 0.42495395889798926, 'min_child_samples': 10}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:38,723] Trial 43 finished with value: 0.9688513275213287 and parameters: {'learning_rate': 0.020027114855481663, 'num_leaves': 46, 'max_depth': 5, 'subsample': 0.7390912212144356, 'colsample_bytree': 0.8569805189051102, 'reg_alpha': 0.24257931487559276, 'reg_lambda': 0.31249097226293326, 'min_child_samples': 13}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:40,335] Trial 44 finished with value: 0.9688512983738627 and parameters: {'learning_rate': 0.013711473783787263, 'num_leaves': 35, 'max_depth': 9, 'subsample': 0.718946053391376, 'colsample_bytree': 0.8798820909535328, 'reg_alpha': 0.49264415805122297, 'reg_lambda': 0.1021079333688392, 'min_child_samples': 15}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:40,867] Trial 45 finished with value: 0.9688513275213287 and parameters: {'learning_rate': 0.057139175346786704, 'num_leaves': 42, 'max_depth': 5, 'subsample': 0.8042558957431453, 'colsample_bytree': 0.7655911331122474, 'reg_alpha': 0.14036525826293106, 'reg_lambda': 0.3464905455671928, 'min_child_samples': 12}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:41,914] Trial 46 finished with value: 0.9688513129475957 and parameters: {'learning_rate': 0.023295579501919253, 'num_leaves': 53, 'max_depth': 6, 'subsample': 0.782761686707786, 'colsample_bytree': 0.9273325996485137, 'reg_alpha': 0.27626084745342194, 'reg_lambda': 0.49219231416399933, 'min_child_samples': 16}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:42,680] Trial 47 finished with value: 0.9688513275213287 and parameters: {'learning_rate': 0.026720599537673376, 'num_leaves': 43, 'max_depth': 5, 'subsample': 0.7498206064760302, 'colsample_bytree': 0.7946610117197427, 'reg_alpha': 0.1028235971120736, 'reg_lambda': 0.16207039855402477, 'min_child_samples': 14}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:43,401] Trial 48 finished with value: 0.9689053086279413 and parameters: {'learning_rate': 0.04991137454113083, 'num_leaves': 48, 'max_depth': 6, 'subsample': 0.8305599966396549, 'colsample_bytree': 0.9643175353254149, 'reg_alpha': 0.21282077673973718, 'reg_lambda': 0.22571582176619145, 'min_child_samples': 11}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:44,334] Trial 49 finished with value: 0.9688513129475957 and parameters: {'learning_rate': 0.01887560982881727, 'num_leaves': 60, 'max_depth': 5, 'subsample': 0.8429694896527737, 'colsample_bytree': 0.7196892051378659, 'reg_alpha': 0.33863904928171595, 'reg_lambda': 0.4106911068147441, 'min_child_samples': 20}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:45,187] Trial 50 finished with value: 0.9689053086279413 and parameters: {'learning_rate': 0.013310331972695908, 'num_leaves': 37, 'max_depth': 8, 'subsample': 0.7782516161698627, 'colsample_bytree': 0.8895213890302867, 'reg_alpha': 0.14245195561610186, 'reg_lambda': 0.27329604232178273, 'min_child_samples': 27}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:46,714] Trial 51 finished with value: 0.9689592897345541 and parameters: {'learning_rate': 0.01020129507327327, 'num_leaves': 46, 'max_depth': 5, 'subsample': 0.8166805348314643, 'colsample_bytree': 0.8602815162206618, 'reg_alpha': 0.2299323567036064, 'reg_lambda': 0.36359646255234473, 'min_child_samples': 10}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:48,062] Trial 52 finished with value: 0.9688513129475957 and parameters: {'learning_rate': 0.01203483227902201, 'num_leaves': 46, 'max_depth': 5, 'subsample': 0.9097840052458275, 'colsample_bytree': 0.838616701332472, 'reg_alpha': 0.17798142865215158, 'reg_lambda': 0.3891004607808136, 'min_child_samples': 14}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:49,119] Trial 53 finished with value: 0.9687973318409832 and parameters: {'learning_rate': 0.016274258532020868, 'num_leaves': 50, 'max_depth': 5, 'subsample': 0.8053050491968455, 'colsample_bytree': 0.8655615724558999, 'reg_alpha': 0.2687636382369343, 'reg_lambda': 0.47259407217834254, 'min_child_samples': 11}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:49,736] Trial 54 finished with value: 0.9687433653081033 and parameters: {'learning_rate': 0.07527882955182971, 'num_leaves': 45, 'max_depth': 6, 'subsample': 0.8706900362120802, 'colsample_bytree': 0.8229401500581939, 'reg_alpha': 0.2121562885091269, 'reg_lambda': 0.3503342443681247, 'min_child_samples': 10}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:50,760] Trial 55 finished with value: 0.9690132708411667 and parameters: {'learning_rate': 0.01694884713716982, 'num_leaves': 42, 'max_depth': 5, 'subsample': 0.825380844682573, 'colsample_bytree': 0.8492834418589646, 'reg_alpha': 0.29406835384315416, 'reg_lambda': 0.6638418934176082, 'min_child_samples': 13}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:51,903] Trial 56 finished with value: 0.9685274263079198 and parameters: {'learning_rate': 0.018027142845383357, 'num_leaves': 34, 'max_depth': 6, 'subsample': 0.856380863027582, 'colsample_bytree': 0.7782438110238705, 'reg_alpha': 0.39330817066534557, 'reg_lambda': 0.6603614048194837, 'min_child_samples': 17}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:52,586] Trial 57 finished with value: 0.9689593043082869 and parameters: {'learning_rate': 0.030267574348828284, 'num_leaves': 30, 'max_depth': 5, 'subsample': 0.84760076751511, 'colsample_bytree': 0.8029740789231857, 'reg_alpha': 0.2941284588570144, 'reg_lambda': 0.5732739382232155, 'min_child_samples': 30}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:53,251] Trial 58 finished with value: 0.9687973464147159 and parameters: {'learning_rate': 0.035767310538348515, 'num_leaves': 42, 'max_depth': 5, 'subsample': 0.8256814239499111, 'colsample_bytree': 0.9033366026976005, 'reg_alpha': 0.33038437452432956, 'reg_lambda': 0.6634056701600056, 'min_child_samples': 13}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:53,902] Trial 59 finished with value: 0.9689053086279416 and parameters: {'learning_rate': 0.06022344691574289, 'num_leaves': 39, 'max_depth': 6, 'subsample': 0.8920627247755376, 'colsample_bytree': 0.7408317902998887, 'reg_alpha': 0.1751924895376501, 'reg_lambda': 0.9162800704376379, 'min_child_samples': 16}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:54,765] Trial 60 finished with value: 0.9687973464147159 and parameters: {'learning_rate': 0.021284167376071906, 'num_leaves': 48, 'max_depth': 5, 'subsample': 0.789532442809909, 'colsample_bytree': 0.8170992110301458, 'reg_alpha': 0.7549627630307071, 'reg_lambda': 0.850996550761306, 'min_child_samples': 19}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:56,069] Trial 61 finished with value: 0.9689592897345539 and parameters: {'learning_rate': 0.012254956981770595, 'num_leaves': 41, 'max_depth': 5, 'subsample': 0.8086882294287686, 'colsample_bytree': 0.8468033649245916, 'reg_alpha': 0.20159528266595728, 'reg_lambda': 0.5238911838085144, 'min_child_samples': 11}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:57,172] Trial 62 finished with value: 0.9689053086279416 and parameters: {'learning_rate': 0.015077610835049809, 'num_leaves': 23, 'max_depth': 5, 'subsample': 0.7714551407658504, 'colsample_bytree': 0.878828703997337, 'reg_alpha': 0.25842754229982146, 'reg_lambda': 0.28327650722146286, 'min_child_samples': 14}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:36:58,524] Trial 63 finished with value: 0.9689053086279413 and parameters: {'learning_rate': 0.012216250426267566, 'num_leaves': 44, 'max_depth': 5, 'subsample': 0.8367238613553281, 'colsample_bytree': 0.8548227620512638, 'reg_alpha': 0.1287934038795036, 'reg_lambda': 0.7094963400390114, 'min_child_samples': 12}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:37:00,032] Trial 64 finished with value: 0.9689592751608211 and parameters: {'learning_rate': 0.010081429385304824, 'num_leaves': 47, 'max_depth': 5, 'subsample': 0.7958339506425264, 'colsample_bytree': 0.7530466193069941, 'reg_alpha': 0.23257698112491568, 'reg_lambda': 0.6309635082973667, 'min_child_samples': 13}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:37:01,060] Trial 65 finished with value: 0.9689053086279416 and parameters: {'learning_rate': 0.0174014277144688, 'num_leaves': 38, 'max_depth': 5, 'subsample': 0.8227465514803541, 'colsample_bytree': 0.8377772752818246, 'reg_alpha': 0.3611646416675481, 'reg_lambda': 0.1760581992080452, 'min_child_samples': 10}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:37:01,618] Trial 66 finished with value: 0.9689053086279413 and parameters: {'learning_rate': 0.04280207719719569, 'num_leaves': 43, 'max_depth': 5, 'subsample': 0.7849523069889475, 'colsample_bytree': 0.8996480011022809, 'reg_alpha': 0.28223124513503567, 'reg_lambda': 0.23310418001193625, 'min_child_samples': 44}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:37:03,073] Trial 67 finished with value: 0.9689592897345541 and parameters: {'learning_rate': 0.015171064461750044, 'num_leaves': 45, 'max_depth': 6, 'subsample': 0.7205784404799767, 'colsample_bytree': 0.8682445915393223, 'reg_alpha': 0.16746527646078455, 'reg_lambda': 0.33029654011957166, 'min_child_samples': 15}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:37:03,868] Trial 68 finished with value: 0.968635403094878 and parameters: {'learning_rate': 0.025704884369373335, 'num_leaves': 41, 'max_depth': 5, 'subsample': 0.8145435088908692, 'colsample_bytree': 0.9161675822696687, 'reg_alpha': 0.5848843957003446, 'reg_lambda': 0.4404178795111796, 'min_child_samples': 17}. Best is trial 37 with value: 0.9691212184806594.\n",
      "[I 2025-07-04 18:37:05,798] Trial 69 finished with value: 0.9688513420950617 and parameters: {'learning_rate': 0.013835513472438636, 'num_leaves': 50, 'max_depth': 9, 'subsample': 0.8713593915799415, 'colsample_bytree': 0.9466366062171334, 'reg_alpha': 0.19433376433274768, 'reg_lambda': 0.39621665917577165, 'min_child_samples': 11}. Best is trial 37 with value: 0.9691212184806594.\n",
      "Best parameters from safer search: {'learning_rate': 0.014204893193943989, 'num_leaves': 43, 'max_depth': 5, 'subsample': 0.747325210998792, 'colsample_bytree': 0.7896239912618939, 'reg_alpha': 0.2315125114419452, 'reg_lambda': 0.18086093684946056, 'min_child_samples': 15}\n",
      "\n",
      "--- Generating predictions from the TUNED model (v3.1) ---\n",
      "Tuned Model - Fold 1\n",
      "Tuned Model - Fold 2\n",
      "Tuned Model - Fold 3\n",
      "Tuned Model - Fold 4\n",
      "Tuned Model - Fold 5\n",
      "Tuned Model - Fold 6\n",
      "Tuned Model - Fold 7\n",
      "Tuned Model - Fold 8\n",
      "Tuned Model - Fold 9\n",
      "Tuned Model - Fold 10\n",
      "Predictions from tuned model generated.\n"
     ]
    }
   ],
   "source": [
    "# --- Use the same processed data from before ---\n",
    "\n",
    "def objective_safer(trial):\n",
    "    # SAFER search space to prevent overfitting\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.08), # Slightly lower max rate\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 60), # Lower max leaves\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 10),      # Lower max depth\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "        'random_state': 42,\n",
    "        'verbose': -1,\n",
    "    }\n",
    "    # ... (Rest of the objective function is the same, using skf and model.fit)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    for train_idx, val_idx in skf.split(X_processed, y_encoded):\n",
    "        X_train, y_train = X_processed.iloc[train_idx], y_encoded[train_idx]\n",
    "        X_val, y_val = X_processed.iloc[val_idx], y_encoded[val_idx]\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='accuracy', callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "        preds = model.predict(X_val)\n",
    "        accuracies.append(accuracy_score(y_val, preds))\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "print(\"\\n--- Starting SAFER Optuna Optimization ---\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_safer, n_trials=70, show_progress_bar=True) # 30 trials is enough for a safer search\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters from safer search:\", best_params)\n",
    "\n",
    "# --- Train with the safer tuned parameters ---\n",
    "print(\"\\n--- Generating predictions from the TUNED model (v3.1) ---\")\n",
    "final_params = { 'objective': 'binary', 'random_state': RANDOM_STATE, 'n_estimators': 2000, 'verbose': -1, **best_params }\n",
    "tuned_model_test_preds = np.zeros((len(X_test_processed),))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_processed, y_encoded)):\n",
    "    print(f\"Tuned Model - Fold {fold+1}\")\n",
    "    X_train, y_train = X_processed.iloc[train_idx], y_encoded[train_idx]\n",
    "    X_val, y_val = X_processed.iloc[val_idx], y_encoded[val_idx]\n",
    "    model = lgb.LGBMClassifier(**final_params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='accuracy', callbacks=[lgb.early_stopping(150, verbose=False)])\n",
    "    \n",
    "    # Get the probability of the positive class\n",
    "    fold_test_preds = model.predict_proba(X_test_processed)[:, 1]\n",
    "    tuned_model_test_preds += fold_test_preds / N_SPLITS\n",
    "\n",
    "print(\"Predictions from tuned model generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df22a33-7337-4a56-8fbd-2264f79dd1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating the ENSEMBLE submission ---\n",
      "\n",
      "Ensemble submission file created successfully: submission_ensemble_v4.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Personality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18524</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18525</td>\n",
       "      <td>Introvert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18526</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18527</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18528</td>\n",
       "      <td>Introvert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id Personality\n",
       "0  18524   Extrovert\n",
       "1  18525   Introvert\n",
       "2  18526   Extrovert\n",
       "3  18527   Extrovert\n",
       "4  18528   Introvert"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n--- Creating the ENSEMBLE submission ---\")\n",
    "\n",
    "# Simple average of the two models' probability predictions\n",
    "ensemble_preds_proba = (simple_model_test_preds + tuned_model_test_preds) / 2\n",
    "\n",
    "# Convert the averaged probabilities to final class labels (0 or 1)\n",
    "# The threshold of 0.5 is standard for binary classification.\n",
    "ensemble_preds_encoded = (ensemble_preds_proba > 0.5).astype(int)\n",
    "\n",
    "# Inverse transform to get the original string labels\n",
    "final_ensemble_preds = le.inverse_transform(ensemble_preds_encoded)\n",
    "\n",
    "# Create the final submission DataFrame\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'Personality': final_ensemble_preds})\n",
    "submission_df.to_csv('submission_ensemble_v4.csv', index=False)\n",
    "\n",
    "print(\"\\nEnsemble submission file created successfully: submission_ensemble_v4.csv\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceeb06a-1c09-4c3d-80d4-20a6bb3ce122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
