{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2cd126-241b-4367-8e15-44be299acddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing ---\n",
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete. Raw data and target variables are ready.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "# --- Basic Setup ---\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "print(\"--- Initializing ---\")\n",
    "\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# --- Helper Function for Winkler Score ---\n",
    "# This is our primary evaluation metric for the competition.\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30 # Increase for more exhaustive search, 30 is a strong start\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv', index_col=\"id\", parse_dates=[\"sale_date\"])\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv', index_col=\"id\", parse_dates=[\"sale_date\"])\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Target Variable ---\n",
    "# We store the true dollar value for final evaluation\n",
    "y_true = df_train['sale_price'].copy()\n",
    "# We create the log-transformed version for training our models\n",
    "y_log = np.log1p(y_true)\n",
    "\n",
    "# Add log-transformed target to df_train for feature engineering\n",
    "df_train['sale_price_log'] = y_log\n",
    "\n",
    "print(\"Setup complete. Raw data and target variables are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0afb66a0-6a1f-43f8-8c14-e5faaf1682e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 2: Ultimate Feature Engineering ---\n",
      "Creating foundational and contextual features...\n",
      "Creating TF-IDF features for high-cardinality text...\n",
      "Finalizing feature set...\n",
      "Ultimate feature engineering complete. Total features: 75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 2: ULTIMATE FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 2: Ultimate Feature Engineering ---\")\n",
    "\n",
    "# Combine for consistent processing\n",
    "df_train['is_train'] = 1\n",
    "df_test['is_train'] = 0\n",
    "all_data = pd.concat([df_train.drop('sale_price', axis=1), df_test], axis=0)\n",
    "\n",
    "# --- A) Foundational & God-Tier Features ---\n",
    "print(\"Creating foundational and contextual features...\")\n",
    "all_data['sale_year'] = all_data['sale_date'].dt.year\n",
    "all_data['age_at_sale'] = all_data['sale_year'] - all_data['year_built']\n",
    "all_data['total_bathrooms'] = all_data['bath_full'] + 0.5 * all_data['bath_half'] + 0.75 * all_data['bath_3qtr']\n",
    "all_data['total_sqft'] = all_data['sqft'] + all_data['sqft_fbsmt']\n",
    "all_data['was_renovated'] = (all_data['year_reno'] > 0).astype(int)\n",
    "all_data['land_val_per_sqft_lot'] = all_data['land_val'] / (all_data['sqft_lot'] + 1e-6)\n",
    "all_data['lot_utilization'] = all_data['total_sqft'] / (all_data['sqft_lot'] + 1e-6)\n",
    "\n",
    "kmeans = KMeans(n_clusters=40, random_state=RANDOM_STATE, n_init='auto')\n",
    "all_data['location_cluster'] = kmeans.fit_predict(all_data[['latitude', 'longitude']])\n",
    "\n",
    "train_copy_for_aggs = all_data[all_data['is_train'] == 1].copy()\n",
    "group_cols_to_agg = ['location_cluster', 'city', 'submarket']\n",
    "\n",
    "for group_col in group_cols_to_agg:\n",
    "    aggs = {'grade': ['mean', 'std'], 'age_at_sale': ['mean', 'std'], 'sale_price_log': ['mean']}\n",
    "    group_aggs = train_copy_for_aggs.groupby(group_col).agg(aggs)\n",
    "    group_aggs.columns = [f'{c[0]}_{c[1]}_{group_col}' for c in group_aggs.columns]\n",
    "    all_data = all_data.merge(group_aggs, on=group_col, how='left')\n",
    "    all_data[f'grade_vs_mean_{group_col}'] = all_data['grade'] - all_data[f'grade_mean_{group_col}']\n",
    "\n",
    "# --- B) TF-IDF Text Features ---\n",
    "print(\"Creating TF-IDF features for high-cardinality text...\")\n",
    "text_cols = ['subdivision', 'zoning']\n",
    "for col in text_cols:\n",
    "    all_data[col] = all_data[col].fillna('missing').astype(str)\n",
    "    tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 4), max_features=32, binary=True)\n",
    "    tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "    svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "    tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "    tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "    all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "# --- C) Final Cleanup ---\n",
    "print(\"Finalizing feature set...\")\n",
    "cols_to_drop = ['sale_date', 'year_built', 'year_reno', 'bath_full', 'bath_half', 'bath_3qtr', 'sqft', 'sqft_fbsmt', 'latitude', 'longitude', 'subdivision', 'zoning']\n",
    "all_data = all_data.drop(columns=cols_to_drop)\n",
    "\n",
    "# Label Encoding for remaining categoricals\n",
    "for col in all_data.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    all_data[col] = le.fit_transform(all_data[col].astype(str))\n",
    "\n",
    "all_data.fillna(0, inplace=True)\n",
    "\n",
    "# Separate final datasets\n",
    "X = all_data[all_data['is_train'] == 1].drop(columns=['is_train', 'sale_price_log'])\n",
    "X_test = all_data[all_data['is_train'] == 0].drop(columns=['is_train', 'sale_price_log'])\n",
    "X_test = X_test[X.columns]\n",
    "\n",
    "print(f\"Ultimate feature engineering complete. Total features: {X.shape[1]}\")\n",
    "del all_data, train_copy_for_aggs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55209913-e160-43c9-a0a4-df8be74a7210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-05 16:49:05,301] A new study created in memory with name: no-name-cde07028-ea78-404d-8be9-a0ce6331d3b4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 1, PART 1: Tuning Mean Prediction Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-05 16:49:35,768] Trial 0 finished with value: 0.13759402577494928 and parameters: {'eta': 0.024720483553243557, 'max_depth': 8, 'subsample': 0.7602914350498581, 'colsample_bytree': 0.7362487413887749, 'min_child_weight': 1, 'lambda': 0.0001216863746901541, 'alpha': 0.2994095376993054}. Best is trial 0 with value: 0.13759402577494928.\n",
      "[I 2025-07-05 16:49:54,495] Trial 1 finished with value: 0.13794838206788593 and parameters: {'eta': 0.019458768678795348, 'max_depth': 9, 'subsample': 0.6455825008512953, 'colsample_bytree': 0.7984418603635689, 'min_child_weight': 13, 'lambda': 5.6533574088665524e-06, 'alpha': 3.329362864088068e-06}. Best is trial 0 with value: 0.13759402577494928.\n",
      "[I 2025-07-05 16:50:04,428] Trial 2 finished with value: 0.1376379610132431 and parameters: {'eta': 0.061120046867275075, 'max_depth': 7, 'subsample': 0.9279446047883021, 'colsample_bytree': 0.6467476867757751, 'min_child_weight': 11, 'lambda': 0.9816210563422542, 'alpha': 0.0033889079412873675}. Best is trial 0 with value: 0.13759402577494928.\n",
      "[I 2025-07-05 16:50:12,257] Trial 3 finished with value: 0.14019257760176046 and parameters: {'eta': 0.09853433150801066, 'max_depth': 8, 'subsample': 0.7952502113898771, 'colsample_bytree': 0.6302168833096984, 'min_child_weight': 5, 'lambda': 0.39850525788805186, 'alpha': 1.0283253696512289e-08}. Best is trial 0 with value: 0.13759402577494928.\n",
      "[I 2025-07-05 16:50:33,162] Trial 4 finished with value: 0.14046259269214728 and parameters: {'eta': 0.02004420534467609, 'max_depth': 8, 'subsample': 0.6025203559030186, 'colsample_bytree': 0.7170442259925037, 'min_child_weight': 3, 'lambda': 3.109250858810287e-08, 'alpha': 4.457672560355951}. Best is trial 0 with value: 0.13759402577494928.\n",
      "[I 2025-07-05 16:51:48,462] Trial 5 finished with value: 0.1385217841561307 and parameters: {'eta': 0.020319488628417558, 'max_depth': 12, 'subsample': 0.726120049501682, 'colsample_bytree': 0.7820379652422349, 'min_child_weight': 18, 'lambda': 0.02567560417848391, 'alpha': 5.705695051530428e-06}. Best is trial 0 with value: 0.13759402577494928.\n",
      "[I 2025-07-05 16:52:01,790] Trial 6 finished with value: 0.13845757172089518 and parameters: {'eta': 0.03972776687743424, 'max_depth': 7, 'subsample': 0.6920822789000229, 'colsample_bytree': 0.892362994965959, 'min_child_weight': 4, 'lambda': 0.0001296161375791179, 'alpha': 0.0022944531064945516}. Best is trial 0 with value: 0.13759402577494928.\n",
      "[I 2025-07-05 16:52:31,167] Trial 7 finished with value: 0.1382271557962951 and parameters: {'eta': 0.03267391686975898, 'max_depth': 11, 'subsample': 0.7492721866193472, 'colsample_bytree': 0.8846382978280851, 'min_child_weight': 8, 'lambda': 1.1781205285676257, 'alpha': 0.19167728537123832}. Best is trial 0 with value: 0.13759402577494928.\n",
      "[I 2025-07-05 16:52:44,112] Trial 8 finished with value: 0.14065608839804172 and parameters: {'eta': 0.017420725530916847, 'max_depth': 7, 'subsample': 0.6562892078127436, 'colsample_bytree': 0.6587168815598559, 'min_child_weight': 13, 'lambda': 0.0003909729316906172, 'alpha': 0.0011395813423000381}. Best is trial 0 with value: 0.13759402577494928.\n",
      "[I 2025-07-05 16:52:59,189] Trial 9 finished with value: 0.14997346110408458 and parameters: {'eta': 0.012690308213744676, 'max_depth': 5, 'subsample': 0.7018848996010167, 'colsample_bytree': 0.8799601004502449, 'min_child_weight': 12, 'lambda': 0.03822767369256715, 'alpha': 0.0003742098003500206}. Best is trial 0 with value: 0.13759402577494928.\n",
      "[I 2025-07-05 16:53:40,495] Trial 10 finished with value: 0.14331580593663504 and parameters: {'eta': 0.010445641769893411, 'max_depth': 10, 'subsample': 0.8993397726736568, 'colsample_bytree': 0.9740984680316004, 'min_child_weight': 18, 'lambda': 2.747221661798617e-07, 'alpha': 6.5640506543317185}. Best is trial 0 with value: 0.13759402577494928.\n",
      "[I 2025-07-05 16:53:55,070] Trial 11 finished with value: 0.1410965530449299 and parameters: {'eta': 0.05803838670121933, 'max_depth': 5, 'subsample': 0.9985744594266259, 'colsample_bytree': 0.7089450013537226, 'min_child_weight': 8, 'lambda': 0.0006829988210665358, 'alpha': 0.04971206907250676}. Best is trial 0 with value: 0.13759402577494928.\n",
      "[I 2025-07-05 16:54:07,103] Trial 12 finished with value: 0.13880759426542388 and parameters: {'eta': 0.052786673266093216, 'max_depth': 6, 'subsample': 0.8794607785814965, 'colsample_bytree': 0.6111698357557104, 'min_child_weight': 1, 'lambda': 8.270142089548973, 'alpha': 0.04004568203161059}. Best is trial 0 with value: 0.13759402577494928.\n",
      "[I 2025-07-05 16:54:15,318] Trial 13 finished with value: 0.14006145881720838 and parameters: {'eta': 0.0909463262189672, 'max_depth': 9, 'subsample': 0.8699984826043066, 'colsample_bytree': 0.7291306662483127, 'min_child_weight': 8, 'lambda': 4.480643784786329e-06, 'alpha': 2.1487863956449682e-05}. Best is trial 0 with value: 0.13759402577494928.\n",
      "[I 2025-07-05 16:54:28,654] Trial 14 finished with value: 0.13747487962563426 and parameters: {'eta': 0.05966190307097686, 'max_depth': 7, 'subsample': 0.9978850431328774, 'colsample_bytree': 0.6744474983037334, 'min_child_weight': 16, 'lambda': 0.0039087357905429245, 'alpha': 0.5753645715201999}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:54:40,982] Trial 15 finished with value: 0.1400977723538528 and parameters: {'eta': 0.02937763303184822, 'max_depth': 6, 'subsample': 0.7979925988892609, 'colsample_bytree': 0.6787386345652939, 'min_child_weight': 16, 'lambda': 0.004673918332898897, 'alpha': 1.106609568771697}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:55:04,136] Trial 16 finished with value: 0.13760870819873827 and parameters: {'eta': 0.027963632129826645, 'max_depth': 10, 'subsample': 0.9713906089714683, 'colsample_bytree': 0.7363896538291037, 'min_child_weight': 15, 'lambda': 1.9300837693006983e-05, 'alpha': 0.21427890845676065}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:55:19,593] Trial 17 finished with value: 0.13749569128464784 and parameters: {'eta': 0.039461636605391495, 'max_depth': 8, 'subsample': 0.8398850393248161, 'colsample_bytree': 0.774647686610625, 'min_child_weight': 20, 'lambda': 0.003103466545429171, 'alpha': 0.028128681992743803}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:55:31,216] Trial 18 finished with value: 0.13957149088649148 and parameters: {'eta': 0.042735879519772015, 'max_depth': 6, 'subsample': 0.8374299322492564, 'colsample_bytree': 0.8393853414834332, 'min_child_weight': 19, 'lambda': 0.005683585109649347, 'alpha': 0.012138541576823843}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:55:42,037] Trial 19 finished with value: 0.13922730258576085 and parameters: {'eta': 0.07542904503015758, 'max_depth': 10, 'subsample': 0.9391098105355443, 'colsample_bytree': 0.7784085383300219, 'min_child_weight': 20, 'lambda': 0.06372345509340746, 'alpha': 5.475479807488886e-05}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:55:57,339] Trial 20 finished with value: 0.1375534398511831 and parameters: {'eta': 0.0434086044258079, 'max_depth': 9, 'subsample': 0.833509905065447, 'colsample_bytree': 0.835404309275218, 'min_child_weight': 16, 'lambda': 0.0025499222914321974, 'alpha': 1.0539598057154574e-07}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:56:12,237] Trial 21 finished with value: 0.13772554227505313 and parameters: {'eta': 0.04239749117311967, 'max_depth': 9, 'subsample': 0.8364525482314197, 'colsample_bytree': 0.843781456914769, 'min_child_weight': 16, 'lambda': 0.004658803809687597, 'alpha': 1.0163480047873297e-08}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:56:23,134] Trial 22 finished with value: 0.13868088449693677 and parameters: {'eta': 0.07006622916654903, 'max_depth': 8, 'subsample': 0.8356097389111802, 'colsample_bytree': 0.9472259697409335, 'min_child_weight': 15, 'lambda': 0.0018964535666569674, 'alpha': 2.3369788025655704e-07}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:56:33,432] Trial 23 finished with value: 0.13817061981436654 and parameters: {'eta': 0.04872904087352834, 'max_depth': 7, 'subsample': 0.9296532477687158, 'colsample_bytree': 0.8371885564106224, 'min_child_weight': 20, 'lambda': 3.400876890517551e-05, 'alpha': 1.3992851678439293e-07}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:57:20,740] Trial 24 finished with value: 0.13764408786656548 and parameters: {'eta': 0.03862841535067139, 'max_depth': 9, 'subsample': 0.9999725015161468, 'colsample_bytree': 0.762018696799013, 'min_child_weight': 17, 'lambda': 0.0556308931070537, 'alpha': 0.015362451287081321}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:57:31,996] Trial 25 finished with value: 0.13760977024213084 and parameters: {'eta': 0.06973447657702547, 'max_depth': 8, 'subsample': 0.7736587582108303, 'colsample_bytree': 0.6894163548910204, 'min_child_weight': 14, 'lambda': 0.0009604695815063572, 'alpha': 0.9266456077440265}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:57:48,372] Trial 26 finished with value: 0.13821082678205796 and parameters: {'eta': 0.03710756687311158, 'max_depth': 11, 'subsample': 0.8656479931369773, 'colsample_bytree': 0.9180728809512912, 'min_child_weight': 18, 'lambda': 0.014148174129795417, 'alpha': 7.572345557632327e-05}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:58:00,681] Trial 27 finished with value: 0.13803874732423305 and parameters: {'eta': 0.05102011526842776, 'max_depth': 7, 'subsample': 0.9023493823073765, 'colsample_bytree': 0.8175796586847276, 'min_child_weight': 10, 'lambda': 0.2601199282112491, 'alpha': 4.146322061769157e-07}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:58:12,451] Trial 28 finished with value: 0.13908125032223406 and parameters: {'eta': 0.08438821686637331, 'max_depth': 6, 'subsample': 0.8200951005315997, 'colsample_bytree': 0.8059776321120753, 'min_child_weight': 20, 'lambda': 0.00015839642080550588, 'alpha': 0.00025624604597113577}. Best is trial 14 with value: 0.13747487962563426.\n",
      "[I 2025-07-05 16:58:29,244] Trial 29 finished with value: 0.13799661851464268 and parameters: {'eta': 0.024332936748458652, 'max_depth': 8, 'subsample': 0.95463755676332, 'colsample_bytree': 0.8584262205642589, 'min_child_weight': 17, 'lambda': 0.0019919543413783923, 'alpha': 0.9597723041364938}. Best is trial 14 with value: 0.13747487962563426.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Model Tuning Complete. Best Validation RMSE: 0.1375\n",
      "Best params for mean model: {'eta': 0.05966190307097686, 'max_depth': 7, 'subsample': 0.9978850431328774, 'colsample_bytree': 0.6744474983037334, 'min_child_weight': 16, 'lambda': 0.0039087357905429245, 'alpha': 0.5753645715201999}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: TUNE THE MEAN PREDICTION MODEL\n",
    "# =============================================================================\n",
    "from sklearn.metrics import mean_squared_error # <--- IMPORT THE MISSING FUNCTION\n",
    "print(\"\\n--- STAGE 1, PART 1: Tuning Mean Prediction Model ---\")\n",
    "\n",
    "def objective_mean(trial):\n",
    "    \"\"\"Optuna objective function to find the best hyperparameters for the mean model.\"\"\"\n",
    "    train_x, val_x, train_y, val_y = train_test_split(X, y_log, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dval = xgb.DMatrix(val_x, label=val_y)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 12),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "    }\n",
    "    \n",
    "    model = xgb.train(params, dtrain, num_boost_round=1500,\n",
    "                      evals=[(dval, 'eval')], early_stopping_rounds=50, verbose_eval=False)\n",
    "    \n",
    "    preds = model.predict(dval, iteration_range=(0, model.best_iteration))\n",
    "    return np.sqrt(mean_squared_error(val_y, preds))\n",
    "\n",
    "study_mean = optuna.create_study(direction='minimize')\n",
    "study_mean.optimize(objective_mean, n_trials=N_OPTUNA_TRIALS)\n",
    "best_params_mean = study_mean.best_params\n",
    "\n",
    "print(f\"\\nMean Model Tuning Complete. Best Validation RMSE: {study_mean.best_value:.4f}\")\n",
    "print(f\"Best params for mean model: {best_params_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f9ea256-75d1-4e6c-8fff-7c50b540853f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 1, PART 2: K-Fold Training of Mean Model ---\n",
      "  Mean Model - Fold 1/5...\n",
      "  Mean Model - Fold 2/5...\n",
      "  Mean Model - Fold 3/5...\n",
      "  Mean Model - Fold 4/5...\n",
      "  Mean Model - Fold 5/5...\n",
      "Mean model K-Fold training complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 4: K-FOLD TRAINING OF MEAN MODEL\n",
    "# =============================================================================\n",
    "print(\"\\n--- STAGE 1, PART 2: K-Fold Training of Mean Model ---\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_mean_preds = np.zeros(len(X))\n",
    "test_mean_preds = np.zeros(len(X_test))\n",
    "final_params_mean = {'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist', **best_params_mean}\n",
    "grade_for_stratify = pd.read_csv(DATA_PATH + 'dataset.csv')['grade']\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, grade_for_stratify)):\n",
    "    print(f\"  Mean Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    dtrain = xgb.DMatrix(X.iloc[train_idx], label=y_log.iloc[train_idx])\n",
    "    dval = xgb.DMatrix(X.iloc[val_idx], label=y_log.iloc[val_idx])\n",
    "    model = xgb.train(final_params_mean, dtrain, num_boost_round=2000,\n",
    "                      evals=[(dval, 'eval')], early_stopping_rounds=100, verbose_eval=False)\n",
    "    \n",
    "    oof_mean_preds[val_idx] = model.predict(dval, iteration_range=(0, model.best_iteration))\n",
    "    test_mean_preds += model.predict(xgb.DMatrix(X_test), iteration_range=(0, model.best_iteration)) / N_SPLITS\n",
    "\n",
    "print(\"Mean model K-Fold training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37ef5bdc-7de9-49c4-b155-02f6a1f3ea5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-05 16:59:46,482] A new study created in memory with name: no-name-914e4312-6360-4cce-9cb4-591698f9b3d7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 2, PART 1: Tuning Error (STD) Prediction Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-05 16:59:48,553] Trial 0 finished with value: 0.08921814228036218 and parameters: {'eta': 0.06808667628017309, 'max_depth': 9, 'subsample': 0.9552038469337909, 'colsample_bytree': 0.8985898959633798, 'min_child_weight': 16, 'lambda': 1.9554977369966e-05}. Best is trial 0 with value: 0.08921814228036218.\n",
      "[I 2025-07-05 16:59:54,238] Trial 1 finished with value: 0.08902112588874593 and parameters: {'eta': 0.020849001534289098, 'max_depth': 9, 'subsample': 0.8879254596158186, 'colsample_bytree': 0.6218093858138868, 'min_child_weight': 22, 'lambda': 0.00023526708744078963}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 16:59:57,048] Trial 2 finished with value: 0.08935693425682914 and parameters: {'eta': 0.02804821578145234, 'max_depth': 7, 'subsample': 0.6175696151789862, 'colsample_bytree': 0.6914934319112296, 'min_child_weight': 15, 'lambda': 0.0038294116881693687}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 16:59:59,499] Trial 3 finished with value: 0.08932161021485323 and parameters: {'eta': 0.08030770891675652, 'max_depth': 5, 'subsample': 0.9998260094493652, 'colsample_bytree': 0.768973445368838, 'min_child_weight': 17, 'lambda': 2.2126122344897348}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 17:00:02,520] Trial 4 finished with value: 0.08909831414971504 and parameters: {'eta': 0.05320633704482154, 'max_depth': 9, 'subsample': 0.8695774287701717, 'colsample_bytree': 0.7602907460988602, 'min_child_weight': 23, 'lambda': 5.705506035532729e-06}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 17:00:06,372] Trial 5 finished with value: 0.08912557886064648 and parameters: {'eta': 0.030602362301439447, 'max_depth': 7, 'subsample': 0.6299150912025991, 'colsample_bytree': 0.9195105205122232, 'min_child_weight': 14, 'lambda': 0.00908323802212889}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 17:00:12,547] Trial 6 finished with value: 0.08961931369830231 and parameters: {'eta': 0.0266442551660148, 'max_depth': 4, 'subsample': 0.7568803626379891, 'colsample_bytree': 0.7990168728646914, 'min_child_weight': 21, 'lambda': 0.006248209589679035}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 17:00:14,849] Trial 7 finished with value: 0.08936060316036161 and parameters: {'eta': 0.06600247719703504, 'max_depth': 7, 'subsample': 0.7257852065402872, 'colsample_bytree': 0.6764289312127245, 'min_child_weight': 6, 'lambda': 5.856057841635153e-06}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 17:00:17,654] Trial 8 finished with value: 0.08921188058042726 and parameters: {'eta': 0.037213576963520664, 'max_depth': 8, 'subsample': 0.6650798482203415, 'colsample_bytree': 0.8905577589594891, 'min_child_weight': 22, 'lambda': 0.008999250596532941}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 17:00:20,225] Trial 9 finished with value: 0.08946600030513366 and parameters: {'eta': 0.0455231542170602, 'max_depth': 7, 'subsample': 0.727673569310779, 'colsample_bytree': 0.8611182343396169, 'min_child_weight': 2, 'lambda': 0.00010559301752560928}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 17:00:26,769] Trial 10 finished with value: 0.08945306521100004 and parameters: {'eta': 0.013464308522461494, 'max_depth': 5, 'subsample': 0.861748143668748, 'colsample_bytree': 0.6012397936456122, 'min_child_weight': 10, 'lambda': 1.9413771696194687e-08}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 17:00:32,464] Trial 11 finished with value: 0.08906981408323686 and parameters: {'eta': 0.015355488411711738, 'max_depth': 9, 'subsample': 0.8669050254803881, 'colsample_bytree': 0.610294872331881, 'min_child_weight': 24, 'lambda': 2.210505581439458e-07}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 17:00:39,982] Trial 12 finished with value: 0.08903745747377564 and parameters: {'eta': 0.015316510401574359, 'max_depth': 9, 'subsample': 0.8751061720103477, 'colsample_bytree': 0.6018967600177799, 'min_child_weight': 24, 'lambda': 4.046495065610287e-08}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 17:00:46,946] Trial 13 finished with value: 0.08905376342297161 and parameters: {'eta': 0.01872913912042881, 'max_depth': 8, 'subsample': 0.9216730515717978, 'colsample_bytree': 0.9949368052921828, 'min_child_weight': 20, 'lambda': 1.7978959729821955}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 17:00:55,991] Trial 14 finished with value: 0.08909037938199833 and parameters: {'eta': 0.010742059291970262, 'max_depth': 8, 'subsample': 0.8101877613530414, 'colsample_bytree': 0.6743436097725737, 'min_child_weight': 25, 'lambda': 1.1768926370361645e-08}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 17:01:01,693] Trial 15 finished with value: 0.08906622202089248 and parameters: {'eta': 0.020117336144973488, 'max_depth': 9, 'subsample': 0.811127535343323, 'colsample_bytree': 0.7089103070874009, 'min_child_weight': 19, 'lambda': 2.961825493472457e-07}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 17:01:08,406] Trial 16 finished with value: 0.08915240231517153 and parameters: {'eta': 0.0214176858657622, 'max_depth': 6, 'subsample': 0.9152211888224551, 'colsample_bytree': 0.63586888765122, 'min_child_weight': 10, 'lambda': 0.09087434005562585}. Best is trial 1 with value: 0.08902112588874593.\n",
      "[I 2025-07-05 17:01:19,946] Trial 17 finished with value: 0.08895905649174926 and parameters: {'eta': 0.010029283660549483, 'max_depth': 8, 'subsample': 0.9146826231596981, 'colsample_bytree': 0.6460046226207146, 'min_child_weight': 18, 'lambda': 0.0003061513513254101}. Best is trial 17 with value: 0.08895905649174926.\n",
      "[I 2025-07-05 17:01:30,773] Trial 18 finished with value: 0.08897676394966503 and parameters: {'eta': 0.010696355511906475, 'max_depth': 8, 'subsample': 0.9923800045716663, 'colsample_bytree': 0.7303135832007654, 'min_child_weight': 12, 'lambda': 0.000252298541117174}. Best is trial 17 with value: 0.08895905649174926.\n",
      "[I 2025-07-05 17:01:43,490] Trial 19 finished with value: 0.08902658519323663 and parameters: {'eta': 0.011792344593168505, 'max_depth': 8, 'subsample': 0.999808623568744, 'colsample_bytree': 0.7527835123953538, 'min_child_weight': 11, 'lambda': 0.0006939388899568028}. Best is trial 17 with value: 0.08895905649174926.\n",
      "[I 2025-07-05 17:02:38,335] Trial 20 finished with value: 0.0892377694690478 and parameters: {'eta': 0.010963975467661872, 'max_depth': 6, 'subsample': 0.9588893226530446, 'colsample_bytree': 0.7258593702253637, 'min_child_weight': 12, 'lambda': 0.1085849990239477}. Best is trial 17 with value: 0.08895905649174926.\n",
      "[I 2025-07-05 17:02:44,433] Trial 21 finished with value: 0.08908676266526731 and parameters: {'eta': 0.015606395394976259, 'max_depth': 8, 'subsample': 0.9286450549193845, 'colsample_bytree': 0.6479940779562725, 'min_child_weight': 19, 'lambda': 0.00022394435400825797}. Best is trial 17 with value: 0.08895905649174926.\n",
      "[I 2025-07-05 17:02:54,759] Trial 22 finished with value: 0.08899541013965122 and parameters: {'eta': 0.01046917198910881, 'max_depth': 8, 'subsample': 0.9549011153988319, 'colsample_bytree': 0.6503371201337222, 'min_child_weight': 18, 'lambda': 3.620285900800085e-05}. Best is trial 17 with value: 0.08895905649174926.\n",
      "[I 2025-07-05 17:03:04,578] Trial 23 finished with value: 0.0889919715997676 and parameters: {'eta': 0.010263468942294083, 'max_depth': 8, 'subsample': 0.9693463471962146, 'colsample_bytree': 0.6565786716213438, 'min_child_weight': 17, 'lambda': 2.4192485349159413e-05}. Best is trial 17 with value: 0.08895905649174926.\n",
      "[I 2025-07-05 17:03:15,480] Trial 24 finished with value: 0.08909300439576359 and parameters: {'eta': 0.013054396138162071, 'max_depth': 7, 'subsample': 0.9796954176210321, 'colsample_bytree': 0.7195814481780297, 'min_child_weight': 14, 'lambda': 1.1578781925766495e-06}. Best is trial 17 with value: 0.08895905649174926.\n",
      "[I 2025-07-05 17:03:29,736] Trial 25 finished with value: 0.08890316834804894 and parameters: {'eta': 0.01015003482647279, 'max_depth': 8, 'subsample': 0.9161208842907641, 'colsample_bytree': 0.6664376176064557, 'min_child_weight': 8, 'lambda': 0.001107878407698638}. Best is trial 25 with value: 0.08890316834804894.\n",
      "[I 2025-07-05 17:03:40,300] Trial 26 finished with value: 0.0892077549713822 and parameters: {'eta': 0.013094754573621298, 'max_depth': 6, 'subsample': 0.83850634604789, 'colsample_bytree': 0.8137674681033501, 'min_child_weight': 7, 'lambda': 0.0008519076464544631}. Best is trial 25 with value: 0.08890316834804894.\n",
      "[I 2025-07-05 17:03:51,096] Trial 27 finished with value: 0.08891189906030361 and parameters: {'eta': 0.017291126656214177, 'max_depth': 8, 'subsample': 0.9139011980931444, 'colsample_bytree': 0.7377515072287409, 'min_child_weight': 7, 'lambda': 0.001475215622662807}. Best is trial 25 with value: 0.08890316834804894.\n",
      "[I 2025-07-05 17:03:58,138] Trial 28 finished with value: 0.08901870915133295 and parameters: {'eta': 0.01766244869744199, 'max_depth': 7, 'subsample': 0.9119056276289925, 'colsample_bytree': 0.7889947004054694, 'min_child_weight': 7, 'lambda': 0.04522720425923912}. Best is trial 25 with value: 0.08890316834804894.\n",
      "[I 2025-07-05 17:04:03,740] Trial 29 finished with value: 0.08902163534696995 and parameters: {'eta': 0.023979438120810187, 'max_depth': 9, 'subsample': 0.8334495806976675, 'colsample_bytree': 0.8358353412602239, 'min_child_weight': 4, 'lambda': 0.0023841374744171784}. Best is trial 25 with value: 0.08890316834804894.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error Model Tuning Complete. Best Validation RMSE: 0.0889\n",
      "Best params for error model: {'eta': 0.01015003482647279, 'max_depth': 8, 'subsample': 0.9161208842907641, 'colsample_bytree': 0.6664376176064557, 'min_child_weight': 8, 'lambda': 0.001107878407698638}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 5: TUNE THE ERROR PREDICTION MODEL\n",
    "# =============================================================================\n",
    "print(\"\\n--- STAGE 2, PART 1: Tuning Error (STD) Prediction Model ---\")\n",
    "\n",
    "# Define the new target: the absolute error of the mean model's OOF predictions\n",
    "error_target = np.abs(y_log - oof_mean_preds)\n",
    "# We add the OOF mean predictions as a new feature for the error model\n",
    "X_for_error = X.copy()\n",
    "X_for_error['mean_pred_oof'] = oof_mean_preds\n",
    "\n",
    "def objective_error(trial):\n",
    "    \"\"\"Optuna objective function for the error model.\"\"\"\n",
    "    train_x, val_x, train_y, val_y = train_test_split(X_for_error, error_target, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dval = xgb.DMatrix(val_x, label=val_y)\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 9),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 25),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "    }\n",
    "    model = xgb.train(params, dtrain, num_boost_round=1500,\n",
    "                      evals=[(dval, 'eval')], early_stopping_rounds=50, verbose_eval=False)\n",
    "    preds = model.predict(dval, iteration_range=(0, model.best_iteration))\n",
    "    return np.sqrt(mean_squared_error(val_y, preds))\n",
    "\n",
    "study_error = optuna.create_study(direction='minimize')\n",
    "study_error.optimize(objective_error, n_trials=N_OPTUNA_TRIALS)\n",
    "best_params_error = study_error.best_params\n",
    "\n",
    "print(f\"\\nError Model Tuning Complete. Best Validation RMSE: {study_error.best_value:.4f}\")\n",
    "print(f\"Best params for error model: {best_params_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18883935-f488-4217-beb0-c0d5ec00cc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 2, PART 2: K-Fold Training of Error Model ---\n",
      "  Error Model - Fold 1/5...\n",
      "  Error Model - Fold 2/5...\n",
      "  Error Model - Fold 3/5...\n",
      "  Error Model - Fold 4/5...\n",
      "  Error Model - Fold 5/5...\n",
      "Error model K-Fold training complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 6: K-FOLD TRAINING OF ERROR MODEL\n",
    "# =============================================================================\n",
    "print(\"\\n--- STAGE 2, PART 2: K-Fold Training of Error Model ---\")\n",
    "# Add mean predictions as a feature to the test set as well\n",
    "X_test_for_error = X_test.copy()\n",
    "X_test_for_error['mean_pred_oof'] = test_mean_preds\n",
    "\n",
    "oof_error_preds = np.zeros(len(X))\n",
    "test_error_preds = np.zeros(len(X_test))\n",
    "final_params_error = {'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist', **best_params_error}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_for_error, grade_for_stratify)):\n",
    "    print(f\"  Error Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    dtrain = xgb.DMatrix(X_for_error.iloc[train_idx], label=error_target.iloc[train_idx])\n",
    "    dval = xgb.DMatrix(X_for_error.iloc[val_idx], label=error_target.iloc[val_idx])\n",
    "    \n",
    "    model = xgb.train(final_params_error, dtrain, num_boost_round=2000,\n",
    "                      evals=[(dval, 'eval')], early_stopping_rounds=100, verbose_eval=False)\n",
    "    \n",
    "    oof_error_preds[val_idx] = model.predict(dval, iteration_range=(0, model.best_iteration))\n",
    "    test_error_preds += model.predict(xgb.DMatrix(X_test_for_error), iteration_range=(0, model.best_iteration)) / N_SPLITS\n",
    "\n",
    "print(\"Error model K-Fold training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "184e4710-dce1-40ab-828d-828b860f7c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Final Calibration and Submission (Corrected) ---\n",
      "Searching for best asymmetric multipliers (a, b) in log space...\n",
      "New Best! a=1.05, b=2.95, Score=405,057.86, Cov=80.05%\n",
      "New Best! a=1.10, b=2.55, Score=376,556.89, Cov=80.03%\n",
      "New Best! a=1.15, b=2.35, Score=362,970.16, Cov=80.17%\n",
      "New Best! a=1.20, b=2.20, Score=353,820.41, Cov=80.29%\n",
      "New Best! a=1.25, b=2.05, Score=347,248.47, Cov=80.17%\n",
      "New Best! a=1.30, b=1.95, Score=343,016.43, Cov=80.24%\n",
      "New Best! a=1.30, b=2.00, Score=342,649.79, Cov=80.62%\n",
      "New Best! a=1.30, b=2.05, Score=342,579.70, Cov=81.01%\n",
      "New Best! a=1.35, b=1.85, Score=340,403.06, Cov=80.10%\n",
      "New Best! a=1.35, b=1.90, Score=339,430.51, Cov=80.59%\n",
      "New Best! a=1.35, b=1.95, Score=338,776.20, Cov=81.05%\n",
      "New Best! a=1.35, b=2.00, Score=338,409.55, Cov=81.43%\n",
      "New Best! a=1.35, b=2.05, Score=338,339.47, Cov=81.83%\n",
      "New Best! a=1.40, b=1.80, Score=337,889.53, Cov=80.37%\n",
      "New Best! a=1.40, b=1.85, Score=336,574.82, Cov=80.89%\n",
      "New Best! a=1.40, b=1.90, Score=335,602.27, Cov=81.38%\n",
      "New Best! a=1.40, b=1.95, Score=334,947.96, Cov=81.84%\n",
      "New Best! a=1.40, b=2.00, Score=334,581.32, Cov=82.22%\n",
      "New Best! a=1.40, b=2.05, Score=334,511.23, Cov=82.61%\n",
      "New Best! a=1.45, b=1.80, Score=334,440.15, Cov=81.10%\n",
      "New Best! a=1.45, b=1.85, Score=333,125.45, Cov=81.62%\n",
      "New Best! a=1.45, b=1.90, Score=332,152.89, Cov=82.11%\n",
      "New Best! a=1.45, b=1.95, Score=331,498.58, Cov=82.56%\n",
      "New Best! a=1.45, b=2.00, Score=331,131.94, Cov=82.95%\n",
      "New Best! a=1.45, b=2.05, Score=331,061.85, Cov=83.34%\n",
      "New Best! a=1.50, b=1.85, Score=330,043.78, Cov=82.33%\n",
      "New Best! a=1.50, b=1.90, Score=329,071.23, Cov=82.82%\n",
      "New Best! a=1.50, b=1.95, Score=328,416.91, Cov=83.27%\n",
      "New Best! a=1.50, b=2.00, Score=328,050.27, Cov=83.65%\n",
      "New Best! a=1.50, b=2.05, Score=327,980.18, Cov=84.05%\n",
      "New Best! a=1.55, b=1.85, Score=327,318.61, Cov=83.03%\n",
      "New Best! a=1.55, b=1.90, Score=326,346.06, Cov=83.52%\n",
      "New Best! a=1.55, b=1.95, Score=325,691.74, Cov=83.97%\n",
      "New Best! a=1.55, b=2.00, Score=325,325.10, Cov=84.36%\n",
      "New Best! a=1.55, b=2.05, Score=325,255.01, Cov=84.75%\n",
      "New Best! a=1.60, b=1.85, Score=324,921.92, Cov=83.64%\n",
      "New Best! a=1.60, b=1.90, Score=323,949.37, Cov=84.13%\n",
      "New Best! a=1.60, b=1.95, Score=323,295.05, Cov=84.59%\n",
      "New Best! a=1.60, b=2.00, Score=322,928.41, Cov=84.97%\n",
      "New Best! a=1.60, b=2.05, Score=322,858.32, Cov=85.37%\n",
      "New Best! a=1.65, b=1.85, Score=322,816.13, Cov=84.27%\n",
      "New Best! a=1.65, b=1.90, Score=321,843.58, Cov=84.77%\n",
      "New Best! a=1.65, b=1.95, Score=321,189.27, Cov=85.22%\n",
      "New Best! a=1.65, b=2.00, Score=320,822.62, Cov=85.60%\n",
      "New Best! a=1.65, b=2.05, Score=320,752.53, Cov=86.00%\n",
      "New Best! a=1.70, b=1.90, Score=320,039.90, Cov=85.35%\n",
      "New Best! a=1.70, b=1.95, Score=319,385.59, Cov=85.80%\n",
      "New Best! a=1.70, b=2.00, Score=319,018.95, Cov=86.18%\n",
      "New Best! a=1.70, b=2.05, Score=318,948.86, Cov=86.58%\n",
      "New Best! a=1.75, b=1.90, Score=318,522.12, Cov=85.92%\n",
      "New Best! a=1.75, b=1.95, Score=317,867.80, Cov=86.38%\n",
      "New Best! a=1.75, b=2.00, Score=317,501.16, Cov=86.76%\n",
      "New Best! a=1.75, b=2.05, Score=317,431.07, Cov=87.15%\n",
      "New Best! a=1.80, b=1.90, Score=317,259.16, Cov=86.43%\n",
      "New Best! a=1.80, b=1.95, Score=316,604.85, Cov=86.89%\n",
      "New Best! a=1.80, b=2.00, Score=316,238.20, Cov=87.27%\n",
      "New Best! a=1.80, b=2.05, Score=316,168.12, Cov=87.66%\n",
      "New Best! a=1.85, b=1.95, Score=315,586.67, Cov=87.36%\n",
      "New Best! a=1.85, b=2.00, Score=315,220.03, Cov=87.74%\n",
      "New Best! a=1.85, b=2.05, Score=315,149.94, Cov=88.14%\n",
      "New Best! a=1.90, b=1.95, Score=314,785.96, Cov=87.83%\n",
      "New Best! a=1.90, b=2.00, Score=314,419.32, Cov=88.21%\n",
      "New Best! a=1.90, b=2.05, Score=314,349.23, Cov=88.60%\n",
      "New Best! a=1.95, b=1.95, Score=314,192.93, Cov=88.24%\n",
      "New Best! a=1.95, b=2.00, Score=313,826.29, Cov=88.62%\n",
      "New Best! a=1.95, b=2.05, Score=313,756.20, Cov=89.02%\n",
      "New Best! a=2.00, b=2.00, Score=313,430.24, Cov=89.03%\n",
      "New Best! a=2.00, b=2.05, Score=313,360.15, Cov=89.43%\n",
      "New Best! a=2.05, b=2.00, Score=313,212.38, Cov=89.40%\n",
      "New Best! a=2.05, b=2.05, Score=313,142.29, Cov=89.80%\n",
      "New Best! a=2.10, b=2.05, Score=313,093.58, Cov=90.16%\n",
      "\n",
      "Grid search complete. Final OOF Score: 313,093.58. Best multipliers: a=2.10, b=2.05\n",
      "\n",
      "Creating final submission file...\n",
      "\n",
      "'submission_mean-error_final_v5.csv' created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>802643.333125</td>\n",
       "      <td>1.120053e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>602250.191158</td>\n",
       "      <td>8.518079e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>416796.373025</td>\n",
       "      <td>6.263170e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>302598.928411</td>\n",
       "      <td>4.378161e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>349629.046804</td>\n",
       "      <td>8.168408e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       pi_lower      pi_upper\n",
       "0  200000  802643.333125  1.120053e+06\n",
       "1  200001  602250.191158  8.518079e+05\n",
       "2  200002  416796.373025  6.263170e+05\n",
       "3  200003  302598.928411  4.378161e+05\n",
       "4  200004  349629.046804  8.168408e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL BLOCK (CORRECTED): ASYMMETRIC CALIBRATION IN LOG SPACE\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Final Calibration and Submission (Corrected) ---\")\n",
    "\n",
    "# --- 1. Asymmetric Grid Search for Best Multipliers (in Log Space) ---\n",
    "# The oof_mean_preds and oof_error_preds are already in log scale, which is what we need.\n",
    "# The y_true is the original dollar value, which is correct for the winkler_score function.\n",
    "\n",
    "print(\"Searching for best asymmetric multipliers (a, b) in log space...\")\n",
    "best_a, best_b, best_metric = 2.0, 2.0, float('inf')\n",
    "\n",
    "# Let's search a wider range for the multipliers now.\n",
    "for a in np.arange(1.0, 3.51, 0.05):\n",
    "    for b in np.arange(1.0, 3.51, 0.05):\n",
    "        \n",
    "        # --- CONSTRUCT THE INTERVAL IN LOG SPACE ---\n",
    "        # The error predictions must be positive\n",
    "        error_pred_log = np.clip(oof_error_preds, 0, None) \n",
    "        \n",
    "        # Calculate lower and upper bounds in log scale\n",
    "        lower_log = oof_mean_preds - error_pred_log * a\n",
    "        upper_log = oof_mean_preds + error_pred_log * b\n",
    "        \n",
    "        # --- CONVERT BOUNDS TO DOLLAR SCALE FOR EVALUATION ---\n",
    "        lower_dollar = np.expm1(lower_log)\n",
    "        upper_dollar = np.expm1(upper_log)\n",
    "        \n",
    "        # Safety check: ensure upper > lower\n",
    "        upper_dollar = np.maximum(lower_dollar, upper_dollar)\n",
    "\n",
    "        metric, coverage = winkler_score(y_true, lower_dollar, upper_dollar, alpha=0.1, return_coverage=True)\n",
    "        \n",
    "        # We only care about solutions that have reasonable coverage\n",
    "        if coverage > 0.80 and metric < best_metric:\n",
    "            best_metric = metric\n",
    "            best_a = a\n",
    "            best_b = b\n",
    "            print(f\"New Best! a={best_a:.2f}, b={best_b:.2f}, Score={best_metric:,.2f}, Cov={coverage:.2%}\")\n",
    "\n",
    "print(f\"\\nGrid search complete. Final OOF Score: {best_metric:,.2f}. Best multipliers: a={best_a:.2f}, b={best_b:.2f}\")\n",
    "\n",
    "# --- 2. Create Final Submission ---\n",
    "print(\"\\nCreating final submission file...\")\n",
    "\n",
    "# Use the same logic on the test set predictions\n",
    "error_pred_test_log = np.clip(test_error_preds, 0, None)\n",
    "lower_final_log = test_mean_preds - error_pred_test_log * best_a\n",
    "upper_final_log = test_mean_preds + error_pred_test_log * best_b\n",
    "\n",
    "# Convert final bounds to dollar scale\n",
    "final_lower = np.expm1(lower_final_log)\n",
    "final_upper = np.expm1(upper_final_log)\n",
    "\n",
    "# Final safety checks\n",
    "final_upper = np.maximum(final_lower, final_upper)\n",
    "final_lower = np.clip(final_lower, y_true.min(), None) # Clip at min training price\n",
    "final_upper = np.clip(final_upper, y_true.min(), None)\n",
    "\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': X_test.index,\n",
    "    'pi_lower': final_lower,\n",
    "    'pi_upper': final_upper\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_mean-error_final_v5.csv', index=False)\n",
    "print(\"\\n'submission_mean-error_final_v5.csv' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b1c6d-2c83-4768-888c-990aaa8a42b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
