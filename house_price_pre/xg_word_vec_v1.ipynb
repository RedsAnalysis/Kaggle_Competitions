{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c885e7-8209-457d-9170-ec9cdebcb899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING (No Gensim)\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm', 'view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c15cc71-1e70-4767-9b34-87135f33facd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 2: Feature Engineering with Scikit-learn LSA ---\n",
      "# Creating brute-force numerical interaction features...\n",
      "# Creating LSA (CountVectorizer + SVD) features for text columns...\n",
      "  Processing LSA for: subdivision\n",
      "  Processing LSA for: zoning\n",
      "  Processing LSA for: city\n",
      "  Processing LSA for: sale_warning\n",
      "  Processing LSA for: join_status\n",
      "  Processing LSA for: submarket\n",
      "    -> Skipping SVD for 'submarket' because it has too few unique terms.\n",
      "# Finalizing feature set...\n",
      "\n",
      "LSA Feature Engineering complete. Total features: 103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 2: FEATURE ENGINEERING WITH SKLEARN LSA (CORRECTED)\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 2: Feature Engineering with Scikit-learn LSA ---\")\n",
    "\n",
    "# --- Helper function to clean text ---\n",
    "def simple_tokenizer(text):\n",
    "    return \" \".join(re.findall(r'\\w+', str(text).lower()))\n",
    "\n",
    "def create_synthesized_features(df_train, df_test):\n",
    "    # Combine for consistent processing\n",
    "    df_train['is_train'] = 1\n",
    "    df_test['is_train'] = 0\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    all_data = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # --- A) Brute-Force Numerical Interactions ---\n",
    "    print(\"# Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # --- B) Date Features ---\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['year'] = all_data['sale_date'].dt.year\n",
    "    all_data['month'] = all_data['sale_date'].dt.month\n",
    "    all_data['year_diff'] = all_data['year'] - all_data['year_built']\n",
    "\n",
    "    # --- C) LSA Text Features (Replaces Word2Vec) ---\n",
    "    print(\"# Creating LSA (CountVectorizer + SVD) features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    \n",
    "    for col in text_cols:\n",
    "        print(f\"  Processing LSA for: {col}\")\n",
    "        all_data[col] = all_data[col].fillna('missing').apply(simple_tokenizer)\n",
    "        \n",
    "        cv = CountVectorizer(min_df=2, max_features=1000, ngram_range=(1, 2))\n",
    "        cv_matrix = cv.fit_transform(all_data[col])\n",
    "        \n",
    "        # --- THE FIX: Check the number of features found ---\n",
    "        if cv_matrix.shape[1] > 8: # We need more features than SVD components\n",
    "            svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "            lsa_features = svd.fit_transform(cv_matrix)\n",
    "            lsa_df = pd.DataFrame(lsa_features, columns=[f'{col}_lsa_{i}' for i in range(8)])\n",
    "            all_data = pd.concat([all_data, lsa_df], axis=1)\n",
    "        else:\n",
    "            print(f\"    -> Skipping SVD for '{col}' because it has too few unique terms.\")\n",
    "            # If we skip, we should still handle the original text column later, e.g., by LabelEncoding it.\n",
    "            # For simplicity now, we will just let it be dropped. A more advanced version might LabelEncode it here.\n",
    "\n",
    "\n",
    "    # --- D) Log transform some interactions ---\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "            \n",
    "    # --- E) Final Cleanup ---\n",
    "    print(\"# Finalizing feature set...\")\n",
    "    # Original text columns will now be dropped\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate final datasets\n",
    "    X = all_data[all_data['is_train'] == 1].drop(columns=['is_train', 'sale_price'])\n",
    "    X_test = all_data[all_data['is_train'] == 0].drop(columns=['is_train', 'sale_price'])\n",
    "    X.index, X_test.index = train_ids, test_ids\n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    return X, X_test\n",
    "\n",
    "X, X_test = create_synthesized_features(df_train, df_test)\n",
    "print(f\"\\nLSA Feature Engineering complete. Total features: {X.shape[1]}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc97d10-cc93-43d7-b114-68e41a7b4227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 16:02:08,478] A new study created in memory with name: no-name-4ff0c390-0a47-45bf-9cd3-39a83c5a8d3c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 3: Two-Stage Modeling Pipeline ---\n",
      "\n",
      "# STAGE 1, PART 1: Tuning Mean Prediction Model...\n",
      "# EXPLANATION: Using Optuna to find the best XGBoost settings for our new Word2Vec features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 16:03:49,117] Trial 0 finished with value: 103511.52237311554 and parameters: {'eta': 0.018878361730983267, 'max_depth': 13, 'subsample': 0.9948780317719812, 'colsample_bytree': 0.7652100022209887, 'lambda': 8.15427269994566, 'alpha': 0.0006275933142659056}. Best is trial 0 with value: 103511.52237311554.\n",
      "[I 2025-07-06 16:04:33,130] Trial 1 finished with value: 101304.93468730929 and parameters: {'eta': 0.033921455822549365, 'max_depth': 10, 'subsample': 0.9218005058988112, 'colsample_bytree': 0.8735835230255592, 'lambda': 0.0023086733637852886, 'alpha': 0.18136659202662755}. Best is trial 1 with value: 101304.93468730929.\n",
      "[I 2025-07-06 16:05:29,360] Trial 2 finished with value: 102688.06980365343 and parameters: {'eta': 0.02313785182380389, 'max_depth': 11, 'subsample': 0.9800280613283818, 'colsample_bytree': 0.8112851702317403, 'lambda': 0.8653458518204712, 'alpha': 0.0014850185830649178}. Best is trial 1 with value: 101304.93468730929.\n",
      "[I 2025-07-06 16:07:23,610] Trial 3 finished with value: 104057.72182783938 and parameters: {'eta': 0.014221464534792009, 'max_depth': 13, 'subsample': 0.7529482423115003, 'colsample_bytree': 0.7784884406149144, 'lambda': 0.020200793236472864, 'alpha': 1.6358634219802808}. Best is trial 1 with value: 101304.93468730929.\n",
      "[I 2025-07-06 16:07:42,442] Trial 4 finished with value: 100641.13958019354 and parameters: {'eta': 0.03400921744996549, 'max_depth': 7, 'subsample': 0.873070353932214, 'colsample_bytree': 0.905276854977487, 'lambda': 4.567606010427453, 'alpha': 0.0060883040598612905}. Best is trial 4 with value: 100641.13958019354.\n",
      "[I 2025-07-06 16:08:04,633] Trial 5 finished with value: 99955.80975611173 and parameters: {'eta': 0.03323955857810233, 'max_depth': 8, 'subsample': 0.7082200073460121, 'colsample_bytree': 0.8520125629226701, 'lambda': 0.0007639327154529937, 'alpha': 2.5748015899610412}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:08:25,923] Trial 6 finished with value: 102759.24483957635 and parameters: {'eta': 0.07887626017447325, 'max_depth': 10, 'subsample': 0.808806361634861, 'colsample_bytree': 0.868971056096611, 'lambda': 0.012912280879782384, 'alpha': 2.86275136011192}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:09:02,040] Trial 7 finished with value: 100523.81719771688 and parameters: {'eta': 0.023952477690685953, 'max_depth': 10, 'subsample': 0.8812659294572445, 'colsample_bytree': 0.8045345674890854, 'lambda': 9.07238556018837, 'alpha': 0.06346467092287802}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:10:57,146] Trial 8 finished with value: 107766.69318486116 and parameters: {'eta': 0.017493808175916736, 'max_depth': 13, 'subsample': 0.993858090552276, 'colsample_bytree': 0.9802578158942012, 'lambda': 0.1874574873034074, 'alpha': 1.0703423097014118}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:11:54,512] Trial 9 finished with value: 102034.83389509683 and parameters: {'eta': 0.019385629079705466, 'max_depth': 11, 'subsample': 0.8242570120520354, 'colsample_bytree': 0.9198372736408054, 'lambda': 0.047335291567251817, 'alpha': 0.03174874844346433}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:12:09,771] Trial 10 finished with value: 102614.35065330775 and parameters: {'eta': 0.05121615545392006, 'max_depth': 6, 'subsample': 0.7014819778289847, 'colsample_bytree': 0.7140126656215697, 'lambda': 0.00010948337157601514, 'alpha': 5.471232452778247}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:12:31,693] Trial 11 finished with value: 100591.16398571 and parameters: {'eta': 0.044805172656595074, 'max_depth': 8, 'subsample': 0.8851290511943741, 'colsample_bytree': 0.8137648780871701, 'lambda': 0.0005696471001371345, 'alpha': 0.1244988195479162}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:12:58,056] Trial 12 finished with value: 100634.37822136132 and parameters: {'eta': 0.02844903002190824, 'max_depth': 8, 'subsample': 0.7768937911465179, 'colsample_bytree': 0.8260820786587834, 'lambda': 0.0022927120253781277, 'alpha': 0.3199711786304831}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:13:22,300] Trial 13 finished with value: 102305.12104484311 and parameters: {'eta': 0.012344533637194477, 'max_depth': 8, 'subsample': 0.9267057906321023, 'colsample_bytree': 0.7276007170202902, 'lambda': 0.5842318846965615, 'alpha': 0.011788728294073564}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:13:53,223] Trial 14 finished with value: 101897.94423834074 and parameters: {'eta': 0.010206674605157062, 'max_depth': 9, 'subsample': 0.7049926265509477, 'colsample_bytree': 0.770277031205748, 'lambda': 0.00012905096385367392, 'alpha': 0.00020998360147224176}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:14:35,017] Trial 15 finished with value: 102847.83571859935 and parameters: {'eta': 0.04616202981636158, 'max_depth': 11, 'subsample': 0.8467026644391205, 'colsample_bytree': 0.9324831916431549, 'lambda': 0.0013229628138834535, 'alpha': 0.05860609677730113}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:14:51,416] Trial 16 finished with value: 103399.0762047708 and parameters: {'eta': 0.026502790978007344, 'max_depth': 6, 'subsample': 0.7636661457903055, 'colsample_bytree': 0.8423121293576602, 'lambda': 0.08178118536517014, 'alpha': 0.6329551123275076}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:15:14,422] Trial 17 finished with value: 101430.81346415398 and parameters: {'eta': 0.0632807668333129, 'max_depth': 9, 'subsample': 0.914247973617365, 'colsample_bytree': 0.8865135098816644, 'lambda': 0.00758868710567598, 'alpha': 0.008402255282170222}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:16:18,955] Trial 18 finished with value: 103251.25672842922 and parameters: {'eta': 0.035257022174429026, 'max_depth': 12, 'subsample': 0.8072416077806168, 'colsample_bytree': 0.961673256973178, 'lambda': 0.9808991135365229, 'alpha': 8.229460378278445}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:16:50,752] Trial 19 finished with value: 100753.38096560333 and parameters: {'eta': 0.023635498971482902, 'max_depth': 9, 'subsample': 0.9457992452298926, 'colsample_bytree': 0.7907615875849595, 'lambda': 0.0005267121705540821, 'alpha': 0.0023385303723712568}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:18:42,377] Trial 20 finished with value: 104901.81313971651 and parameters: {'eta': 0.03967741088992955, 'max_depth': 14, 'subsample': 0.7386815682377209, 'colsample_bytree': 0.7432739904252351, 'lambda': 2.4737156421549424, 'alpha': 0.07067177344127844}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:19:04,997] Trial 21 finished with value: 100626.88372398303 and parameters: {'eta': 0.04850951321871466, 'max_depth': 8, 'subsample': 0.8743789063051545, 'colsample_bytree': 0.8066278666578244, 'lambda': 0.000460457513928606, 'alpha': 0.24969243941485686}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:19:23,448] Trial 22 finished with value: 100930.93210706022 and parameters: {'eta': 0.05787075947702396, 'max_depth': 7, 'subsample': 0.882244522990405, 'colsample_bytree': 0.8372185588722767, 'lambda': 0.00045794750207757667, 'alpha': 0.09967034172940735}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:19:40,697] Trial 23 finished with value: 101491.49986082579 and parameters: {'eta': 0.0284332458001824, 'max_depth': 7, 'subsample': 0.8400181971912402, 'colsample_bytree': 0.8589043745698421, 'lambda': 0.00650920317788456, 'alpha': 0.020416910977587896}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:20:10,335] Trial 24 finished with value: 100573.4139024822 and parameters: {'eta': 0.040916295464127055, 'max_depth': 9, 'subsample': 0.8903292851923114, 'colsample_bytree': 0.8090789752968198, 'lambda': 0.0010664225087697093, 'alpha': 0.7101177704734352}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:20:38,228] Trial 25 finished with value: 100502.08165008324 and parameters: {'eta': 0.03883147253201451, 'max_depth': 9, 'subsample': 0.9565590656611141, 'colsample_bytree': 0.7430463600831487, 'lambda': 0.14664812958766663, 'alpha': 0.4757609241802687}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:21:17,305] Trial 26 finished with value: 101139.14755424825 and parameters: {'eta': 0.023397039265957247, 'max_depth': 10, 'subsample': 0.9511664829500294, 'colsample_bytree': 0.7449558899836364, 'lambda': 0.24672066133599957, 'alpha': 2.58845713428928}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:21:47,261] Trial 27 finished with value: 100829.13102868634 and parameters: {'eta': 0.030657245211631906, 'max_depth': 9, 'subsample': 0.9684709033197155, 'colsample_bytree': 0.7446109318881298, 'lambda': 0.08843990920190885, 'alpha': 0.3694695988052884}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:22:25,338] Trial 28 finished with value: 100742.5664950025 and parameters: {'eta': 0.03829685296270983, 'max_depth': 10, 'subsample': 0.7278702185560718, 'colsample_bytree': 0.7927611208537384, 'lambda': 2.4419186355591394, 'alpha': 4.819132926920988}. Best is trial 5 with value: 99955.80975611173.\n",
      "[I 2025-07-06 16:23:17,544] Trial 29 finished with value: 101177.22937499326 and parameters: {'eta': 0.017100895391526257, 'max_depth': 11, 'subsample': 0.7907347021931384, 'colsample_bytree': 0.7075569672674814, 'lambda': 5.293656046608368, 'alpha': 0.5384347253312847}. Best is trial 5 with value: 99955.80975611173.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Mean Model Tuning Complete. Best Validation RMSE: $99,955.81\n",
      "\n",
      "# STAGE 1, PART 2: K-Fold Training of Mean Model...\n",
      "# EXPLANATION: Using the best settings to train 5 models for robust mean predictions.\n",
      "  Mean Model - Fold 1/5...\n",
      "  Mean Model - Fold 2/5...\n",
      "  Mean Model - Fold 3/5...\n",
      "  Mean Model - Fold 4/5...\n",
      "  Mean Model - Fold 5/5...\n",
      "\n",
      "# STAGE 2: K-Fold Training of Error Model...\n",
      "# EXPLANATION: Training a second model to predict the size of the first model's errors.\n",
      "  Error Model - Fold 1/5...\n",
      "  Error Model - Fold 2/5...\n",
      "  Error Model - Fold 3/5...\n",
      "  Error Model - Fold 4/5...\n",
      "  Error Model - Fold 5/5...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: TWO-STAGE TUNING, TRAINING, AND SUBMISSION\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 3: Two-Stage Modeling Pipeline ---\")\n",
    "\n",
    "# --- STAGE 1, PART 1: Tuning Mean Prediction Model ---\n",
    "print(\"\\n# STAGE 1, PART 1: Tuning Mean Prediction Model...\")\n",
    "print(\"# EXPLANATION: Using Optuna to find the best XGBoost settings for our new Word2Vec features.\")\n",
    "def objective_mean(trial):\n",
    "    train_x, val_x, train_y, val_y = train_test_split(X, y_true, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.08, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 14),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-4, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-4, 10.0, log=True),\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params, n_estimators=1500, random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=50)\n",
    "    model.fit(train_x, train_y, eval_set=[(val_x, val_y)], verbose=False)\n",
    "    preds = model.predict(val_x)\n",
    "    return np.sqrt(mean_squared_error(val_y, preds))\n",
    "\n",
    "study_mean = optuna.create_study(direction='minimize')\n",
    "study_mean.optimize(objective_mean, n_trials=N_OPTUNA_TRIALS)\n",
    "best_params_mean = study_mean.best_params\n",
    "print(f\"# Mean Model Tuning Complete. Best Validation RMSE: ${study_mean.best_value:,.2f}\")\n",
    "\n",
    "# --- STAGE 1, PART 2: K-Fold Training of Mean Model ---\n",
    "print(\"\\n# STAGE 1, PART 2: K-Fold Training of Mean Model...\")\n",
    "print(\"# EXPLANATION: Using the best settings to train 5 models for robust mean predictions.\")\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_mean_preds = np.zeros(len(X))\n",
    "test_mean_preds = np.zeros(len(X_test))\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, df_train['grade'])):\n",
    "    print(f\"  Mean Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**best_params_mean, n_estimators=2000, objective='reg:squarederror', eval_metric='rmse', tree_method='hist', random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(X.iloc[train_idx], y_true.iloc[train_idx], eval_set=[(X.iloc[val_idx], y_true.iloc[val_idx])], verbose=False)\n",
    "    oof_mean_preds[val_idx] = model.predict(X.iloc[val_idx])\n",
    "    test_mean_preds += model.predict(X_test) / N_SPLITS\n",
    "\n",
    "# --- STAGE 2: K-Fold Training of Error Model ---\n",
    "print(\"\\n# STAGE 2: K-Fold Training of Error Model...\")\n",
    "print(\"# EXPLANATION: Training a second model to predict the size of the first model's errors.\")\n",
    "error_target = np.abs(y_true - oof_mean_preds)\n",
    "X_for_error = X.copy()\n",
    "X_for_error['mean_pred_oof'] = oof_mean_preds\n",
    "X_test_for_error = X_test.copy()\n",
    "X_test_for_error['mean_pred_oof'] = test_mean_preds\n",
    "params_error = {'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist', 'eta': 0.03, 'max_depth': 7, 'random_state': RANDOM_STATE}\n",
    "oof_error_preds = np.zeros(len(X))\n",
    "test_error_preds = np.zeros(len(X_test))\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_for_error, df_train['grade'])):\n",
    "    print(f\"  Error Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**params_error, n_estimators=1500, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(X_for_error.iloc[train_idx], error_target.iloc[train_idx], eval_set=[(X_for_error.iloc[val_idx], error_target.iloc[val_idx])], verbose=False)\n",
    "    oof_error_preds[val_idx] = model.predict(X_for_error.iloc[val_idx])\n",
    "    test_error_preds += model.predict(X_test_for_error) / N_SPLITS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e8eb95-4f0b-49e4-b039-90227c3b96d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# FINAL STAGE: Asymmetric Calibration & Submission...\n",
      "# EXPLANATION: Finding the best 'stretch' multipliers (a and b) for our intervals to hit 90% coverage.\n",
      "\n",
      "# Grid search complete.\n",
      "# Final OOF Winkler Score: 306,417.29\n",
      "# Best multipliers found: a (lower)=1.98, b (upper)=2.17\n"
     ]
    }
   ],
   "source": [
    "# --- FINAL ASYMMETRIC CALIBRATION AND SUBMISSION ---\n",
    "print(\"\\n# FINAL STAGE: Asymmetric Calibration & Submission...\")\n",
    "print(\"# EXPLANATION: Finding the best 'stretch' multipliers (a and b) for our intervals to hit 90% coverage.\")\n",
    "oof_error_final = np.clip(oof_error_preds, 0, None)\n",
    "best_a, best_b, best_metric = 2.0, 2.0, float('inf')\n",
    "for a in np.arange(1.90, 2.31, 0.01):\n",
    "    for b in np.arange(2.10, 2.51, 0.01):\n",
    "        low = oof_mean_preds - oof_error_final * a\n",
    "        high = oof_mean_preds + oof_error_final * b\n",
    "        metric, coverage = winkler_score(y_true, low, high, alpha=COMPETITION_ALPHA, return_coverage=True)\n",
    "        if metric < best_metric:\n",
    "            best_metric = metric\n",
    "            best_a, best_b = a, b\n",
    "\n",
    "print(f\"\\n# Grid search complete.\")\n",
    "print(f\"# Final OOF Winkler Score: {best_metric:,.2f}\")\n",
    "print(f\"# Best multipliers found: a (lower)={best_a:.2f}, b (upper)={best_b:.2f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5919acd-e38f-4f64-a7fc-d355ba33a20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Final Submission File ---\n",
      "# This will use the models and calibration factors just trained.\n",
      "# Original test IDs loaded successfully.\n",
      "# Applying best found multipliers: a=1.98, b=2.17\n",
      "\n",
      "'submission_xg_word_to_vec_v1.csv' created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>818527.393027</td>\n",
       "      <td>1.048742e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>613586.912266</td>\n",
       "      <td>8.343276e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>430134.787520</td>\n",
       "      <td>6.477051e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>287239.714434</td>\n",
       "      <td>4.115860e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>327391.678672</td>\n",
       "      <td>8.471287e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       pi_lower      pi_upper\n",
       "0  200000  818527.393027  1.048742e+06\n",
       "1  200001  613586.912266  8.343276e+05\n",
       "2  200002  430134.787520  6.477051e+05\n",
       "3  200003  287239.714434  4.115860e+05\n",
       "4  200004  327391.678672  8.471287e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUBMISSION BLOCK\n",
    "# =============================================================================\n",
    "print(\"--- Creating Final Submission File ---\")\n",
    "print(f\"# This will use the models and calibration factors just trained.\")\n",
    "\n",
    "# --- 1. Load the Original Test IDs ---\n",
    "# This is the safest way to ensure the submission IDs are correct.\n",
    "try:\n",
    "    original_test_ids = pd.read_csv('./test.csv', usecols=['id'])['id']\n",
    "    print(\"# Original test IDs loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: test.csv not found. Please ensure it's in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Use the live variables from the previous cell ---\n",
    "# The variables 'test_mean_preds', 'test_error_preds', 'best_a', and 'best_b'\n",
    "# should all be in memory from the long run you just completed.\n",
    "print(f\"# Applying best found multipliers: a={best_a:.2f}, b={best_b:.2f}\")\n",
    "\n",
    "# The predictions are already on the correct dollar scale.\n",
    "# We just need to clip the error predictions to be non-negative.\n",
    "test_mean_final = test_mean_preds\n",
    "test_error_final = np.clip(test_error_preds, 0, None)\n",
    "\n",
    "# --- 3. Construct the Final Calibrated Intervals ---\n",
    "final_lower = test_mean_final - test_error_final * best_a\n",
    "final_upper = test_mean_final + test_error_final * best_b\n",
    "\n",
    "# Final safety checks to ensure lower <= upper and prices are plausible.\n",
    "final_upper = np.maximum(final_lower, final_upper)\n",
    "# We can clip at a reasonable minimum price, e.g., $1000\n",
    "final_lower = np.clip(final_lower, 1000, None)\n",
    "final_upper = np.clip(final_upper, 1000, None)\n",
    "\n",
    "# --- 4. Create and Save the Submission DataFrame ---\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': original_test_ids,\n",
    "    'pi_lower': final_lower,\n",
    "    'pi_upper': final_upper\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_xg_word_to_vec_v1.csv', index=False)\n",
    "print(\"\\n'submission_xg_word_to_vec_v1.csv' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f373f-f0f0-4bb7-8678-98620d77e7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
