{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf06614-55d8-4b1b-9460-e8e8e059fd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BLOCK 1: SETUP ---\n",
      "# Libraries imported successfully.\n",
      "# Global constants defined.\n",
      "# Raw data loaded successfully.\n",
      "# Target variable 'y_true' created.\n",
      "# Setup complete.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "print(\"--- BLOCK 1: SETUP ---\")\n",
    "print(\"# Libraries imported successfully.\")\n",
    "\n",
    "# Helper Function for Winkler Score\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "\n",
    "# Global Constants\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30\n",
    "COMPETITION_ALPHA = 0.1\n",
    "print(\"# Global constants defined.\")\n",
    "\n",
    "# Load Raw Data\n",
    "try:\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm', 'view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"# Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "\n",
    "# Prepare Target Variable\n",
    "y_true = df_train['sale_price'].copy()\n",
    "print(\"# Target variable 'y_true' created.\")\n",
    "print(\"# Setup complete.\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe187db8-e199-4284-99de-18cb24eeecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BLOCK 2: FEATURE ENGINEERING ---\n",
      "# This block creates all features using the winning 'mean-error' strategy's feature set.\n",
      "# Creating brute-force numerical interaction features...\n",
      "# Creating TF-IDF features for text columns...\n",
      "# Finalizing feature set...\n",
      "# Feature engineering complete. Total features: 111\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 2: FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n--- BLOCK 2: FEATURE ENGINEERING ---\")\n",
    "print(\"# This block creates all features using the winning 'mean-error' strategy's feature set.\")\n",
    "\n",
    "# Combine for consistent processing\n",
    "df_train['is_train'] = 1\n",
    "df_test['is_train'] = 0\n",
    "train_ids = df_train.index\n",
    "test_ids = df_test.index\n",
    "all_data = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "# A) Brute-Force Numerical Interactions\n",
    "print(\"# Creating brute-force numerical interaction features...\")\n",
    "NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "for i in range(len(NUMS)):\n",
    "    for j in range(i + 1, len(NUMS)):\n",
    "        all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "# B) Date Features\n",
    "all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "all_data['year'] = all_data['sale_date'].dt.year\n",
    "all_data['month'] = all_data['sale_date'].dt.month\n",
    "all_data['year_diff'] = all_data['year'] - all_data['year_built']\n",
    "\n",
    "# C) TF-IDF Text Features\n",
    "print(\"# Creating TF-IDF features for text columns...\")\n",
    "text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "for col in text_cols:\n",
    "    tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "    tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "    svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "    tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "    tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "    all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "# D) Log transform some interactions\n",
    "for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "    if c in all_data.columns:\n",
    "        all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "        \n",
    "# E) Final Cleanup\n",
    "print(\"# Finalizing feature set...\")\n",
    "cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "all_data = all_data.drop(columns=cols_to_drop)\n",
    "all_data.fillna(0, inplace=True)\n",
    "\n",
    "# Separate final datasets\n",
    "X = all_data[all_data['is_train'] == 1].drop(columns=['is_train', 'sale_price'])\n",
    "X_test = all_data[all_data['is_train'] == 0].drop(columns=['is_train', 'sale_price'])\n",
    "X.index, X_test.index = train_ids, test_ids\n",
    "X_test = X_test[X.columns]\n",
    "print(f\"# Feature engineering complete. Total features: {X.shape[1]}\")\n",
    "del all_data\n",
    "gc.collect()\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d393255-681b-40f5-bdf1-14b67fa5253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 15:11:47,004] A new study created in memory with name: no-name-a51abc5a-1355-4a5b-bf19-7c412cccc1f3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BLOCK 3: TWO-STAGE MODELING PIPELINE ---\n",
      "\n",
      "# STAGE 1, PART 1: Tuning Mean Prediction Model...\n",
      "# EXPLANATION: Using Optuna to find the best XGBoost settings to predict the mean price.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 15:12:01,651] Trial 0 finished with value: 101513.52288242192 and parameters: {'eta': 0.05277751877165471, 'max_depth': 6, 'subsample': 0.9502685680203791, 'colsample_bytree': 0.8594411179712, 'lambda': 0.0014597297975626454, 'alpha': 0.00019263141276771274}. Best is trial 0 with value: 101513.52288242192.\n",
      "[I 2025-07-06 15:12:59,480] Trial 1 finished with value: 101451.42143903159 and parameters: {'eta': 0.010378702349099324, 'max_depth': 11, 'subsample': 0.7663025711400787, 'colsample_bytree': 0.7430461143646169, 'lambda': 5.195518231489485, 'alpha': 0.0034817142150336384}. Best is trial 1 with value: 101451.42143903159.\n",
      "[I 2025-07-06 15:13:40,679] Trial 2 finished with value: 105021.21629461354 and parameters: {'eta': 0.015305674435956491, 'max_depth': 6, 'subsample': 0.9874705575030425, 'colsample_bytree': 0.8644345656996169, 'lambda': 0.03689287276806164, 'alpha': 0.631069603949334}. Best is trial 1 with value: 101451.42143903159.\n",
      "[I 2025-07-06 15:15:12,962] Trial 3 finished with value: 105710.87028305084 and parameters: {'eta': 0.05479105945500278, 'max_depth': 13, 'subsample': 0.7175489335337792, 'colsample_bytree': 0.7996639900829541, 'lambda': 0.00624189305981557, 'alpha': 0.013588339693072481}. Best is trial 1 with value: 101451.42143903159.\n",
      "[I 2025-07-06 15:17:49,277] Trial 4 finished with value: 108025.7059777903 and parameters: {'eta': 0.027924344529562523, 'max_depth': 14, 'subsample': 0.9839029585707697, 'colsample_bytree': 0.7288538067966273, 'lambda': 0.013620686645560624, 'alpha': 0.00012769272795423416}. Best is trial 1 with value: 101451.42143903159.\n",
      "[I 2025-07-06 15:18:18,222] Trial 5 finished with value: 100092.69048237239 and parameters: {'eta': 0.03656490028384056, 'max_depth': 9, 'subsample': 0.9217080337856836, 'colsample_bytree': 0.7987105674438768, 'lambda': 0.2987836689105259, 'alpha': 0.019937629024068943}. Best is trial 5 with value: 100092.69048237239.\n",
      "[I 2025-07-06 15:20:38,382] Trial 6 finished with value: 106162.19801793857 and parameters: {'eta': 0.028544194368086546, 'max_depth': 14, 'subsample': 0.8629179481509782, 'colsample_bytree': 0.8368363365636577, 'lambda': 0.28230035338040793, 'alpha': 0.00020726066011024074}. Best is trial 5 with value: 100092.69048237239.\n",
      "[I 2025-07-06 15:21:11,120] Trial 7 finished with value: 99464.6431250824 and parameters: {'eta': 0.037941472817144786, 'max_depth': 9, 'subsample': 0.8606491609974063, 'colsample_bytree': 0.7303771780009662, 'lambda': 0.16067413492025173, 'alpha': 0.002206135478228619}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:22:45,609] Trial 8 finished with value: 104728.8185744497 and parameters: {'eta': 0.04489878919113822, 'max_depth': 14, 'subsample': 0.8417236441139083, 'colsample_bytree': 0.7132319051570623, 'lambda': 4.0781013707534575, 'alpha': 0.09145101347599167}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:24:28,028] Trial 9 finished with value: 103948.388424256 and parameters: {'eta': 0.01967721507610672, 'max_depth': 13, 'subsample': 0.7722338029021646, 'colsample_bytree': 0.8279561903035201, 'lambda': 0.36026484078415855, 'alpha': 0.006022753586152545}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:24:48,886] Trial 10 finished with value: 102243.61057787425 and parameters: {'eta': 0.07842058954579026, 'max_depth': 9, 'subsample': 0.867506972679299, 'colsample_bytree': 0.9912985338573135, 'lambda': 0.000294330028802647, 'alpha': 4.143198450152677}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:25:15,358] Trial 11 finished with value: 99924.1952682132 and parameters: {'eta': 0.03516813140569197, 'max_depth': 9, 'subsample': 0.9045410772689159, 'colsample_bytree': 0.7770659227802185, 'lambda': 0.27623155317920156, 'alpha': 0.002020358517226487}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:25:35,912] Trial 12 finished with value: 100040.8185492302 and parameters: {'eta': 0.028283874754552928, 'max_depth': 8, 'subsample': 0.8826305390127628, 'colsample_bytree': 0.7610594500026058, 'lambda': 0.08681074191386746, 'alpha': 0.0013932842881920196}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:26:25,358] Trial 13 finished with value: 101659.80476078045 and parameters: {'eta': 0.03464284684896484, 'max_depth': 11, 'subsample': 0.8138526943860505, 'colsample_bytree': 0.9041748104967094, 'lambda': 1.08110349921405, 'alpha': 0.0011244147099433823}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:26:48,581] Trial 14 finished with value: 100530.82536217436 and parameters: {'eta': 0.021372333090514027, 'max_depth': 8, 'subsample': 0.8992052482184612, 'colsample_bytree': 0.780509456487041, 'lambda': 0.09534167589362201, 'alpha': 0.09560435388406877}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:27:23,883] Trial 15 finished with value: 101473.94258626201 and parameters: {'eta': 0.07062579253053076, 'max_depth': 10, 'subsample': 0.9327968730692074, 'colsample_bytree': 0.703590233020355, 'lambda': 1.180469135345393, 'alpha': 0.000751355456081954}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:27:53,226] Trial 16 finished with value: 100116.55543415385 and parameters: {'eta': 0.0421506886012897, 'max_depth': 7, 'subsample': 0.8301087553782841, 'colsample_bytree': 0.9276161847762674, 'lambda': 0.00985618485270785, 'alpha': 0.06732784841687232}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:28:49,644] Trial 17 finished with value: 101861.83583658798 and parameters: {'eta': 0.0199078614482696, 'max_depth': 11, 'subsample': 0.8010874340495132, 'colsample_bytree': 0.7620232607238085, 'lambda': 0.00238134094133413, 'alpha': 0.003234545014039952}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:29:17,380] Trial 18 finished with value: 100387.83543836375 and parameters: {'eta': 0.05883214564864611, 'max_depth': 9, 'subsample': 0.9148507650839238, 'colsample_bytree': 0.8005303413702717, 'lambda': 0.047902541145980776, 'alpha': 0.0004768587724407729}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:29:55,105] Trial 19 finished with value: 100136.54421838213 and parameters: {'eta': 0.036783667040441816, 'max_depth': 10, 'subsample': 0.9579235829576782, 'colsample_bytree': 0.7432735063219755, 'lambda': 1.0591847531047454, 'alpha': 0.007313121423344488}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:30:20,454] Trial 20 finished with value: 100127.58580930631 and parameters: {'eta': 0.024146037928403054, 'max_depth': 8, 'subsample': 0.8880803588853634, 'colsample_bytree': 0.7727047755920664, 'lambda': 0.15362553755018243, 'alpha': 0.5961311427576657}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:30:44,267] Trial 21 finished with value: 99772.66447279035 and parameters: {'eta': 0.030747266155215426, 'max_depth': 8, 'subsample': 0.8826269300009136, 'colsample_bytree': 0.7586014546972599, 'lambda': 0.07602876853064211, 'alpha': 0.001493322398778791}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:31:04,582] Trial 22 finished with value: 100414.70999808743 and parameters: {'eta': 0.03268554158188338, 'max_depth': 7, 'subsample': 0.8488895823688479, 'colsample_bytree': 0.7266771319269787, 'lambda': 0.029926152915783193, 'alpha': 0.002006723077978634}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:31:45,593] Trial 23 finished with value: 100986.29641689015 and parameters: {'eta': 0.04318239039667385, 'max_depth': 10, 'subsample': 0.9018705349322125, 'colsample_bytree': 0.8181595655618749, 'lambda': 0.6368957538597882, 'alpha': 0.00048168077104447275}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:32:08,215] Trial 24 finished with value: 100861.52281222012 and parameters: {'eta': 0.02436429149458433, 'max_depth': 7, 'subsample': 0.9443894303464412, 'colsample_bytree': 0.7026659271984271, 'lambda': 2.148796604015171, 'alpha': 0.03246344986010584}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:32:41,807] Trial 25 finished with value: 100695.3463075628 and parameters: {'eta': 0.014850108839414154, 'max_depth': 9, 'subsample': 0.8656105725712961, 'colsample_bytree': 0.7425029995754158, 'lambda': 0.1795957513863964, 'alpha': 0.007694839529651711}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:33:04,659] Trial 26 finished with value: 100211.37085181501 and parameters: {'eta': 0.04645639477754111, 'max_depth': 8, 'subsample': 0.7909776583880257, 'colsample_bytree': 0.7795865141131723, 'lambda': 0.019282973313191624, 'alpha': 0.0003224897874648793}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:34:29,440] Trial 27 finished with value: 103944.05880087616 and parameters: {'eta': 0.0314600670231689, 'max_depth': 12, 'subsample': 0.8316031304231438, 'colsample_bytree': 0.884088777265393, 'lambda': 0.0710724375799298, 'alpha': 0.002370155259324711}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:34:58,546] Trial 28 finished with value: 100068.937518093 and parameters: {'eta': 0.03775399015193455, 'max_depth': 9, 'subsample': 0.8778488265742441, 'colsample_bytree': 0.7534536948242032, 'lambda': 0.004822258587569339, 'alpha': 0.0008806735399817445}. Best is trial 7 with value: 99464.6431250824.\n",
      "[I 2025-07-06 15:35:15,647] Trial 29 finished with value: 100733.3671828754 and parameters: {'eta': 0.053130795566950675, 'max_depth': 7, 'subsample': 0.964053864729923, 'colsample_bytree': 0.8581009887695777, 'lambda': 0.0002381579816187973, 'alpha': 0.00013732461244804267}. Best is trial 7 with value: 99464.6431250824.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Mean Model Tuning Complete. Best Validation RMSE: $99,464.64\n",
      "\n",
      "# STAGE 1, PART 2: K-Fold Training of Mean Model...\n",
      "# EXPLANATION: Using the best settings to train 5 models for robust mean predictions.\n",
      "  Mean Model - Fold 1/5...\n",
      "  Mean Model - Fold 2/5...\n",
      "  Mean Model - Fold 3/5...\n",
      "  Mean Model - Fold 4/5...\n",
      "  Mean Model - Fold 5/5...\n",
      "\n",
      "# STAGE 2: K-Fold Training of Error Model...\n",
      "# EXPLANATION: Training a second model to predict the size of the first model's errors.\n",
      "  Error Model - Fold 1/5...\n",
      "  Error Model - Fold 2/5...\n",
      "  Error Model - Fold 3/5...\n",
      "  Error Model - Fold 4/5...\n",
      "  Error Model - Fold 5/5...\n",
      "\n",
      "# FINAL STAGE: Asymmetric Calibration & Submission...\n",
      "# EXPLANATION: Finding the best 'stretch' multipliers (a and b) for our intervals to hit 90% coverage.\n",
      "\n",
      "# Grid search complete.\n",
      "# Final OOF Winkler Score: 305,277.21\n",
      "# Best multipliers found: a (lower)=1.98, b (upper)=2.18\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: TWO-STAGE MODELING, TUNING, AND SUBMISSION\n",
    "# =============================================================================\n",
    "print(\"\\n--- BLOCK 3: TWO-STAGE MODELING PIPELINE ---\")\n",
    "\n",
    "# --- STAGE 1, PART 1: Tuning Mean Prediction Model ---\n",
    "print(\"\\n# STAGE 1, PART 1: Tuning Mean Prediction Model...\")\n",
    "print(\"# EXPLANATION: Using Optuna to find the best XGBoost settings to predict the mean price.\")\n",
    "def objective_mean(trial):\n",
    "    train_x, val_x, train_y, val_y = train_test_split(X, y_true, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.08, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 14),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-4, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-4, 10.0, log=True),\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params, n_estimators=1500, random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=50)\n",
    "    model.fit(train_x, train_y, eval_set=[(val_x, val_y)], verbose=False)\n",
    "    preds = model.predict(val_x)\n",
    "    return np.sqrt(mean_squared_error(val_y, preds))\n",
    "\n",
    "study_mean = optuna.create_study(direction='minimize')\n",
    "study_mean.optimize(objective_mean, n_trials=N_OPTUNA_TRIALS)\n",
    "best_params_mean = study_mean.best_params\n",
    "print(f\"# Mean Model Tuning Complete. Best Validation RMSE: ${study_mean.best_value:,.2f}\")\n",
    "\n",
    "# --- STAGE 1, PART 2: K-Fold Training of Mean Model ---\n",
    "print(\"\\n# STAGE 1, PART 2: K-Fold Training of Mean Model...\")\n",
    "print(\"# EXPLANATION: Using the best settings to train 5 models for robust mean predictions.\")\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_mean_preds = np.zeros(len(X))\n",
    "test_mean_preds = np.zeros(len(X_test))\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, df_train['grade'])):\n",
    "    print(f\"  Mean Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**best_params_mean, n_estimators=2000, objective='reg:squarederror', eval_metric='rmse', tree_method='hist', random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(X.iloc[train_idx], y_true.iloc[train_idx], eval_set=[(X.iloc[val_idx], y_true.iloc[val_idx])], verbose=False)\n",
    "    oof_mean_preds[val_idx] = model.predict(X.iloc[val_idx])\n",
    "    test_mean_preds += model.predict(X_test) / N_SPLITS\n",
    "\n",
    "# --- STAGE 2: K-Fold Training of Error Model ---\n",
    "print(\"\\n# STAGE 2: K-Fold Training of Error Model...\")\n",
    "print(\"# EXPLANATION: Training a second model to predict the size of the first model's errors.\")\n",
    "error_target = np.abs(y_true - oof_mean_preds)\n",
    "X_for_error = X.copy()\n",
    "X_for_error['mean_pred_oof'] = oof_mean_preds\n",
    "X_test_for_error = X_test.copy()\n",
    "X_test_for_error['mean_pred_oof'] = test_mean_preds\n",
    "params_error = {'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist', 'eta': 0.03, 'max_depth': 7, 'random_state': RANDOM_STATE}\n",
    "oof_error_preds = np.zeros(len(X))\n",
    "test_error_preds = np.zeros(len(X_test))\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_for_error, df_train['grade'])):\n",
    "    print(f\"  Error Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**params_error, n_estimators=1500, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(X_for_error.iloc[train_idx], error_target.iloc[train_idx], eval_set=[(X_for_error.iloc[val_idx], error_target.iloc[val_idx])], verbose=False)\n",
    "    oof_error_preds[val_idx] = model.predict(X_for_error.iloc[val_idx])\n",
    "    test_error_preds += model.predict(X_test_for_error) / N_SPLITS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0f30a77-32fa-4d95-bff1-c3cf53caee57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# FINAL STAGE: Asymmetric Calibration & Submission...\n",
      "# EXPLANATION: Finding the best 'stretch' multipliers (a and b) for our intervals to hit 90% coverage.\n",
      "\n",
      "# Grid search complete.\n",
      "# Final OOF Winkler Score: 305,277.21\n",
      "# Best multipliers found: a (lower)=1.98, b (upper)=2.18\n"
     ]
    }
   ],
   "source": [
    "# --- FINAL ASYMMETRIC CALIBRATION AND SUBMISSION ---\n",
    "print(\"\\n# FINAL STAGE: Asymmetric Calibration & Submission...\")\n",
    "print(\"# EXPLANATION: Finding the best 'stretch' multipliers (a and b) for our intervals to hit 90% coverage.\")\n",
    "oof_error_final = np.clip(oof_error_preds, 0, None)\n",
    "best_a, best_b, best_metric = 2.0, 2.0, float('inf')\n",
    "for a in np.arange(1.90, 2.31, 0.01):\n",
    "    for b in np.arange(2.10, 2.51, 0.01):\n",
    "        low = oof_mean_preds - oof_error_final * a\n",
    "        high = oof_mean_preds + oof_error_final * b\n",
    "        metric, coverage = winkler_score(y_true, low, high, alpha=COMPETITION_ALPHA, return_coverage=True)\n",
    "        if metric < best_metric:\n",
    "            best_metric = metric\n",
    "            best_a, best_b = a, b\n",
    "\n",
    "print(f\"\\n# Grid search complete.\")\n",
    "print(f\"# Final OOF Winkler Score: {best_metric:,.2f}\")\n",
    "print(f\"# Best multipliers found: a (lower)={best_a:.2f}, b (upper)={best_b:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41545419-930c-44aa-a6e2-62b5b7ce3171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Final Submission File ---\n",
      "# This will use the models and calibration factors just trained.\n",
      "# Original test IDs loaded successfully.\n",
      "# Applying best found multipliers: a=1.98, b=2.18\n",
      "\n",
      "'submission_final_synthesis.csv' created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>822784.689746</td>\n",
       "      <td>1.021477e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>587754.373164</td>\n",
       "      <td>8.132073e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>440741.856504</td>\n",
       "      <td>6.714315e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>279736.628760</td>\n",
       "      <td>4.072457e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>319360.056016</td>\n",
       "      <td>7.650621e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       pi_lower      pi_upper\n",
       "0  200000  822784.689746  1.021477e+06\n",
       "1  200001  587754.373164  8.132073e+05\n",
       "2  200002  440741.856504  6.714315e+05\n",
       "3  200003  279736.628760  4.072457e+05\n",
       "4  200004  319360.056016  7.650621e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUBMISSION BLOCK\n",
    "# =============================================================================\n",
    "print(\"--- Creating Final Submission File ---\")\n",
    "print(f\"# This will use the models and calibration factors just trained.\")\n",
    "\n",
    "# --- 1. Load the Original Test IDs ---\n",
    "# This is the safest way to ensure the submission IDs are correct.\n",
    "try:\n",
    "    original_test_ids = pd.read_csv('./test.csv', usecols=['id'])['id']\n",
    "    print(\"# Original test IDs loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: test.csv not found. Please ensure it's in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Use the live variables from the previous cell ---\n",
    "# The variables 'test_mean_preds', 'test_error_preds', 'best_a', and 'best_b'\n",
    "# should all be in memory from the long run you just completed.\n",
    "print(f\"# Applying best found multipliers: a={best_a:.2f}, b={best_b:.2f}\")\n",
    "\n",
    "# The predictions are already on the correct dollar scale.\n",
    "# We just need to clip the error predictions to be non-negative.\n",
    "test_mean_final = test_mean_preds\n",
    "test_error_final = np.clip(test_error_preds, 0, None)\n",
    "\n",
    "# --- 3. Construct the Final Calibrated Intervals ---\n",
    "final_lower = test_mean_final - test_error_final * best_a\n",
    "final_upper = test_mean_final + test_error_final * best_b\n",
    "\n",
    "# Final safety checks to ensure lower <= upper and prices are plausible.\n",
    "final_upper = np.maximum(final_lower, final_upper)\n",
    "# We can clip at a reasonable minimum price, e.g., $1000\n",
    "final_lower = np.clip(final_lower, 1000, None)\n",
    "final_upper = np.clip(final_upper, 1000, None)\n",
    "\n",
    "# --- 4. Create and Save the Submission DataFrame ---\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': original_test_ids,\n",
    "    'pi_lower': final_lower,\n",
    "    'pi_upper': final_upper\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_final_synthesis.csv', index=False)\n",
    "print(\"\\n'submission_final_synthesis.csv' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32402360-ed46-423f-89cd-027a5b7e405d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd7881-f6b5-45bf-8d4e-8d852bd808b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
