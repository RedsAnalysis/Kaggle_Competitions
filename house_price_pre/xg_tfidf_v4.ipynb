{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d873fc5-79e3-41d0-9907-da9cd99a809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BLOCK 1: SETUP ---\n",
      "# Libraries imported successfully.\n",
      "# Global constants defined.\n",
      "# Raw data loaded successfully.\n",
      "# Target variable 'y_true' created.\n",
      "# Setup complete.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP AND IMPORTS (GENSIM REMOVED)\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "print(\"--- BLOCK 1: SETUP ---\")\n",
    "print(\"# Libraries imported successfully.\")\n",
    "\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30\n",
    "COMPETITION_ALPHA = 0.1\n",
    "print(\"# Global constants defined.\")\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm', 'view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"# Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "print(\"# Target variable 'y_true' created.\")\n",
    "print(\"# Setup complete.\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2ab21-d4c5-46f3-8513-2c792beef151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bddfc431-ff04-4dc2-bf67-dd8271d45837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 2: Final Feature Engineering with Peer-Comparisons ---\n",
      "# Creating foundational and brute-force interaction features...\n",
      "# Creating TF-IDF features for text columns...\n",
      "# Creating NEW peer-comparison (relative) features...\n",
      "# Finalizing feature set...\n",
      "\n",
      "Ultimate FE complete. Total features: 131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 2 (UPGRADED): ADDING PEER-COMPARISON FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 2: Final Feature Engineering with Peer-Comparisons ---\")\n",
    "\n",
    "def create_ultimate_features(df_train, df_test):\n",
    "    # Combine for consistent processing and reset the index\n",
    "    df_train['is_train'] = 1\n",
    "    df_test['is_train'] = 0\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    all_data = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # --- A) Foundational & Brute-Force Features ---\n",
    "    print(\"# Creating foundational and brute-force interaction features...\")\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['year'] = all_data['sale_date'].dt.year\n",
    "    all_data['month'] = all_data['sale_date'].dt.month\n",
    "    all_data['year_diff'] = all_data['year'] - all_data['year_built']\n",
    "    all_data['total_sqft'] = all_data['sqft'] + all_data['sqft_fbsmt']\n",
    "    \n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'total_sqft', 'grade', 'year_diff']\n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # --- B) TF-IDF Text Features (Proven Winner) ---\n",
    "    print(\"# Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "        \n",
    "    # --- C) NEW: Peer-Comparison & Contextual Features ---\n",
    "    print(\"# Creating NEW peer-comparison (relative) features...\")\n",
    "    # First, create location clusters to define \"neighborhoods\"\n",
    "    kmeans = KMeans(n_clusters=40, random_state=RANDOM_STATE, n_init='auto')\n",
    "    all_data['location_cluster'] = kmeans.fit_predict(all_data[['latitude', 'longitude']])\n",
    "    \n",
    "    # We calculate the group averages ONLY from the training data to prevent leakage\n",
    "    train_copy_for_aggs = all_data[all_data['is_train'] == 1].copy()\n",
    "    \n",
    "    group_cols = ['location_cluster', 'city', 'year'] # Define our peer groups\n",
    "    \n",
    "    for group_col in group_cols:\n",
    "        # Define the stats we want to calculate for each group\n",
    "        aggs = {\n",
    "            'grade': ['mean', 'std'],\n",
    "            'total_sqft': ['mean', 'std'],\n",
    "            'year_diff': ['mean', 'std'], # Average age of houses in the group\n",
    "        }\n",
    "        \n",
    "        group_aggs = train_copy_for_aggs.groupby(group_col).agg(aggs)\n",
    "        group_aggs.columns = [f'{c[0]}_{c[1]}_by_{group_col}' for c in group_aggs.columns]\n",
    "        \n",
    "        # Merge these new \"group stats\" back to the main dataframe\n",
    "        all_data = all_data.merge(group_aggs, on=group_col, how='left')\n",
    "\n",
    "        # --- Now create the powerful relative features ---\n",
    "        # How does this house's grade compare to its peers?\n",
    "        all_data[f'grade_vs_mean_{group_col}'] = all_data['grade'] - all_data[f'grade_mean_by_{group_col}']\n",
    "        # How does its size compare?\n",
    "        all_data[f'sqft_vs_mean_{group_col}'] = all_data['total_sqft'] - all_data[f'total_sqft_mean_by_{group_col}']\n",
    "        # How does its age compare (in terms of standard deviations)?\n",
    "        all_data[f'age_zscore_{group_col}'] = (all_data['year_diff'] - all_data[f'year_diff_mean_by_{group_col}']) / (all_data[f'year_diff_std_by_{group_col}'] + 1e-6)\n",
    "\n",
    "    # --- D) Final Cleanup ---\n",
    "    print(\"# Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket', 'latitude', 'longitude']\n",
    "    all_data = all_data.drop(columns=cols_to_drop, errors='ignore')\n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate final datasets\n",
    "    X = all_data[all_data['is_train'] == 1].drop(columns=['is_train', 'sale_price'])\n",
    "    X_test = all_data[all_data['is_train'] == 0].drop(columns=['is_train', 'sale_price'])\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    return X, X_test\n",
    "\n",
    "# We need to re-run this from the original dataframes\n",
    "X, X_test = create_ultimate_features(df_train, df_test)\n",
    "\n",
    "print(f\"\\nUltimate FE complete. Total features: {X.shape[1]}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca871e-1032-4743-9718-4530f93f98bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d57205-46ad-4d8a-a11b-8c6933567a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 18:22:25,739] A new study created in memory with name: no-name-9ebb5466-445b-4179-9912-80ce31e78e8b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 3: Two-Stage Modeling Pipeline ---\n",
      "\n",
      "# STAGE 1, PART 1: Tuning Mean Prediction Model...\n",
      "# EXPLANATION: Using Optuna to find the best XGBoost settings for our new feature set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 18:22:59,740] Trial 0 finished with value: 101084.31775503063 and parameters: {'eta': 0.032748807472869006, 'max_depth': 9, 'subsample': 0.7207252793137036, 'colsample_bytree': 0.912763112955326, 'lambda': 0.06181255372807286, 'alpha': 0.19193762440484527}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:23:25,387] Trial 1 finished with value: 101116.1265080897 and parameters: {'eta': 0.04555982599041849, 'max_depth': 8, 'subsample': 0.909441087279321, 'colsample_bytree': 0.7346101487444393, 'lambda': 0.016010670746096235, 'alpha': 0.030237178001563465}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:24:30,863] Trial 2 finished with value: 102817.06566519004 and parameters: {'eta': 0.010691927101240695, 'max_depth': 11, 'subsample': 0.7877667898407389, 'colsample_bytree': 0.7694344720198496, 'lambda': 0.0003225109581853205, 'alpha': 0.4538085146886243}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:27:24,395] Trial 3 finished with value: 107460.91710012528 and parameters: {'eta': 0.02009788519895573, 'max_depth': 14, 'subsample': 0.9679546511481951, 'colsample_bytree': 0.7781259401121365, 'lambda': 0.0009405082053680565, 'alpha': 0.012266249372850313}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:27:57,948] Trial 4 finished with value: 103519.72302899578 and parameters: {'eta': 0.07678947209350988, 'max_depth': 11, 'subsample': 0.9085607807555545, 'colsample_bytree': 0.7809685362778468, 'lambda': 2.6373283824798577, 'alpha': 0.012713281736058402}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:28:12,765] Trial 5 finished with value: 102554.55835797841 and parameters: {'eta': 0.03627713576653134, 'max_depth': 6, 'subsample': 0.8515242975328776, 'colsample_bytree': 0.70264254355958, 'lambda': 1.1078286566832127, 'alpha': 0.02779273561879814}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:28:42,133] Trial 6 finished with value: 102667.88958579017 and parameters: {'eta': 0.025174673709568784, 'max_depth': 9, 'subsample': 0.9970260932100019, 'colsample_bytree': 0.9397718002040295, 'lambda': 0.02012648134378366, 'alpha': 0.13931328858529274}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:31:54,175] Trial 7 finished with value: 108402.32735508957 and parameters: {'eta': 0.01024450710050065, 'max_depth': 14, 'subsample': 0.941725553662297, 'colsample_bytree': 0.9336216953874485, 'lambda': 0.005868143131679575, 'alpha': 0.893643869888316}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:32:13,704] Trial 8 finished with value: 101632.05037782127 and parameters: {'eta': 0.05381563320285545, 'max_depth': 7, 'subsample': 0.9642693837690686, 'colsample_bytree': 0.8875123480922397, 'lambda': 0.0006430992087495105, 'alpha': 0.006525272905045207}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:33:27,282] Trial 9 finished with value: 104437.74459456696 and parameters: {'eta': 0.01215865855637973, 'max_depth': 12, 'subsample': 0.9975848585843012, 'colsample_bytree': 0.7440623943840131, 'lambda': 0.9521035269112985, 'alpha': 1.5186198654740686}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:33:56,969] Trial 10 finished with value: 101480.35033443666 and parameters: {'eta': 0.018943765087405327, 'max_depth': 9, 'subsample': 0.7127707464472752, 'colsample_bytree': 0.9904483116102657, 'lambda': 0.1468244215776221, 'alpha': 0.00018636219763135223}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:34:19,376] Trial 11 finished with value: 101146.01184426404 and parameters: {'eta': 0.041335438313214325, 'max_depth': 8, 'subsample': 0.8683031680594158, 'colsample_bytree': 0.8413265942395705, 'lambda': 0.10336225865681578, 'alpha': 6.081927876586906}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:34:42,500] Trial 12 finished with value: 101539.24734800824 and parameters: {'eta': 0.05410908383286722, 'max_depth': 8, 'subsample': 0.778655744518816, 'colsample_bytree': 0.8492989134149558, 'lambda': 0.005937457867492181, 'alpha': 0.0016241044746982645}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:34:58,976] Trial 13 finished with value: 102674.28764788192 and parameters: {'eta': 0.031677806877251066, 'max_depth': 6, 'subsample': 0.7082863305621646, 'colsample_bytree': 0.8912008076917145, 'lambda': 0.09971325868047538, 'alpha': 0.09122553099549116}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:35:31,635] Trial 14 finished with value: 102136.14004846668 and parameters: {'eta': 0.04768084346700418, 'max_depth': 10, 'subsample': 0.8010846542394896, 'colsample_bytree': 0.8293316687110376, 'lambda': 0.014946534653163427, 'alpha': 0.0014641608395444383}. Best is trial 0 with value: 101084.31775503063.\n",
      "[I 2025-07-06 18:35:53,852] Trial 15 finished with value: 100773.39585426304 and parameters: {'eta': 0.06694915925337379, 'max_depth': 8, 'subsample': 0.8932274012150795, 'colsample_bytree': 0.7104586611116963, 'lambda': 9.361694804919676, 'alpha': 0.11736141294379332}. Best is trial 15 with value: 100773.39585426304.\n",
      "[I 2025-07-06 18:36:23,224] Trial 16 finished with value: 102508.21282219293 and parameters: {'eta': 0.07998173867146222, 'max_depth': 10, 'subsample': 0.7471963783021067, 'colsample_bytree': 0.9897694528384313, 'lambda': 4.335759707971665, 'alpha': 0.1928194239405468}. Best is trial 15 with value: 100773.39585426304.\n",
      "[I 2025-07-06 18:36:41,219] Trial 17 finished with value: 101038.13394951433 and parameters: {'eta': 0.06251862670725408, 'max_depth': 7, 'subsample': 0.8296053496127679, 'colsample_bytree': 0.9042447330535378, 'lambda': 0.5478937344360744, 'alpha': 4.754750906774064}. Best is trial 15 with value: 100773.39585426304.\n",
      "[I 2025-07-06 18:36:59,890] Trial 18 finished with value: 100536.17792615751 and parameters: {'eta': 0.064345569055992, 'max_depth': 7, 'subsample': 0.8145732492057648, 'colsample_bytree': 0.8129138667813477, 'lambda': 9.318160177356699, 'alpha': 8.816383867681395}. Best is trial 18 with value: 100536.17792615751.\n",
      "[I 2025-07-06 18:37:16,925] Trial 19 finished with value: 101141.95204760485 and parameters: {'eta': 0.0670901327728194, 'max_depth': 7, 'subsample': 0.8862124408819366, 'colsample_bytree': 0.805522746140149, 'lambda': 7.269969113162409, 'alpha': 1.6446250389556074}. Best is trial 18 with value: 100536.17792615751.\n",
      "[I 2025-07-06 18:37:32,796] Trial 20 finished with value: 101987.28328571166 and parameters: {'eta': 0.06329305306721553, 'max_depth': 6, 'subsample': 0.8194496295160456, 'colsample_bytree': 0.7018643339484025, 'lambda': 0.4045889010914799, 'alpha': 9.170882732990714}. Best is trial 18 with value: 100536.17792615751.\n",
      "[I 2025-07-06 18:37:50,169] Trial 21 finished with value: 100904.67707693236 and parameters: {'eta': 0.06043587491330934, 'max_depth': 7, 'subsample': 0.8261422546906865, 'colsample_bytree': 0.8712355819867609, 'lambda': 8.77314675877538, 'alpha': 3.4962447755214296}. Best is trial 18 with value: 100536.17792615751.\n",
      "[I 2025-07-06 18:38:08,608] Trial 22 finished with value: 100975.98859134779 and parameters: {'eta': 0.0554652905523378, 'max_depth': 7, 'subsample': 0.8370776572418085, 'colsample_bytree': 0.8666995118892646, 'lambda': 9.60819583950667, 'alpha': 2.898576912876369}. Best is trial 18 with value: 100536.17792615751.\n",
      "[I 2025-07-06 18:38:29,485] Trial 23 finished with value: 101268.67551222342 and parameters: {'eta': 0.07111530251163123, 'max_depth': 8, 'subsample': 0.7556406982869249, 'colsample_bytree': 0.8076210645500909, 'lambda': 2.327772107061462, 'alpha': 0.6152278712124385}. Best is trial 18 with value: 100536.17792615751.\n",
      "[I 2025-07-06 18:38:48,096] Trial 24 finished with value: 101040.55106738086 and parameters: {'eta': 0.041541653993618616, 'max_depth': 7, 'subsample': 0.8714523262833237, 'colsample_bytree': 0.868006562706029, 'lambda': 2.780676083905433, 'alpha': 2.1163714869888763}. Best is trial 18 with value: 100536.17792615751.\n",
      "[I 2025-07-06 18:39:02,927] Trial 25 finished with value: 103292.21802246285 and parameters: {'eta': 0.02486809738943006, 'max_depth': 6, 'subsample': 0.8066806809676674, 'colsample_bytree': 0.8173414731265584, 'lambda': 8.54717963110852, 'alpha': 7.903814329440736}. Best is trial 18 with value: 100536.17792615751.\n",
      "[I 2025-07-06 18:39:24,411] Trial 26 finished with value: 101021.07059420821 and parameters: {'eta': 0.05592190900997578, 'max_depth': 8, 'subsample': 0.9062207059079409, 'colsample_bytree': 0.743523139119209, 'lambda': 0.2845600427105124, 'alpha': 0.3261317772292621}. Best is trial 18 with value: 100536.17792615751.\n",
      "[I 2025-07-06 18:39:53,330] Trial 27 finished with value: 101037.91605135173 and parameters: {'eta': 0.047792167790222, 'max_depth': 9, 'subsample': 0.8632001845420617, 'colsample_bytree': 0.9411062559780307, 'lambda': 0.9635713782077108, 'alpha': 0.0629602235248486}. Best is trial 18 with value: 100536.17792615751.\n",
      "[I 2025-07-06 18:40:10,388] Trial 28 finished with value: 100897.89785719027 and parameters: {'eta': 0.06367718335883381, 'max_depth': 7, 'subsample': 0.7639901318827551, 'colsample_bytree': 0.7207485448553501, 'lambda': 1.6413874357732499, 'alpha': 0.9524439332268934}. Best is trial 18 with value: 100536.17792615751.\n",
      "[I 2025-07-06 18:40:38,643] Trial 29 finished with value: 100789.93727550385 and parameters: {'eta': 0.03181891572824006, 'max_depth': 9, 'subsample': 0.7401042114280878, 'colsample_bytree': 0.7308142260093133, 'lambda': 2.632815758984602, 'alpha': 0.8067412093321437}. Best is trial 18 with value: 100536.17792615751.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Mean Model Tuning Complete. Best Validation RMSE: $100,536.18\n",
      "\n",
      "# STAGE 1, PART 2: K-Fold Training of Mean Model...\n",
      "# EXPLANATION: Using the best settings to train 5 models for robust mean predictions.\n",
      "  Mean Model - Fold 1/5...\n",
      "  Mean Model - Fold 2/5...\n",
      "  Mean Model - Fold 3/5...\n",
      "  Mean Model - Fold 4/5...\n",
      "  Mean Model - Fold 5/5...\n",
      "\n",
      "# STAGE 2: K-Fold Training of Error Model...\n",
      "# EXPLANATION: Training a second model to predict the size of the first model's errors.\n",
      "  Error Model - Fold 1/5...\n",
      "  Error Model - Fold 2/5...\n",
      "  Error Model - Fold 3/5...\n",
      "  Error Model - Fold 4/5...\n",
      "  Error Model - Fold 5/5...\n",
      "\n",
      "# FINAL STAGE: Asymmetric Calibration & Submission...\n",
      "# EXPLANATION: Finding the best 'stretch' multipliers (a and b) for our intervals to hit 90% coverage.\n",
      "\n",
      "# Grid search complete.\n",
      "# Final OOF Winkler Score: 310,800.50\n",
      "# Best multipliers found: a (lower)=1.96, b (upper)=2.19\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: TWO-STAGE TUNING, TRAINING, AND SUBMISSION (CORRECTED)\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Block 3: Two-Stage Modeling Pipeline ---\")\n",
    "\n",
    "# --- STAGE 1, PART 1: Tuning Mean Prediction Model ---\n",
    "print(\"\\n# STAGE 1, PART 1: Tuning Mean Prediction Model...\")\n",
    "print(\"# EXPLANATION: Using Optuna to find the best XGBoost settings for our new feature set.\")\n",
    "def objective_mean(trial):\n",
    "    # This function uses the X and y_true created in the previous blocks\n",
    "    train_x, val_x, train_y, val_y = train_test_split(X, y_true, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # We are using the scikit-learn wrapper (XGBRegressor), which is compatible with most environments\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.08, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 14),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-4, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-4, 10.0, log=True),\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params, n_estimators=1500, random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=50)\n",
    "    model.fit(train_x, train_y, eval_set=[(val_x, val_y)], verbose=False)\n",
    "    preds = model.predict(val_x)\n",
    "    return np.sqrt(mean_squared_error(val_y, preds))\n",
    "\n",
    "study_mean = optuna.create_study(direction='minimize')\n",
    "study_mean.optimize(objective_mean, n_trials=N_OPTUNA_TRIALS)\n",
    "best_params_mean = study_mean.best_params\n",
    "print(f\"# Mean Model Tuning Complete. Best Validation RMSE: ${study_mean.best_value:,.2f}\")\n",
    "\n",
    "# --- STAGE 1, PART 2: K-Fold Training of Mean Model ---\n",
    "print(\"\\n# STAGE 1, PART 2: K-Fold Training of Mean Model...\")\n",
    "print(\"# EXPLANATION: Using the best settings to train 5 models for robust mean predictions.\")\n",
    "\n",
    "# THE FIX: We need the 'grade' column for stratification. The safest way is to reload it.\n",
    "grade_for_stratify = pd.read_csv(DATA_PATH + 'dataset.csv')['grade']\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_mean_preds = np.zeros(len(X))\n",
    "test_mean_preds = np.zeros(len(X_test))\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, grade_for_stratify)):\n",
    "    print(f\"  Mean Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**best_params_mean, n_estimators=2000, objective='reg:squarederror', eval_metric='rmse', tree_method='hist', random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(X.iloc[train_idx], y_true.iloc[train_idx], eval_set=[(X.iloc[val_idx], y_true.iloc[val_idx])], verbose=False)\n",
    "    oof_mean_preds[val_idx] = model.predict(X.iloc[val_idx])\n",
    "    test_mean_preds += model.predict(X_test) / N_SPLITS\n",
    "\n",
    "# --- STAGE 2: K-Fold Training of Error Model ---\n",
    "print(\"\\n# STAGE 2: K-Fold Training of Error Model...\")\n",
    "print(\"# EXPLANATION: Training a second model to predict the size of the first model's errors.\")\n",
    "error_target = np.abs(y_true - oof_mean_preds)\n",
    "X_for_error = X.copy()\n",
    "X_for_error['mean_pred_oof'] = oof_mean_preds\n",
    "X_test_for_error = X_test.copy()\n",
    "X_test_for_error['mean_pred_oof'] = test_mean_preds\n",
    "\n",
    "# We will not tune the error model for speed, but use good default parameters\n",
    "params_error = {'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist', 'eta': 0.03, 'max_depth': 7, 'random_state': RANDOM_STATE}\n",
    "oof_error_preds = np.zeros(len(X))\n",
    "test_error_preds = np.zeros(len(X_test))\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_for_error, grade_for_stratify)):\n",
    "    print(f\"  Error Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**params_error, n_estimators=1500, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(X_for_error.iloc[train_idx], error_target.iloc[train_idx], eval_set=[(X_for_error.iloc[val_idx], error_target.iloc[val_idx])], verbose=False)\n",
    "    oof_error_preds[val_idx] = model.predict(X_for_error.iloc[val_idx])\n",
    "    test_error_preds += model.predict(X_test_for_error) / N_SPLITS\n",
    "\n",
    "# --- FINAL ASYMMETRIC CALIBRATION AND SUBMISSION ---\n",
    "print(\"\\n# FINAL STAGE: Asymmetric Calibration & Submission...\")\n",
    "print(\"# EXPLANATION: Finding the best 'stretch' multipliers (a and b) for our intervals to hit 90% coverage.\")\n",
    "oof_error_final = np.clip(oof_error_preds, 0, None) # Ensure error is not negative\n",
    "best_a, best_b, best_metric = 2.0, 2.0, float('inf')\n",
    "for a in np.arange(1.90, 2.31, 0.01):\n",
    "    for b in np.arange(2.10, 2.51, 0.01):\n",
    "        low = oof_mean_preds - oof_error_final * a\n",
    "        high = oof_mean_preds + oof_error_final * b\n",
    "        metric, coverage = winkler_score(y_true, low, high, alpha=COMPETITION_ALPHA, return_coverage=True)\n",
    "        # We only care about solutions that are close to the target coverage\n",
    "        if coverage > 0.88 and metric < best_metric:\n",
    "            best_metric = metric\n",
    "            best_a, best_b = a, b\n",
    "\n",
    "print(f\"\\n# Grid search complete.\")\n",
    "print(f\"# Final OOF Winkler Score: {best_metric:,.2f}\")\n",
    "print(f\"# Best multipliers found: a (lower)={best_a:.2f}, b (upper)={best_b:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50fcfcff-e8ce-4ddd-a536-c9a957b3bee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Final Submission File with Correct Logic ---\n",
      "Applying best multipliers: a=1.97, b=2.19\n",
      "\n",
      "'submission_ultimate_synthesis_CORRECT_LOGIC.csv' created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>811591.846895</td>\n",
       "      <td>1.055108e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>555288.410410</td>\n",
       "      <td>7.994956e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>465397.940078</td>\n",
       "      <td>6.771260e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>299764.264575</td>\n",
       "      <td>4.305402e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>314232.609277</td>\n",
       "      <td>7.993606e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       pi_lower      pi_upper\n",
       "0  200000  811591.846895  1.055108e+06\n",
       "1  200001  555288.410410  7.994956e+05\n",
       "2  200002  465397.940078  6.771260e+05\n",
       "3  200003  299764.264575  4.305402e+05\n",
       "4  200004  314232.609277  7.993606e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUBMISSION BLOCK (CORRECTED FOR MEAN-ERROR MODEL)\n",
    "# =============================================================================\n",
    "print(\"--- Creating Final Submission File with Correct Logic ---\")\n",
    "\n",
    "# --- Reload the original test set to get the correct index ---\n",
    "original_test_ids = pd.read_csv('./test.csv', usecols=['id'])['id']\n",
    "\n",
    "# --- Use the best found multipliers from your last successful run ---\n",
    "# The grid search found these values were optimal\n",
    "best_a = 1.97\n",
    "best_b = 2.19 # Corrected to the final best value from your log\n",
    "print(f\"Applying best multipliers: a={best_a}, b={best_b}\")\n",
    "\n",
    "# --- Use the prediction arrays directly (THEY ARE ALREADY IN DOLLAR SCALE) ---\n",
    "# The test_mean_preds and test_error_preds arrays are still in memory.\n",
    "# DO NOT apply np.expm1() to them.\n",
    "test_mean_final = test_mean_preds\n",
    "test_error_final = np.clip(test_error_preds, 0, None) # Still clip error to be non-negative\n",
    "\n",
    "final_lower = test_mean_final - test_error_final * best_a\n",
    "final_upper = test_mean_final + test_error_final * best_b\n",
    "\n",
    "# Final safety checks\n",
    "final_upper = np.maximum(final_lower, final_upper)\n",
    "\n",
    "# --- Create submission dataframe with the CORRECT IDs ---\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': original_test_ids,\n",
    "    'pi_lower': final_lower,\n",
    "    'pi_upper': final_upper\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_ultimate_synthesis_CORRECT_LOGIC.csv', index=False)\n",
    "print(\"\\n'submission_ultimate_synthesis_CORRECT_LOGIC.csv' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dbcb61-58dd-4089-96af-81258985b476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b1f4d-300d-4e32-87f2-e28b20397f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
