{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d873fc5-79e3-41d0-9907-da9cd99a809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm', 'view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bddfc431-ff04-4dc2-bf67-dd8271d45837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Block 2: Synthesized Feature Engineering ---\n",
      "Creating brute-force numerical interaction features...\n",
      "Creating TF-IDF features for text columns...\n",
      "Finalizing feature set...\n",
      "\n",
      "Synthesized FE complete. Total features: 111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 2: SYNTHESIZED FEATURE ENGINEERING (CORRECTED)\n",
    "# =============================================================================\n",
    "print(\"--- Starting Block 2: Synthesized Feature Engineering ---\")\n",
    "\n",
    "def create_synthesized_features(df_train, df_test):\n",
    "    # Combine for consistent processing and reset the index\n",
    "    df_train['is_train'] = 1\n",
    "    df_test['is_train'] = 0\n",
    "    # Store the original id for later, as reset_index will remove it\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    all_data = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # --- A) Brute-Force Numerical Interactions ---\n",
    "    print(\"Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # --- B) Date Features ---\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['year'] = all_data['sale_date'].dt.year\n",
    "    all_data['month'] = all_data['sale_date'].dt.month\n",
    "    all_data['year_diff'] = all_data['year'] - all_data['year_built']\n",
    "\n",
    "    # --- C) TF-IDF Text Features ---\n",
    "    print(\"Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "\n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        # This concat will now work because both have a simple 0-based index\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # --- D) Log transform some of the new interaction features ---\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            # Add a small constant to avoid log(0)\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "            \n",
    "    # --- E) Final Cleanup ---\n",
    "    print(\"Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate final datasets\n",
    "    X = all_data[all_data['is_train'] == 1].drop(columns=['is_train', 'sale_price'])\n",
    "    X_test = all_data[all_data['is_train'] == 0].drop(columns=['is_train', 'sale_price'])\n",
    "    \n",
    "    # Restore the original 'id' as the index\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    \n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    return X, X_test\n",
    "\n",
    "# We need to re-run this from the original dataframes\n",
    "X, X_test = create_synthesized_features(df_train, df_test)\n",
    "\n",
    "print(f\"\\nSynthesized FE complete. Total features: {X.shape[1]}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d57205-46ad-4d8a-a11b-8c6933567a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-05 20:08:03,955] A new study created in memory with name: no-name-7cd66601-7f4a-4807-a9cf-2838f61bfe3e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 3: Two-Stage Modeling Pipeline ---\n",
      "\n",
      "--- STAGE 1, PART 1: Tuning Mean Prediction Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-05 20:08:42,536] Trial 0 finished with value: 101504.37326539187 and parameters: {'eta': 0.05573933645904832, 'max_depth': 10, 'subsample': 0.9699253518931916, 'colsample_bytree': 0.8121635404786202, 'lambda': 0.00012827397807913733, 'alpha': 0.04128350144143836}. Best is trial 0 with value: 101504.37326539187.\n",
      "[I 2025-07-05 20:09:08,078] Trial 1 finished with value: 101361.48886041483 and parameters: {'eta': 0.06341774877418375, 'max_depth': 9, 'subsample': 0.9300405157754874, 'colsample_bytree': 0.8047386540220012, 'lambda': 0.007092525993245265, 'alpha': 0.013134048012628113}. Best is trial 1 with value: 101361.48886041483.\n",
      "[I 2025-07-05 20:09:50,289] Trial 2 finished with value: 102032.18441256661 and parameters: {'eta': 0.03973065266364448, 'max_depth': 10, 'subsample': 0.9854435167551862, 'colsample_bytree': 0.9815832276926475, 'lambda': 0.3701776220260173, 'alpha': 0.0014275603813008694}. Best is trial 1 with value: 101361.48886041483.\n",
      "[I 2025-07-05 20:10:07,948] Trial 3 finished with value: 103773.09400803274 and parameters: {'eta': 0.020513254121225805, 'max_depth': 6, 'subsample': 0.8533012326998135, 'colsample_bytree': 0.9222017163243018, 'lambda': 0.008344925317534878, 'alpha': 0.00010939493723752159}. Best is trial 1 with value: 101361.48886041483.\n",
      "[I 2025-07-05 20:12:58,608] Trial 4 finished with value: 106861.3109408639 and parameters: {'eta': 0.028704302364464807, 'max_depth': 14, 'subsample': 0.8533705554741399, 'colsample_bytree': 0.7964118087350869, 'lambda': 0.0031776386352681697, 'alpha': 0.00033819743810671203}. Best is trial 1 with value: 101361.48886041483.\n",
      "[I 2025-07-05 20:13:16,995] Trial 5 finished with value: 102768.36244681531 and parameters: {'eta': 0.027007011436496097, 'max_depth': 6, 'subsample': 0.9529222069287695, 'colsample_bytree': 0.876615758643901, 'lambda': 2.082113888733831, 'alpha': 2.3575133131249095}. Best is trial 1 with value: 101361.48886041483.\n",
      "[I 2025-07-05 20:16:12,194] Trial 6 finished with value: 107035.97242049049 and parameters: {'eta': 0.023935041036867576, 'max_depth': 14, 'subsample': 0.8752131593293218, 'colsample_bytree': 0.8937101062745778, 'lambda': 0.0017861480545926216, 'alpha': 0.015774990145186355}. Best is trial 1 with value: 101361.48886041483.\n",
      "[I 2025-07-05 20:17:02,742] Trial 7 finished with value: 102575.51458315966 and parameters: {'eta': 0.04608166347998252, 'max_depth': 11, 'subsample': 0.8323271029911605, 'colsample_bytree': 0.8316843083153067, 'lambda': 0.0002255210661964334, 'alpha': 0.010774232662177268}. Best is trial 1 with value: 101361.48886041483.\n",
      "[I 2025-07-05 20:17:30,913] Trial 8 finished with value: 101365.30246588326 and parameters: {'eta': 0.062244640397040155, 'max_depth': 10, 'subsample': 0.8878937646268107, 'colsample_bytree': 0.8899389572262255, 'lambda': 0.07975602797940143, 'alpha': 0.15113389240001585}. Best is trial 1 with value: 101361.48886041483.\n",
      "[I 2025-07-05 20:18:09,887] Trial 9 finished with value: 100885.79459963628 and parameters: {'eta': 0.020819389004029635, 'max_depth': 10, 'subsample': 0.9724943978469593, 'colsample_bytree': 0.7029322573731387, 'lambda': 0.5850240261639676, 'alpha': 9.424059476344603}. Best is trial 9 with value: 100885.79459963628.\n",
      "[I 2025-07-05 20:19:19,036] Trial 10 finished with value: 101710.39293995476 and parameters: {'eta': 0.010797043567908325, 'max_depth': 12, 'subsample': 0.7080466759396798, 'colsample_bytree': 0.7032696071874734, 'lambda': 7.312071092018967, 'alpha': 8.976523781805499}. Best is trial 9 with value: 100885.79459963628.\n",
      "[I 2025-07-05 20:19:43,419] Trial 11 finished with value: 101400.9969181763 and parameters: {'eta': 0.015061743120223467, 'max_depth': 8, 'subsample': 0.9163035405843574, 'colsample_bytree': 0.7036780576335542, 'lambda': 0.06166911177615359, 'alpha': 0.7278993855181941}. Best is trial 9 with value: 100885.79459963628.\n",
      "[I 2025-07-05 20:20:06,023] Trial 12 finished with value: 99792.06028537541 and parameters: {'eta': 0.03594390146209524, 'max_depth': 8, 'subsample': 0.9363913047937791, 'colsample_bytree': 0.7759323493885808, 'lambda': 0.39133731829915686, 'alpha': 0.0034844732827190067}. Best is trial 12 with value: 99792.06028537541.\n",
      "[I 2025-07-05 20:20:27,890] Trial 13 finished with value: 99728.04492217823 and parameters: {'eta': 0.03706668971016038, 'max_depth': 8, 'subsample': 0.793389078477993, 'colsample_bytree': 0.7562576200595996, 'lambda': 0.33972028805218296, 'alpha': 0.002226998840810822}. Best is trial 13 with value: 99728.04492217823.\n",
      "[I 2025-07-05 20:20:50,148] Trial 14 finished with value: 99677.51393368516 and parameters: {'eta': 0.03740634184482622, 'max_depth': 8, 'subsample': 0.7597654010801302, 'colsample_bytree': 0.7517306786824699, 'lambda': 0.3429501074709537, 'alpha': 0.0017300948399407148}. Best is trial 14 with value: 99677.51393368516.\n",
      "[I 2025-07-05 20:21:09,114] Trial 15 finished with value: 100234.97257943457 and parameters: {'eta': 0.07725363478196845, 'max_depth': 7, 'subsample': 0.7808628858448549, 'colsample_bytree': 0.7560373071012152, 'lambda': 0.12228298379265286, 'alpha': 0.0011891220049019275}. Best is trial 14 with value: 99677.51393368516.\n",
      "[I 2025-07-05 20:21:33,356] Trial 16 finished with value: 99623.50373280393 and parameters: {'eta': 0.03914043741572593, 'max_depth': 8, 'subsample': 0.7685084744503429, 'colsample_bytree': 0.7389464479889367, 'lambda': 2.5975079817028472, 'alpha': 0.0004887370301118097}. Best is trial 16 with value: 99623.50373280393.\n",
      "[I 2025-07-05 20:21:52,530] Trial 17 finished with value: 100315.37086608412 and parameters: {'eta': 0.03176210927692335, 'max_depth': 7, 'subsample': 0.7216851874776251, 'colsample_bytree': 0.7419904660084689, 'lambda': 8.940006813156637, 'alpha': 0.0003108031531422547}. Best is trial 16 with value: 99623.50373280393.\n",
      "[I 2025-07-05 20:22:44,729] Trial 18 finished with value: 103474.5668461579 and parameters: {'eta': 0.0504240973154943, 'max_depth': 12, 'subsample': 0.7551794814831465, 'colsample_bytree': 0.7422010997623988, 'lambda': 1.269953055954583, 'alpha': 0.0004335770736443396}. Best is trial 16 with value: 99623.50373280393.\n",
      "[I 2025-07-05 20:23:16,144] Trial 19 finished with value: 99268.23975471711 and parameters: {'eta': 0.04398439528422641, 'max_depth': 9, 'subsample': 0.8084603446618345, 'colsample_bytree': 0.851619578583859, 'lambda': 3.8869745770111965, 'alpha': 0.00011819240290024047}. Best is trial 19 with value: 99268.23975471711.\n",
      "[I 2025-07-05 20:23:42,888] Trial 20 finished with value: 100428.84812642232 and parameters: {'eta': 0.07781513134275302, 'max_depth': 9, 'subsample': 0.8138346797023719, 'colsample_bytree': 0.8520926808480421, 'lambda': 2.8895398636311205, 'alpha': 0.00012329069420165527}. Best is trial 19 with value: 99268.23975471711.\n",
      "[I 2025-07-05 20:24:13,202] Trial 21 finished with value: 99984.4288676992 and parameters: {'eta': 0.046237007398922614, 'max_depth': 9, 'subsample': 0.7514460827712793, 'colsample_bytree': 0.8421301351768204, 'lambda': 2.1290975595018646, 'alpha': 0.0006546917604730697}. Best is trial 19 with value: 99268.23975471711.\n",
      "[I 2025-07-05 20:24:31,561] Trial 22 finished with value: 99809.56972154524 and parameters: {'eta': 0.042166134657811834, 'max_depth': 7, 'subsample': 0.763250176778745, 'colsample_bytree': 0.9459592805263547, 'lambda': 4.540577236741204, 'alpha': 0.0001690379271378101}. Best is trial 19 with value: 99268.23975471711.\n",
      "[I 2025-07-05 20:24:54,448] Trial 23 finished with value: 99595.88618010284 and parameters: {'eta': 0.03288989928786827, 'max_depth': 8, 'subsample': 0.7325569039715015, 'colsample_bytree': 0.7825808302123672, 'lambda': 0.9244571713688868, 'alpha': 0.005761738020039474}. Best is trial 19 with value: 99268.23975471711.\n",
      "[I 2025-07-05 20:25:24,224] Trial 24 finished with value: 99543.52390788664 and parameters: {'eta': 0.03286675211466991, 'max_depth': 9, 'subsample': 0.7292064716354298, 'colsample_bytree': 0.7905271817987753, 'lambda': 1.1953799641903544, 'alpha': 0.004281042807154854}. Best is trial 19 with value: 99268.23975471711.\n",
      "[I 2025-07-05 20:26:17,209] Trial 25 finished with value: 101464.46645008291 and parameters: {'eta': 0.03176121854251361, 'max_depth': 11, 'subsample': 0.7336740119728259, 'colsample_bytree': 0.7844943822227528, 'lambda': 0.9337062972020359, 'alpha': 0.004750476234386677}. Best is trial 19 with value: 99268.23975471711.\n",
      "[I 2025-07-05 20:26:49,574] Trial 26 finished with value: 100362.44843565745 and parameters: {'eta': 0.022641792322077935, 'max_depth': 9, 'subsample': 0.8036766527032441, 'colsample_bytree': 0.8210961572685889, 'lambda': 0.1615393214462318, 'alpha': 0.06301965825140676}. Best is trial 19 with value: 99268.23975471711.\n",
      "[I 2025-07-05 20:27:45,650] Trial 27 finished with value: 101569.49715342693 and parameters: {'eta': 0.027019784625581904, 'max_depth': 11, 'subsample': 0.7046627774694747, 'colsample_bytree': 0.8611174285981895, 'lambda': 0.02392482754998621, 'alpha': 0.2252351473136093}. Best is trial 19 with value: 99268.23975471711.\n",
      "[I 2025-07-05 20:28:15,678] Trial 28 finished with value: 100257.71591254212 and parameters: {'eta': 0.01614810555352784, 'max_depth': 9, 'subsample': 0.7340369778171244, 'colsample_bytree': 0.7742993162454617, 'lambda': 0.7830490562436693, 'alpha': 0.09188607494672793}. Best is trial 19 with value: 99268.23975471711.\n",
      "[I 2025-07-05 20:28:33,624] Trial 29 finished with value: 100285.29415622212 and parameters: {'eta': 0.05624182213452764, 'max_depth': 7, 'subsample': 0.8160260743914391, 'colsample_bytree': 0.8179106253541402, 'lambda': 0.03302877510510134, 'alpha': 0.024159636394718683}. Best is trial 19 with value: 99268.23975471711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Model Tuning Complete. Best Validation RMSE: $99,268.24\n",
      "\n",
      "--- STAGE 1, PART 2: K-Fold Training of Mean Model ---\n",
      "  Mean Model - Fold 1/5...\n",
      "  Mean Model - Fold 2/5...\n",
      "  Mean Model - Fold 3/5...\n",
      "  Mean Model - Fold 4/5...\n",
      "  Mean Model - Fold 5/5...\n",
      "\n",
      "--- STAGE 2: K-Fold Training of Error Model ---\n",
      "  Error Model - Fold 1/5...\n",
      "  Error Model - Fold 2/5...\n",
      "  Error Model - Fold 3/5...\n",
      "  Error Model - Fold 4/5...\n",
      "  Error Model - Fold 5/5...\n",
      "\n",
      "--- Final Asymmetric Calibration ---\n",
      "New Best! a=1.90, b=2.10, Score=305,368.62, Cov=88.65%\n",
      "New Best! a=1.90, b=2.11, Score=305,308.54, Cov=88.72%\n",
      "New Best! a=1.90, b=2.12, Score=305,255.48, Cov=88.78%\n",
      "New Best! a=1.90, b=2.13, Score=305,209.68, Cov=88.86%\n",
      "New Best! a=1.90, b=2.14, Score=305,171.02, Cov=88.93%\n",
      "New Best! a=1.90, b=2.15, Score=305,140.38, Cov=88.99%\n",
      "New Best! a=1.90, b=2.16, Score=305,117.26, Cov=89.07%\n",
      "New Best! a=1.90, b=2.17, Score=305,100.35, Cov=89.12%\n",
      "New Best! a=1.90, b=2.18, Score=305,090.14, Cov=89.18%\n",
      "New Best! a=1.90, b=2.19, Score=305,086.60, Cov=89.24%\n",
      "New Best! a=1.91, b=2.15, Score=305,084.68, Cov=89.08%\n",
      "New Best! a=1.91, b=2.16, Score=305,061.56, Cov=89.15%\n",
      "New Best! a=1.91, b=2.17, Score=305,044.65, Cov=89.20%\n",
      "New Best! a=1.91, b=2.18, Score=305,034.44, Cov=89.27%\n",
      "New Best! a=1.91, b=2.19, Score=305,030.90, Cov=89.33%\n",
      "New Best! a=1.92, b=2.16, Score=305,016.00, Cov=89.24%\n",
      "New Best! a=1.92, b=2.17, Score=304,999.09, Cov=89.30%\n",
      "New Best! a=1.92, b=2.18, Score=304,988.87, Cov=89.36%\n",
      "New Best! a=1.92, b=2.19, Score=304,985.33, Cov=89.42%\n",
      "New Best! a=1.93, b=2.16, Score=304,979.36, Cov=89.33%\n",
      "New Best! a=1.93, b=2.17, Score=304,962.44, Cov=89.39%\n",
      "New Best! a=1.93, b=2.18, Score=304,952.23, Cov=89.45%\n",
      "New Best! a=1.93, b=2.19, Score=304,948.69, Cov=89.51%\n",
      "New Best! a=1.94, b=2.17, Score=304,935.08, Cov=89.47%\n",
      "New Best! a=1.94, b=2.18, Score=304,924.87, Cov=89.53%\n",
      "New Best! a=1.94, b=2.19, Score=304,921.33, Cov=89.59%\n",
      "New Best! a=1.95, b=2.17, Score=304,916.05, Cov=89.56%\n",
      "New Best! a=1.95, b=2.18, Score=304,905.84, Cov=89.62%\n",
      "New Best! a=1.95, b=2.19, Score=304,902.30, Cov=89.68%\n",
      "New Best! a=1.96, b=2.18, Score=304,895.59, Cov=89.70%\n",
      "New Best! a=1.96, b=2.19, Score=304,892.04, Cov=89.76%\n",
      "New Best! a=1.97, b=2.19, Score=304,889.29, Cov=89.83%\n",
      "\n",
      "Grid search complete. Final OOF Score: 304,889.29. Best multipliers: a=1.97, b=2.19\n",
      "\n",
      "Creating final submission file...\n",
      "\n",
      "'submission_ultimate_synthesis.csv' created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>830685.178574</td>\n",
       "      <td>1.036100e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>609813.159248</td>\n",
       "      <td>8.248728e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>437463.070010</td>\n",
       "      <td>6.571075e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>285017.655005</td>\n",
       "      <td>4.073771e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>308928.458281</td>\n",
       "      <td>7.717259e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       pi_lower      pi_upper\n",
       "0   0  830685.178574  1.036100e+06\n",
       "1   1  609813.159248  8.248728e+05\n",
       "2   2  437463.070010  6.571075e+05\n",
       "3   3  285017.655005  4.073771e+05\n",
       "4   4  308928.458281  7.717259e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: TWO-STAGE TUNING, TRAINING, AND SUBMISSION\n",
    "# =============================================================================\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"\\n--- Starting Block 3: Two-Stage Modeling Pipeline ---\")\n",
    "\n",
    "# --- STAGE 1: TUNE AND TRAIN THE MEAN MODEL ---\n",
    "def objective_mean(trial):\n",
    "    train_x, val_x, train_y, val_y = train_test_split(X, y_true, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.08, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 14),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-4, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-4, 10.0, log=True),\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params, n_estimators=1500, random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=50)\n",
    "    model.fit(train_x, train_y, eval_set=[(val_x, val_y)], verbose=False)\n",
    "    preds = model.predict(val_x)\n",
    "    return np.sqrt(mean_squared_error(val_y, preds))\n",
    "\n",
    "print(\"\\n--- STAGE 1, PART 1: Tuning Mean Prediction Model ---\")\n",
    "study_mean = optuna.create_study(direction='minimize')\n",
    "study_mean.optimize(objective_mean, n_trials=N_OPTUNA_TRIALS)\n",
    "best_params_mean = study_mean.best_params\n",
    "print(f\"Mean Model Tuning Complete. Best Validation RMSE: ${study_mean.best_value:,.2f}\")\n",
    "\n",
    "print(\"\\n--- STAGE 1, PART 2: K-Fold Training of Mean Model ---\")\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_mean_preds = np.zeros(len(X))\n",
    "test_mean_preds = np.zeros(len(X_test))\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, df_train['grade'])):\n",
    "    print(f\"  Mean Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**best_params_mean, n_estimators=2000, objective='reg:squarederror', eval_metric='rmse', tree_method='hist', random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(X.iloc[train_idx], y_true.iloc[train_idx], eval_set=[(X.iloc[val_idx], y_true.iloc[val_idx])], verbose=False)\n",
    "    oof_mean_preds[val_idx] = model.predict(X.iloc[val_idx])\n",
    "    test_mean_preds += model.predict(X_test) / N_SPLITS\n",
    "\n",
    "# --- STAGE 2: TRAIN THE ERROR MODEL (No tuning for speed, using good defaults) ---\n",
    "print(\"\\n--- STAGE 2: K-Fold Training of Error Model ---\")\n",
    "error_target = np.abs(y_true - oof_mean_preds)\n",
    "X_for_error = X.copy()\n",
    "X_for_error['mean_pred_oof'] = oof_mean_preds\n",
    "X_test_for_error = X_test.copy()\n",
    "X_test_for_error['mean_pred_oof'] = test_mean_preds\n",
    "params_error = {'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist', 'eta': 0.03, 'max_depth': 7, 'random_state': RANDOM_STATE}\n",
    "oof_error_preds = np.zeros(len(X))\n",
    "test_error_preds = np.zeros(len(X_test))\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_for_error, df_train['grade'])):\n",
    "    print(f\"  Error Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**params_error, n_estimators=1500, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(X_for_error.iloc[train_idx], error_target.iloc[train_idx], eval_set=[(X_for_error.iloc[val_idx], error_target.iloc[val_idx])], verbose=False)\n",
    "    oof_error_preds[val_idx] = model.predict(X_for_error.iloc[val_idx])\n",
    "    test_error_preds += model.predict(X_test_for_error) / N_SPLITS\n",
    "\n",
    "# --- FINAL ASYMMETRIC CALIBRATION AND SUBMISSION ---\n",
    "print(\"\\n--- Final Asymmetric Calibration ---\")\n",
    "oof_error_final = np.clip(oof_error_preds, 0, None) # Ensure error is not negative\n",
    "best_a, best_b, best_metric = 2.0, 2.0, float('inf')\n",
    "for a in np.arange(1.90, 2.31, 0.01):\n",
    "    for b in np.arange(2.10, 2.51, 0.01):\n",
    "        low = oof_mean_preds - oof_error_final * a\n",
    "        high = oof_mean_preds + oof_error_final * b\n",
    "        metric, coverage = winkler_score(y_true, low, high, alpha=COMPETITION_ALPHA, return_coverage=True)\n",
    "        if metric < best_metric:\n",
    "            best_metric = metric\n",
    "            best_a, best_b = a, b\n",
    "            print(f\"New Best! a={best_a:.2f}, b={best_b:.2f}, Score={best_metric:,.2f}, Cov={coverage:.2%}\")\n",
    "print(f\"\\nGrid search complete. Final OOF Score: {best_metric:,.2f}. Best multipliers: a={best_a:.2f}, b={best_b:.2f}\")\n",
    "\n",
    "# --- Create Final Submission ---\n",
    "print(\"\\nCreating final submission file...\")\n",
    "test_error_final = np.clip(test_error_preds, 0, None)\n",
    "final_lower = test_mean_preds - test_error_final * best_a\n",
    "final_upper = test_mean_preds + test_error_final * best_b\n",
    "final_upper = np.maximum(final_lower, final_upper)\n",
    "submission_df = pd.DataFrame({'id': X_test.index, 'pi_lower': final_lower, 'pi_upper': final_upper})\n",
    "submission_df.to_csv('submission_ultimate_synthesis.csv', index=False)\n",
    "print(\"\\n'submission_ultimate_synthesis.csv' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50fcfcff-e8ce-4ddd-a536-c9a957b3bee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Final Submission File with Correct Logic ---\n",
      "Applying best multipliers: a=1.97, b=2.19\n",
      "\n",
      "'submission_ultimate_synthesis_CORRECT_LOGIC.csv' created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>830685.178574</td>\n",
       "      <td>1.036100e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>609813.159248</td>\n",
       "      <td>8.248728e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>437463.070010</td>\n",
       "      <td>6.571075e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>285017.655005</td>\n",
       "      <td>4.073771e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>308928.458281</td>\n",
       "      <td>7.717259e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       pi_lower      pi_upper\n",
       "0  200000  830685.178574  1.036100e+06\n",
       "1  200001  609813.159248  8.248728e+05\n",
       "2  200002  437463.070010  6.571075e+05\n",
       "3  200003  285017.655005  4.073771e+05\n",
       "4  200004  308928.458281  7.717259e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUBMISSION BLOCK (CORRECTED FOR MEAN-ERROR MODEL)\n",
    "# =============================================================================\n",
    "print(\"--- Creating Final Submission File with Correct Logic ---\")\n",
    "\n",
    "# --- Reload the original test set to get the correct index ---\n",
    "original_test_ids = pd.read_csv('./test.csv', usecols=['id'])['id']\n",
    "\n",
    "# --- Use the best found multipliers from your last successful run ---\n",
    "# The grid search found these values were optimal\n",
    "best_a = 1.97\n",
    "best_b = 2.19 # Corrected to the final best value from your log\n",
    "print(f\"Applying best multipliers: a={best_a}, b={best_b}\")\n",
    "\n",
    "# --- Use the prediction arrays directly (THEY ARE ALREADY IN DOLLAR SCALE) ---\n",
    "# The test_mean_preds and test_error_preds arrays are still in memory.\n",
    "# DO NOT apply np.expm1() to them.\n",
    "test_mean_final = test_mean_preds\n",
    "test_error_final = np.clip(test_error_preds, 0, None) # Still clip error to be non-negative\n",
    "\n",
    "final_lower = test_mean_final - test_error_final * best_a\n",
    "final_upper = test_mean_final + test_error_final * best_b\n",
    "\n",
    "# Final safety checks\n",
    "final_upper = np.maximum(final_lower, final_upper)\n",
    "\n",
    "# --- Create submission dataframe with the CORRECT IDs ---\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': original_test_ids,\n",
    "    'pi_lower': final_lower,\n",
    "    'pi_upper': final_upper\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_ultimate_synthesis_CORRECT_LOGIC.csv', index=False)\n",
    "print(\"\\n'submission_ultimate_synthesis_CORRECT_LOGIC.csv' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dbcb61-58dd-4089-96af-81258985b476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b1f4d-300d-4e32-87f2-e28b20397f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
