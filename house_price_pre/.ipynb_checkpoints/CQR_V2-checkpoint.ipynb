{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfcd3989-df30-4f71-861d-138315869c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "# --- Standard Libraries ---\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# --- Machine Learning Libraries ---\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from lightgbm import LGBMRegressor # Our main model for CQR\n",
    "import optuna\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "COMPETITION_ALPHA = 0.1 # This is the alpha for the competition metric (90% interval)\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm', 'view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fef945c-13b7-4c45-a60c-d37c1ba17752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Block 2: Synthesized Feature Engineering ---\n",
      "\n",
      "Synthesized FE complete. Total features: 111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 2: SYNTHESIZED FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"--- Starting Block 2: Synthesized Feature Engineering ---\")\n",
    "\n",
    "def create_synthesized_features(df_train, df_test):\n",
    "    # Combine for consistent processing and reset the index\n",
    "    df_train['is_train'] = 1\n",
    "    df_test['is_train'] = 0\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    all_data = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # A) Brute-Force Numerical Interactions\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # B) Date Features\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['year'] = all_data['sale_date'].dt.year\n",
    "    all_data['month'] = all_data['sale_date'].dt.month\n",
    "    all_data['year_diff'] = all_data['year'] - all_data['year_built']\n",
    "    \n",
    "    # C) TF-IDF Text Features\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # D) Log transform\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "\n",
    "    # E) Final Cleanup\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate final datasets\n",
    "    X = all_data[all_data['is_train'] == 1].drop(columns=['is_train', 'sale_price'])\n",
    "    X_test = all_data[all_data['is_train'] == 0].drop(columns=['is_train', 'sale_price'])\n",
    "    \n",
    "    return X, X_test, test_ids # Return test_ids for the final submission\n",
    "\n",
    "# --- Run Feature Engineering ---\n",
    "X, X_test, test_ids = create_synthesized_features(df_train, df_test)\n",
    "print(f\"\\nSynthesized FE complete. Total features: {X.shape[1]}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94f8d9-f7ac-415a-88b9-96fe3289eb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting CQR Pipeline with Hyperparameter Tuning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 14:05:05,543] A new study created in memory with name: no-name-b912af93-0034-4abe-9923-2e863196989d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Tuning LightGBM Quantile Models with K-Fold Aware Optuna...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-09 14:06:44,634] Trial 0 finished with value: 341350.43243549863 and parameters: {'learning_rate': 0.029163306289483194, 'n_estimators': 1840, 'num_leaves': 24, 'max_depth': 11, 'subsample': 0.8841312008294709, 'colsample_bytree': 0.7180920315446796}. Best is trial 0 with value: 341350.43243549863.\n",
      "[I 2025-07-09 14:08:40,568] Trial 1 finished with value: 338143.326838844 and parameters: {'learning_rate': 0.040216738030709735, 'n_estimators': 2198, 'num_leaves': 28, 'max_depth': 12, 'subsample': 0.8269627113129824, 'colsample_bytree': 0.7910258013563782}. Best is trial 1 with value: 338143.326838844.\n",
      "[I 2025-07-09 14:10:17,615] Trial 2 finished with value: 338286.5339959337 and parameters: {'learning_rate': 0.043492387896136894, 'n_estimators': 1902, 'num_leaves': 28, 'max_depth': 8, 'subsample': 0.8069694636860422, 'colsample_bytree': 0.8314257543816337}. Best is trial 1 with value: 338143.326838844.\n",
      "[I 2025-07-09 14:11:39,071] Trial 3 finished with value: 343820.80441999936 and parameters: {'learning_rate': 0.03187280538088589, 'n_estimators': 1518, 'num_leaves': 21, 'max_depth': 9, 'subsample': 0.7929677693911984, 'colsample_bytree': 0.7927600436215215}. Best is trial 1 with value: 338143.326838844.\n",
      "[I 2025-07-09 14:13:45,973] Trial 4 finished with value: 337586.74049346545 and parameters: {'learning_rate': 0.03578500525578497, 'n_estimators': 2438, 'num_leaves': 26, 'max_depth': 7, 'subsample': 0.7426127863387518, 'colsample_bytree': 0.8191711313163268}. Best is trial 4 with value: 337586.74049346545.\n",
      "[I 2025-07-09 14:16:07,193] Trial 5 finished with value: 338350.55821656267 and parameters: {'learning_rate': 0.024022104849206576, 'n_estimators': 2546, 'num_leaves': 34, 'max_depth': 10, 'subsample': 0.8061212140748114, 'colsample_bytree': 0.9248402029527136}. Best is trial 4 with value: 337586.74049346545.\n",
      "[I 2025-07-09 14:18:14,878] Trial 6 finished with value: 338456.00210726575 and parameters: {'learning_rate': 0.030163376412980843, 'n_estimators': 2300, 'num_leaves': 27, 'max_depth': 7, 'subsample': 0.8692131352685271, 'colsample_bytree': 0.7716043168698818}. Best is trial 4 with value: 337586.74049346545.\n",
      "[I 2025-07-09 14:20:45,458] Trial 7 finished with value: 340856.9721620453 and parameters: {'learning_rate': 0.02360685568694292, 'n_estimators': 2094, 'num_leaves': 25, 'max_depth': 10, 'subsample': 0.8382916610513588, 'colsample_bytree': 0.7309598101262749}. Best is trial 4 with value: 337586.74049346545.\n",
      "[I 2025-07-09 14:22:52,586] Trial 8 finished with value: 339491.7603155725 and parameters: {'learning_rate': 0.025459849603298078, 'n_estimators': 2164, 'num_leaves': 31, 'max_depth': 11, 'subsample': 0.8961194931267848, 'colsample_bytree': 0.7544147152030388}. Best is trial 4 with value: 337586.74049346545.\n",
      "[I 2025-07-09 14:25:13,730] Trial 9 finished with value: 335914.90697335545 and parameters: {'learning_rate': 0.04427348877167297, 'n_estimators': 2459, 'num_leaves': 49, 'max_depth': 10, 'subsample': 0.835917946834307, 'colsample_bytree': 0.7121691943942371}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 14:28:17,288] Trial 10 finished with value: 340457.77969207504 and parameters: {'learning_rate': 0.01070955922631208, 'n_estimators': 2908, 'num_leaves': 49, 'max_depth': 9, 'subsample': 0.7156153212947577, 'colsample_bytree': 0.883873504026263}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 14:30:34,791] Trial 11 finished with value: 337249.8192553771 and parameters: {'learning_rate': 0.048467685410240005, 'n_estimators': 2592, 'num_leaves': 43, 'max_depth': 7, 'subsample': 0.7361459831766948, 'colsample_bytree': 0.8518572864595073}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 14:33:13,902] Trial 12 finished with value: 337585.1428849936 and parameters: {'learning_rate': 0.04943180862705564, 'n_estimators': 2724, 'num_leaves': 46, 'max_depth': 8, 'subsample': 0.9440147334575639, 'colsample_bytree': 0.873662090540372}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 14:35:42,454] Trial 13 finished with value: 337087.07233340805 and parameters: {'learning_rate': 0.04982098814064943, 'n_estimators': 2662, 'num_leaves': 42, 'max_depth': 8, 'subsample': 0.770780103263897, 'colsample_bytree': 0.8675936343457928}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 14:38:30,621] Trial 14 finished with value: 337007.8577822974 and parameters: {'learning_rate': 0.0435009481625739, 'n_estimators': 2978, 'num_leaves': 40, 'max_depth': 8, 'subsample': 0.770130693079495, 'colsample_bytree': 0.9321730276862169}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 14:41:15,855] Trial 15 finished with value: 337050.9505840685 and parameters: {'learning_rate': 0.042211862677475884, 'n_estimators': 2949, 'num_leaves': 38, 'max_depth': 9, 'subsample': 0.7622708282649323, 'colsample_bytree': 0.9422198912085226}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 14:44:07,725] Trial 16 finished with value: 336388.79004257143 and parameters: {'learning_rate': 0.037260227451513056, 'n_estimators': 2801, 'num_leaves': 49, 'max_depth': 11, 'subsample': 0.849759973935681, 'colsample_bytree': 0.9052398189080887}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 14:46:51,932] Trial 17 finished with value: 336490.7094829612 and parameters: {'learning_rate': 0.03754401375195052, 'n_estimators': 2410, 'num_leaves': 50, 'max_depth': 12, 'subsample': 0.9218739668267789, 'colsample_bytree': 0.9002861871067926}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 14:49:51,269] Trial 18 finished with value: 337344.6171203305 and parameters: {'learning_rate': 0.01731560104664731, 'n_estimators': 2779, 'num_leaves': 46, 'max_depth': 11, 'subsample': 0.8518660550496948, 'colsample_bytree': 0.7019557986212616}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 14:52:55,674] Trial 19 finished with value: 336644.7150555332 and parameters: {'learning_rate': 0.03409065385524904, 'n_estimators': 2777, 'num_leaves': 46, 'max_depth': 10, 'subsample': 0.8510093680854232, 'colsample_bytree': 0.8987820897786263}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 14:55:37,684] Trial 20 finished with value: 336900.9362412208 and parameters: {'learning_rate': 0.03881521923827188, 'n_estimators': 2383, 'num_leaves': 36, 'max_depth': 11, 'subsample': 0.9133776004564018, 'colsample_bytree': 0.8261132608178231}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 14:58:28,499] Trial 21 finished with value: 336118.4738241007 and parameters: {'learning_rate': 0.03729299118597723, 'n_estimators': 2492, 'num_leaves': 50, 'max_depth': 12, 'subsample': 0.940557680015527, 'colsample_bytree': 0.9009959150995722}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 15:01:20,213] Trial 22 finished with value: 336605.4694026307 and parameters: {'learning_rate': 0.044947355780350626, 'n_estimators': 2529, 'num_leaves': 48, 'max_depth': 12, 'subsample': 0.9424525439636778, 'colsample_bytree': 0.9174334410649441}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 15:04:09,478] Trial 23 finished with value: 336414.5255670791 and parameters: {'learning_rate': 0.034534075451658285, 'n_estimators': 2816, 'num_leaves': 43, 'max_depth': 12, 'subsample': 0.8723151255576727, 'colsample_bytree': 0.9490722713093476}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 15:07:00,734] Trial 24 finished with value: 336028.91465152765 and parameters: {'learning_rate': 0.039713982791201405, 'n_estimators': 2647, 'num_leaves': 50, 'max_depth': 11, 'subsample': 0.9095340525610108, 'colsample_bytree': 0.8520886609236036}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 15:09:01,165] Trial 25 finished with value: 337072.44896339683 and parameters: {'learning_rate': 0.04616254760368429, 'n_estimators': 2035, 'num_leaves': 46, 'max_depth': 10, 'subsample': 0.9180597401396458, 'colsample_bytree': 0.8498752938887308}. Best is trial 9 with value: 335914.90697335545.\n",
      "[I 2025-07-09 15:11:20,213] Trial 26 finished with value: 335250.9372955736 and parameters: {'learning_rate': 0.04056493499322852, 'n_estimators': 2345, 'num_leaves': 50, 'max_depth': 11, 'subsample': 0.8959903628403271, 'colsample_bytree': 0.8050021339222921}. Best is trial 26 with value: 335250.9372955736.\n",
      "[I 2025-07-09 15:13:29,993] Trial 27 finished with value: 335530.0401416304 and parameters: {'learning_rate': 0.04077701292696813, 'n_estimators': 2272, 'num_leaves': 44, 'max_depth': 11, 'subsample': 0.8969768776108862, 'colsample_bytree': 0.8023165131304699}. Best is trial 26 with value: 335250.9372955736.\n",
      "[I 2025-07-09 15:16:21,710] Trial 28 finished with value: 336072.3776072656 and parameters: {'learning_rate': 0.04689894052720384, 'n_estimators': 2310, 'num_leaves': 41, 'max_depth': 10, 'subsample': 0.882691025177246, 'colsample_bytree': 0.7520458798882349}. Best is trial 26 with value: 335250.9372955736.\n",
      "[I 2025-07-09 15:18:14,159] Trial 29 finished with value: 336767.67630009574 and parameters: {'learning_rate': 0.0413551017297425, 'n_estimators': 1728, 'num_leaves': 44, 'max_depth': 11, 'subsample': 0.896912871809956, 'colsample_bytree': 0.7953910867163062}. Best is trial 26 with value: 335250.9372955736.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Quantile Model Tuning Complete. Best Avg Winkler Score: $335,250.94\n",
      "\n",
      "# Final CQR Training with Tuned Hyperparameters...\n",
      "\n",
      "# FOLD 1/5\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: CQR WITH K-FOLD AWARE OPTUNA TUNING\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting CQR Pipeline with Hyperparameter Tuning ---\")\n",
    "\n",
    "# --- Initialize K-Fold and Data ---\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "grade_for_stratify = pd.read_csv(DATA_PATH + 'dataset.csv')['grade']\n",
    "LOWER_ALPHA = 0.05\n",
    "UPPER_ALPHA = 0.95\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: K-FOLD AWARE TUNING FOR QUANTILE MODELS\n",
    "# ==============================================================================\n",
    "print(\"\\n# Tuning LightGBM Quantile Models with K-Fold Aware Optuna...\")\n",
    "\n",
    "def objective_quantile_kfold(trial):\n",
    "    # We will tune a single set of parameters that should work well for both quantiles\n",
    "    params = {\n",
    "        'objective': 'quantile', 'metric': 'quantile', 'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1, 'verbose': -1,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1500, 3000),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 50),\n",
    "        'max_depth': trial.suggest_int('max_depth', 7, 12),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.95),\n",
    "    }\n",
    "    \n",
    "    # We will evaluate the parameters based on the average Winkler score\n",
    "    oof_winkler_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, grade_for_stratify):\n",
    "        X_train_fold, X_calib_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_calib_fold = y_true.iloc[train_idx], y_true.iloc[val_idx]\n",
    "\n",
    "        # Train lower model\n",
    "        model_lower = LGBMRegressor(**params, alpha=LOWER_ALPHA)\n",
    "        model_lower.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Train upper model\n",
    "        model_upper = LGBMRegressor(**params, alpha=UPPER_ALPHA)\n",
    "        model_upper.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Get initial predictions\n",
    "        calib_pred_lower = model_lower.predict(X_calib_fold)\n",
    "        calib_pred_upper = model_upper.predict(X_calib_fold)\n",
    "        \n",
    "        # Calculate scores and correction\n",
    "        nonconformity_scores = np.maximum(calib_pred_lower - y_calib_fold, y_calib_fold - calib_pred_upper)\n",
    "        q_correction = np.quantile(nonconformity_scores, (1 - COMPETITION_ALPHA) * (1 + 1/len(y_calib_fold)))\n",
    "        \n",
    "        # Apply correction and calculate Winkler score for this fold\n",
    "        fold_lower = calib_pred_lower - q_correction\n",
    "        fold_upper = calib_pred_upper + q_correction\n",
    "        fold_score, _ = winkler_score(y_calib_fold, fold_lower, fold_upper, return_coverage=True)\n",
    "        oof_winkler_scores.append(fold_score)\n",
    "        \n",
    "    return np.mean(oof_winkler_scores)\n",
    "\n",
    "# --- Run the Optuna Study ---\n",
    "N_OPTUNA_TRIALS_CQR = 30 # Start with 30, this is very computationally expensive\n",
    "study_quantile = optuna.create_study(direction='minimize')\n",
    "study_quantile.optimize(objective_quantile_kfold, n_trials=N_OPTUNA_TRIALS_CQR)\n",
    "\n",
    "best_params_quantile = study_quantile.best_params\n",
    "print(f\"\\n# Quantile Model Tuning Complete. Best Avg Winkler Score: ${study_quantile.best_value:,.2f}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: FINAL CQR TRAINING WITH TUNED PARAMETERS\n",
    "# ==============================================================================\n",
    "print(\"\\n# Final CQR Training with Tuned Hyperparameters...\")\n",
    "\n",
    "# --- Initialize arrays to store OOF and Test predictions ---\n",
    "oof_lower = np.zeros(len(X))\n",
    "oof_upper = np.zeros(len(X))\n",
    "final_test_lower = np.zeros(len(X_test))\n",
    "final_test_upper = np.zeros(len(X_test))\n",
    "\n",
    "# Combine the best tuned params with the fixed ones\n",
    "final_params_quantile = {\n",
    "    'objective': 'quantile', 'metric': 'quantile', 'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1, 'verbose': -1,\n",
    "    **best_params_quantile\n",
    "}\n",
    "\n",
    "# --- Main K-Fold Loop ---\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, grade_for_stratify)):\n",
    "    print(f\"\\n# FOLD {fold+1}/{N_SPLITS}\")\n",
    "    X_train_fold, X_calib_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_calib_fold = y_true.iloc[train_idx], y_true.iloc[val_idx]\n",
    "\n",
    "    # Train Lower Model with Tuned Params\n",
    "    model_lower = LGBMRegressor(**final_params_quantile, alpha=LOWER_ALPHA)\n",
    "    model_lower.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Train Upper Model with Tuned Params\n",
    "    model_upper = LGBMRegressor(**final_params_quantile, alpha=UPPER_ALPHA)\n",
    "    model_upper.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    calib_pred_lower = model_lower.predict(X_calib_fold)\n",
    "    calib_pred_upper = model_upper.predict(X_calib_fold)\n",
    "    \n",
    "    nonconformity_scores = np.maximum(calib_pred_lower - y_calib_fold, y_calib_fold - calib_pred_upper)\n",
    "    q_correction = np.quantile(nonconformity_scores, (1 - COMPETITION_ALPHA) * (1 + 1/len(y_calib_fold)))\n",
    "    print(f\"  Correction value 'q' for this fold: {q_correction:.2f}\")\n",
    "\n",
    "    oof_lower[val_idx] = calib_pred_lower - q_correction\n",
    "    oof_upper[val_idx] = calib_pred_upper + q_correction\n",
    "    final_test_lower += (model_lower.predict(X_test) - q_correction) / N_SPLITS\n",
    "    final_test_upper += (model_upper.predict(X_test) + q_correction) / N_SPLITS\n",
    "\n",
    "# --- Calculate and Display Final OOF Score ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"# CQR K-Fold Training Complete.\")\n",
    "final_winkler_score, final_coverage = winkler_score(y_true, oof_lower, oof_upper, return_coverage=True)\n",
    "print(f\"# Final OOF Winkler Score: {final_winkler_score:,.2f}\")\n",
    "print(f\"# Final OOF Coverage: {final_coverage:.2%}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69221943-bb40-45f8-9bfe-5e6f3f0f8f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 4: FINAL SUBMISSION\n",
    "# =============================================================================\n",
    "print(\"\\n--- STAGE 3: Creating Final Submission File ---\")\n",
    "\n",
    "# Ensure the upper bound is always greater than or equal to the lower bound\n",
    "final_test_upper = np.maximum(final_test_lower, final_test_upper)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'pi_lower': final_test_lower,\n",
    "    'pi_upper': final_test_upper\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_cqr_v1.csv', index=False)\n",
    "print(\"\\n'submission_cqr_v1.csv' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a918ee5-be7d-441a-9041-425def395fca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
