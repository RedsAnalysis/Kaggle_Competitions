{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8192dd7b-5d58-4368-9c9d-4903a8485ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm', 'view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a955ee5d-37c7-4628-afc7-39bfddcbca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Block 2: Synthesized Feature Engineering ---\n",
      "Creating advanced date features...\n",
      "Creating domain-specific ratio and interaction features...\n",
      "Creating brute-force numerical interaction features...\n",
      "Creating polynomial features for key predictors...\n",
      "Creating TF-IDF features for text columns...\n",
      "Finalizing feature set...\n",
      "\n",
      "Synthesized FE complete. Total features: 126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 2: SYNTHESIZED FEATURE ENGINEERING (CORRECTED)\n",
    "# =============================================================================\n",
    "print(\"--- Starting Block 2: Synthesized Feature Engineering ---\")\n",
    "\n",
    "def create_synthesized_features(df_train, df_test):\n",
    "    # Combine for consistent processing and reset the index\n",
    "    df_train['is_train'] = 1\n",
    "    df_test['is_train'] = 0\n",
    "    # Store the original id for later, as reset_index will remove it\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    all_data = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # --- A) Advanced Date Features ---\n",
    "    print(\"Creating advanced date features...\")\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['sale_year'] = all_data['sale_date'].dt.year\n",
    "    all_data['sale_month'] = all_data['sale_date'].dt.month\n",
    "    all_data['sale_dayofyear'] = all_data['sale_date'].dt.dayofyear\n",
    "    # Cyclical features for seasonality\n",
    "    all_data['month_sin'] = np.sin(2 * np.pi * all_data['sale_month']/12)\n",
    "    all_data['month_cos'] = np.cos(2 * np.pi * all_data['sale_month']/12)\n",
    "    \n",
    "    \n",
    "    # --- B) Domain-Specific Ratio and Interaction Features ---\n",
    "    print(\"Creating domain-specific ratio and interaction features...\")\n",
    "    \n",
    "    # Property Age\n",
    "    all_data['age'] = all_data['sale_year'] - all_data['year_built']\n",
    "    all_data['age'] = all_data['age'].apply(lambda x: max(x, 0)) # Can't have negative age\n",
    "    \n",
    "    # Value Ratios (captures land vs. improvement value)\n",
    "    # Use a small epsilon to avoid division by zero\n",
    "    epsilon = 1e-6\n",
    "    all_data['imp_to_land_val_ratio'] = all_data['imp_val'] / (all_data['land_val'] + epsilon)\n",
    "    all_data['val_per_sqft'] = all_data['imp_val'] / (all_data['sqft'] + epsilon)\n",
    "    \n",
    "    # Space Ratios\n",
    "    all_data['lot_to_house_ratio'] = all_data['sqft_lot'] / (all_data['sqft'] + epsilon)\n",
    "    all_data['sqft_vs_sqft1'] = all_data['sqft'] - all_data['sqft_1'] # Difference in size measurement\n",
    "    \n",
    "    # Grade Interactions (very powerful)\n",
    "    all_data['grade_x_sqft'] = all_data['grade'] * all_data['sqft']\n",
    "    all_data['grade_x_age'] = all_data['grade'] * all_data['age']\n",
    "    all_data['grade_x_imp_val'] = all_data['grade'] * all_data['imp_val']\n",
    "\n",
    "    # --- A) Brute-Force Numerical Interactions ---\n",
    "    print(\"Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "    \n",
    "    \n",
    "    # --- C) Polynomial Features for Key Predictors ---\n",
    "    print(\"Creating polynomial features for key predictors...\")\n",
    "    for col in ['area', 'grade', 'age', 'sqft', 'imp_val']:\n",
    "        all_data[f'{col}_cub'] = all_data[col]**3\n",
    "\n",
    "    # --- C) TF-IDF Text Features ---\n",
    "    print(\"Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "\n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        # This concat will now work because both have a simple 0-based index\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # --- D) Log transform some of the new interaction features ---\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            # Add a small constant to avoid log(0)\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "            \n",
    "    # --- E) Final Cleanup ---\n",
    "    print(\"Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate final datasets\n",
    "    X = all_data[all_data['is_train'] == 1].drop(columns=['is_train', 'sale_price'])\n",
    "    X_test = all_data[all_data['is_train'] == 0].drop(columns=['is_train', 'sale_price'])\n",
    "    \n",
    "    # Restore the original 'id' as the index\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    \n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    return X, X_test\n",
    "\n",
    "# We need to re-run this from the original dataframes\n",
    "X, X_test = create_synthesized_features(df_train, df_test)\n",
    "\n",
    "print(f\"\\nSynthesized FE complete. Total features: {X.shape[1]}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e89910-15a3-4262-89ed-1ccc01ca61cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-08 21:30:34,304] A new study created in memory with name: no-name-14016501-8c0a-4e2a-bed7-3a7ae299cbf1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 3: Two-Stage Modeling Pipeline ---\n",
      "\n",
      "# STAGE 1, PART 1: Tuning Mean Prediction Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-08 21:31:40,169] Trial 0 finished with value: 100470.74607068468 and parameters: {'eta': 0.021137828486356834, 'max_depth': 10, 'subsample': 0.8394733332069202, 'colsample_bytree': 0.8676485885208814, 'lambda': 2.448142734214261, 'alpha': 3.764944983255964e-05, 'min_child_weight': 3}. Best is trial 0 with value: 100470.74607068468.\n",
      "[I 2025-07-08 21:32:32,316] Trial 1 finished with value: 100119.33743288556 and parameters: {'eta': 0.02881540129765727, 'max_depth': 9, 'subsample': 0.72873596380745, 'colsample_bytree': 0.8092045551351673, 'lambda': 2.908707509797906, 'alpha': 0.0002839704281298098, 'min_child_weight': 1}. Best is trial 1 with value: 100119.33743288556.\n",
      "[I 2025-07-08 21:33:21,988] Trial 2 finished with value: 99993.28233436485 and parameters: {'eta': 0.020469782665619496, 'max_depth': 9, 'subsample': 0.7672850375627377, 'colsample_bytree': 0.8720928514378324, 'lambda': 4.458338077762658, 'alpha': 3.2738100807219774e-05, 'min_child_weight': 3}. Best is trial 2 with value: 99993.28233436485.\n",
      "[I 2025-07-08 21:34:02,365] Trial 3 finished with value: 99780.47456291236 and parameters: {'eta': 0.028314102443306968, 'max_depth': 8, 'subsample': 0.7453919121573751, 'colsample_bytree': 0.7500919744258256, 'lambda': 3.757357433518134, 'alpha': 2.1674607233184884e-05, 'min_child_weight': 1}. Best is trial 3 with value: 99780.47456291236.\n",
      "[I 2025-07-08 21:34:52,587] Trial 4 finished with value: 100711.5243455286 and parameters: {'eta': 0.03111431981662916, 'max_depth': 9, 'subsample': 0.7617330729140155, 'colsample_bytree': 0.8806499653575135, 'lambda': 2.7100327996497713, 'alpha': 8.039404793436721e-05, 'min_child_weight': 3}. Best is trial 3 with value: 99780.47456291236.\n",
      "[I 2025-07-08 21:35:41,212] Trial 5 finished with value: 99938.60771493668 and parameters: {'eta': 0.038263331159400366, 'max_depth': 9, 'subsample': 0.8329891244565771, 'colsample_bytree': 0.7451290287005663, 'lambda': 4.257430428287985, 'alpha': 2.9355333109253274e-05, 'min_child_weight': 3}. Best is trial 3 with value: 99780.47456291236.\n",
      "[I 2025-07-08 21:36:20,609] Trial 6 finished with value: 99763.42704618763 and parameters: {'eta': 0.03315862541172469, 'max_depth': 8, 'subsample': 0.7646958765196733, 'colsample_bytree': 0.8378115361596744, 'lambda': 3.03802885500065, 'alpha': 2.4804603047258365e-05, 'min_child_weight': 3}. Best is trial 6 with value: 99763.42704618763.\n",
      "[I 2025-07-08 21:36:59,042] Trial 7 finished with value: 99892.88343020238 and parameters: {'eta': 0.022959872885428417, 'max_depth': 8, 'subsample': 0.7669574079926096, 'colsample_bytree': 0.7536380928270433, 'lambda': 3.2552373580655214, 'alpha': 3.964298606566309e-05, 'min_child_weight': 3}. Best is trial 6 with value: 99763.42704618763.\n",
      "[I 2025-07-08 21:37:52,393] Trial 8 finished with value: 101519.10102044836 and parameters: {'eta': 0.04974883856317866, 'max_depth': 10, 'subsample': 0.8577641476556126, 'colsample_bytree': 0.8719904631774342, 'lambda': 2.999470059376801, 'alpha': 4.0307938999706995e-05, 'min_child_weight': 1}. Best is trial 6 with value: 99763.42704618763.\n",
      "[I 2025-07-08 21:38:38,222] Trial 9 finished with value: 100515.92224120515 and parameters: {'eta': 0.04760618214743771, 'max_depth': 9, 'subsample': 0.827954144334689, 'colsample_bytree': 0.8545014666611697, 'lambda': 2.236778120867647, 'alpha': 1.8299841974948833e-05, 'min_child_weight': 2}. Best is trial 6 with value: 99763.42704618763.\n",
      "[I 2025-07-08 21:39:16,665] Trial 10 finished with value: 100052.4764710999 and parameters: {'eta': 0.03974514810324433, 'max_depth': 8, 'subsample': 0.7948707164327055, 'colsample_bytree': 0.8169783580638432, 'lambda': 3.6574983961544754, 'alpha': 1.1027561015717426e-05, 'min_child_weight': 2}. Best is trial 6 with value: 99763.42704618763.\n",
      "[I 2025-07-08 21:39:56,403] Trial 11 finished with value: 99875.19379705854 and parameters: {'eta': 0.02979039012565559, 'max_depth': 8, 'subsample': 0.7286883033792964, 'colsample_bytree': 0.7922628189790442, 'lambda': 3.8124937075661416, 'alpha': 0.00010030127825496452, 'min_child_weight': 1}. Best is trial 6 with value: 99763.42704618763.\n",
      "[I 2025-07-08 21:40:36,048] Trial 12 finished with value: 100077.0263347188 and parameters: {'eta': 0.0348147961981881, 'max_depth': 8, 'subsample': 0.7915745462642728, 'colsample_bytree': 0.8387573945377977, 'lambda': 3.6633437646340745, 'alpha': 1.1208119240861038e-05, 'min_child_weight': 2}. Best is trial 6 with value: 99763.42704618763.\n",
      "[I 2025-07-08 21:41:14,478] Trial 13 finished with value: 99821.36749213567 and parameters: {'eta': 0.026372261967534078, 'max_depth': 8, 'subsample': 0.7494648651880041, 'colsample_bytree': 0.7731945838488143, 'lambda': 3.39329674685042, 'alpha': 0.00013860776079836822, 'min_child_weight': 2}. Best is trial 6 with value: 99763.42704618763.\n",
      "[I 2025-07-08 21:41:53,870] Trial 14 finished with value: 99852.50690894044 and parameters: {'eta': 0.03374277965063812, 'max_depth': 8, 'subsample': 0.8876416102161324, 'colsample_bytree': 0.835680769695654, 'lambda': 4.047711076984851, 'alpha': 1.9390955754708668e-05, 'min_child_weight': 1}. Best is trial 6 with value: 99763.42704618763.\n",
      "[I 2025-07-08 21:42:33,231] Trial 15 finished with value: 99545.97217366456 and parameters: {'eta': 0.0406473783734478, 'max_depth': 8, 'subsample': 0.7408001626441578, 'colsample_bytree': 0.7739857255535075, 'lambda': 3.3288171466220002, 'alpha': 1.991458770103446e-05, 'min_child_weight': 2}. Best is trial 15 with value: 99545.97217366456.\n",
      "[I 2025-07-08 21:43:13,103] Trial 16 finished with value: 100316.98879053337 and parameters: {'eta': 0.04515860702457086, 'max_depth': 8, 'subsample': 0.7778452239307331, 'colsample_bytree': 0.7824135100410667, 'lambda': 3.2247713533715707, 'alpha': 5.8305313512109216e-05, 'min_child_weight': 2}. Best is trial 15 with value: 99545.97217366456.\n",
      "[I 2025-07-08 21:43:51,156] Trial 17 finished with value: 100110.34679792094 and parameters: {'eta': 0.0411208613190343, 'max_depth': 8, 'subsample': 0.7218641063075436, 'colsample_bytree': 0.8083789456749373, 'lambda': 2.692526922556425, 'alpha': 1.5779598290201828e-05, 'min_child_weight': 2}. Best is trial 15 with value: 99545.97217366456.\n",
      "[I 2025-07-08 21:44:41,909] Trial 18 finished with value: 100953.7773042693 and parameters: {'eta': 0.04352783259453615, 'max_depth': 10, 'subsample': 0.8167172975997384, 'colsample_bytree': 0.8308507969093866, 'lambda': 3.427980669200759, 'alpha': 0.0001641114371763811, 'min_child_weight': 3}. Best is trial 15 with value: 99545.97217366456.\n",
      "[I 2025-07-08 21:45:33,623] Trial 19 finished with value: 100337.67754936327 and parameters: {'eta': 0.0375948573514026, 'max_depth': 9, 'subsample': 0.7428610098761282, 'colsample_bytree': 0.7971188177405163, 'lambda': 3.065628567030174, 'alpha': 5.621567145424988e-05, 'min_child_weight': 2}. Best is trial 15 with value: 99545.97217366456.\n",
      "[I 2025-07-08 21:46:22,678] Trial 20 finished with value: 100268.25076762833 and parameters: {'eta': 0.0325742286570352, 'max_depth': 9, 'subsample': 0.7821439853471852, 'colsample_bytree': 0.7645178166251215, 'lambda': 2.7042602946924736, 'alpha': 1.4578095614614913e-05, 'min_child_weight': 3}. Best is trial 15 with value: 99545.97217366456.\n",
      "[I 2025-07-08 21:47:01,173] Trial 21 finished with value: 99632.18885480736 and parameters: {'eta': 0.02601301017207608, 'max_depth': 8, 'subsample': 0.7480148610434646, 'colsample_bytree': 0.7636212091779387, 'lambda': 3.9937606441088427, 'alpha': 2.366219385128742e-05, 'min_child_weight': 1}. Best is trial 15 with value: 99545.97217366456.\n",
      "[I 2025-07-08 21:47:40,571] Trial 22 finished with value: 99793.748261101 and parameters: {'eta': 0.02642061260523699, 'max_depth': 8, 'subsample': 0.7551892705592245, 'colsample_bytree': 0.765234682918617, 'lambda': 3.9927814510188138, 'alpha': 2.5306877324133214e-05, 'min_child_weight': 1}. Best is trial 15 with value: 99545.97217366456.\n",
      "[I 2025-07-08 21:48:20,658] Trial 23 finished with value: 99896.94786128352 and parameters: {'eta': 0.036505750207838306, 'max_depth': 8, 'subsample': 0.7380253545639635, 'colsample_bytree': 0.7828791720710486, 'lambda': 3.4950500056071254, 'alpha': 5.2230058269847924e-05, 'min_child_weight': 1}. Best is trial 15 with value: 99545.97217366456.\n",
      "[I 2025-07-08 21:48:59,294] Trial 24 finished with value: 99786.75503292007 and parameters: {'eta': 0.0243939134085749, 'max_depth': 8, 'subsample': 0.803757513418182, 'colsample_bytree': 0.8496492797347731, 'lambda': 4.112870450853794, 'alpha': 2.4591312629228646e-05, 'min_child_weight': 2}. Best is trial 15 with value: 99545.97217366456.\n",
      "[I 2025-07-08 21:49:37,660] Trial 25 finished with value: 99872.01537968482 and parameters: {'eta': 0.0442479828879111, 'max_depth': 8, 'subsample': 0.7760087870782494, 'colsample_bytree': 0.7414403122998736, 'lambda': 4.485122336883103, 'alpha': 1.3887284712841317e-05, 'min_child_weight': 2}. Best is trial 15 with value: 99545.97217366456.\n",
      "[I 2025-07-08 21:50:17,763] Trial 26 finished with value: 100065.67315518344 and parameters: {'eta': 0.041261395156959, 'max_depth': 8, 'subsample': 0.7560428188138599, 'colsample_bytree': 0.8179206085369818, 'lambda': 3.0895131340181954, 'alpha': 0.0004351288513569294, 'min_child_weight': 1}. Best is trial 15 with value: 99545.97217366456.\n",
      "[I 2025-07-08 21:50:55,914] Trial 27 finished with value: 100129.68742585788 and parameters: {'eta': 0.03209881228001255, 'max_depth': 8, 'subsample': 0.7358842625329667, 'colsample_bytree': 0.7936286083577372, 'lambda': 3.8832666557573616, 'alpha': 2.8017513297908917e-05, 'min_child_weight': 2}. Best is trial 15 with value: 99545.97217366456.\n",
      "[I 2025-07-08 21:51:44,556] Trial 28 finished with value: 100007.62850902925 and parameters: {'eta': 0.02641398303798845, 'max_depth': 9, 'subsample': 0.7870930202313137, 'colsample_bytree': 0.759280373235285, 'lambda': 2.1093018094804608, 'alpha': 1.0018432757442342e-05, 'min_child_weight': 3}. Best is trial 15 with value: 99545.97217366456.\n",
      "[I 2025-07-08 21:52:32,211] Trial 29 finished with value: 100056.01759014797 and parameters: {'eta': 0.02288382872365453, 'max_depth': 9, 'subsample': 0.7707475907560353, 'colsample_bytree': 0.7777510180400612, 'lambda': 2.4602030398504184, 'alpha': 4.160175015523815e-05, 'min_child_weight': 3}. Best is trial 15 with value: 99545.97217366456.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Mean Model Tuning Complete. Best Validation RMSE: $99,545.97\n",
      "\n",
      "# STAGE 1, PART 2: K-Fold Training of Mean Model...\n",
      "  Mean Model - Fold 1/5...\n",
      "  Mean Model - Fold 2/5...\n",
      "  Mean Model - Fold 3/5...\n",
      "  Mean Model - Fold 4/5...\n",
      "  Mean Model - Fold 5/5...\n",
      "\n",
      "# Mean model K-Fold training complete.\n",
      "# Final OOF RMSE for Mean Model: $100,467.33\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: TWO-STAGE TUNING, TRAINING, AND SUBMISSION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Starting Block 3: Two-Stage Modeling Pipeline ---\")\n",
    "\n",
    "# --- STAGE 1, PART 1: Tuning Mean Prediction Model ---\n",
    "print(\"\\n# STAGE 1, PART 1: Tuning Mean Prediction Model...\")\n",
    "def objective_mean(trial):\n",
    "    train_x, val_x, train_y, val_y = train_test_split(X, y_true, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    params = {\n",
    "            'objective': 'reg:squarederror', \n",
    "            'eval_metric': 'rmse', \n",
    "            'tree_method':'hist',\n",
    "            'eta': trial.suggest_float('eta', 0.02, 0.05),\n",
    "            'max_depth': trial.suggest_int('max_depth', 8, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.72, 0.89),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.74, 0.89),\n",
    "            'lambda': trial.suggest_float('lambda', 2.1, 4.5),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-5, 5e-4, log=True),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 3)\n",
    "        }\n",
    "    \n",
    "    model = xgb.XGBRegressor(**params, n_estimators=2500, random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(train_x, train_y, eval_set=[(val_x, val_y)], verbose=False)\n",
    "    preds = model.predict(val_x)\n",
    "    return np.sqrt(mean_squared_error(val_y, preds))\n",
    "\n",
    "study_mean = optuna.create_study(direction='minimize')\n",
    "study_mean.optimize(objective_mean, n_trials=N_OPTUNA_TRIALS)\n",
    "best_params_mean = study_mean.best_params\n",
    "print(f\"# Mean Model Tuning Complete. Best Validation RMSE: ${study_mean.best_value:,.2f}\")\n",
    "\n",
    "# --- STAGE 1, PART 2: K-Fold Training of Mean Model ---\n",
    "print(\"\\n# STAGE 1, PART 2: K-Fold Training of Mean Model...\")\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_mean_preds = np.zeros(len(X))\n",
    "test_mean_preds = np.zeros(len(X_test))\n",
    "grade_for_stratify = pd.read_csv(DATA_PATH + 'dataset.csv')['grade']\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, grade_for_stratify)):\n",
    "    print(f\"  Mean Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**best_params_mean, n_estimators=2500, objective='reg:squarederror', eval_metric='rmse', tree_method='hist', random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(X.iloc[train_idx], y_true.iloc[train_idx], eval_set=[(X.iloc[val_idx], y_true.iloc[val_idx])], verbose=False)\n",
    "    oof_mean_preds[val_idx] = model.predict(X.iloc[val_idx])\n",
    "    test_mean_preds += model.predict(X_test) / N_SPLITS\n",
    "    \n",
    "# --- NEW: CALCULATE AND PRINT FINAL OOF RMSE ---\n",
    "final_mean_rmse = np.sqrt(mean_squared_error(y_true, oof_mean_preds))\n",
    "print(f\"\\n# Mean model K-Fold training complete.\")\n",
    "print(f\"# Final OOF RMSE for Mean Model: ${final_mean_rmse:,.2f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f875dbf2-b95e-425d-a616-3e54f9cd1fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Performing feature engineering for the error model...\n",
      "  Adding 'mean_pred_oof' feature...\n",
      "  Adding 'pred_deviation' feature...\n",
      "  Adding 'pred_bin' feature...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-08 21:55:59,796] A new study created in memory with name: no-name-2f5ee471-46c4-4d48-8df0-4743eb928ed7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Feature engineering complete. Total features for error model: 129\n",
      "\n",
      "# STAGE 2, PART 1: Tuning Error Prediction Model (on new features)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-08 21:56:11,347] Trial 0 finished with value: 63832.8877184405 and parameters: {'eta': 0.024369832803053784, 'max_depth': 8, 'subsample': 0.9254193579648227, 'colsample_bytree': 0.7480713045559889, 'lambda': 0.401579908612991, 'alpha': 0.5170024596156164}. Best is trial 0 with value: 63832.8877184405.\n",
      "[I 2025-07-08 21:56:20,586] Trial 1 finished with value: 64284.18612174746 and parameters: {'eta': 0.04037040659197692, 'max_depth': 10, 'subsample': 0.873090966105899, 'colsample_bytree': 0.7486149531632349, 'lambda': 0.6068511622257085, 'alpha': 0.46954794597295985}. Best is trial 0 with value: 63832.8877184405.\n",
      "[I 2025-07-08 21:56:31,945] Trial 2 finished with value: 63987.25912455223 and parameters: {'eta': 0.026008751416083643, 'max_depth': 7, 'subsample': 0.9834624471078448, 'colsample_bytree': 0.7227054935595059, 'lambda': 0.40523964074614344, 'alpha': 0.5193664261515867}. Best is trial 0 with value: 63832.8877184405.\n",
      "[I 2025-07-08 21:56:50,421] Trial 3 finished with value: 64053.02287067258 and parameters: {'eta': 0.0363503046869902, 'max_depth': 7, 'subsample': 0.8866285174924128, 'colsample_bytree': 0.7354633341854819, 'lambda': 0.4501353799459747, 'alpha': 0.46479297781596196}. Best is trial 0 with value: 63832.8877184405.\n",
      "[I 2025-07-08 21:57:01,024] Trial 4 finished with value: 64274.0471384382 and parameters: {'eta': 0.04272362252568337, 'max_depth': 10, 'subsample': 0.9454162831966287, 'colsample_bytree': 0.7215997735368646, 'lambda': 0.6388158491145568, 'alpha': 0.523527496057977}. Best is trial 0 with value: 63832.8877184405.\n",
      "[I 2025-07-08 21:57:17,976] Trial 5 finished with value: 63921.3523981671 and parameters: {'eta': 0.03434389987511372, 'max_depth': 7, 'subsample': 0.8954293173765288, 'colsample_bytree': 0.7346442378213794, 'lambda': 0.705284367495991, 'alpha': 0.4722641524281761}. Best is trial 0 with value: 63832.8877184405.\n",
      "[I 2025-07-08 21:57:37,548] Trial 6 finished with value: 64021.12664449922 and parameters: {'eta': 0.014433128700884874, 'max_depth': 9, 'subsample': 0.810402661681529, 'colsample_bytree': 0.7481260918286279, 'lambda': 0.45240376311240926, 'alpha': 0.4792289591546099}. Best is trial 0 with value: 63832.8877184405.\n",
      "[I 2025-07-08 21:57:48,564] Trial 7 finished with value: 64099.258878326844 and parameters: {'eta': 0.02860608226369693, 'max_depth': 8, 'subsample': 0.8715287858185861, 'colsample_bytree': 0.7434169325169793, 'lambda': 0.4294930533715161, 'alpha': 0.5902718053577023}. Best is trial 0 with value: 63832.8877184405.\n",
      "[I 2025-07-08 21:57:59,389] Trial 8 finished with value: 64142.688232811575 and parameters: {'eta': 0.03575718571414534, 'max_depth': 10, 'subsample': 0.8079655694100245, 'colsample_bytree': 0.7398792660625703, 'lambda': 0.4302384026005036, 'alpha': 0.4750808449804304}. Best is trial 0 with value: 63832.8877184405.\n",
      "[I 2025-07-08 21:58:07,257] Trial 9 finished with value: 64011.49793742359 and parameters: {'eta': 0.03927569898448081, 'max_depth': 8, 'subsample': 0.8063643965337117, 'colsample_bytree': 0.7425656185456356, 'lambda': 0.5251859488862413, 'alpha': 0.5803835868904076}. Best is trial 0 with value: 63832.8877184405.\n",
      "[I 2025-07-08 21:58:26,762] Trial 10 finished with value: 63854.177150151554 and parameters: {'eta': 0.01604098169762419, 'max_depth': 9, 'subsample': 0.938668577709522, 'colsample_bytree': 0.6960813644034358, 'lambda': 0.35762040998138633, 'alpha': 0.6622050178860421}. Best is trial 0 with value: 63832.8877184405.\n",
      "[I 2025-07-08 21:58:46,078] Trial 11 finished with value: 63941.33692416336 and parameters: {'eta': 0.01615761728096211, 'max_depth': 9, 'subsample': 0.93260082659652, 'colsample_bytree': 0.692963833973368, 'lambda': 0.34799195881061834, 'alpha': 0.6844118283921141}. Best is trial 0 with value: 63832.8877184405.\n",
      "[I 2025-07-08 21:58:59,411] Trial 12 finished with value: 63856.11298750404 and parameters: {'eta': 0.020662989850214166, 'max_depth': 8, 'subsample': 0.9303123755693216, 'colsample_bytree': 0.696569959484238, 'lambda': 0.349880009761422, 'alpha': 0.6652772221679872}. Best is trial 0 with value: 63832.8877184405.\n",
      "[I 2025-07-08 21:59:15,552] Trial 13 finished with value: 64001.64111563515 and parameters: {'eta': 0.019979421963736588, 'max_depth': 9, 'subsample': 0.9683250326655733, 'colsample_bytree': 0.7082925870988613, 'lambda': 0.3778638077900167, 'alpha': 0.6237259450663176}. Best is trial 0 with value: 63832.8877184405.\n",
      "[I 2025-07-08 21:59:42,902] Trial 14 finished with value: 63747.01515096744 and parameters: {'eta': 0.011243164688540447, 'max_depth': 8, 'subsample': 0.9103526193404224, 'colsample_bytree': 0.7078764066222972, 'lambda': 0.505096157936141, 'alpha': 0.5343933773368161}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:00:09,161] Trial 15 finished with value: 63927.11325177938 and parameters: {'eta': 0.011661879242275451, 'max_depth': 8, 'subsample': 0.909630275378912, 'colsample_bytree': 0.7093459541047091, 'lambda': 0.5208561314018584, 'alpha': 0.5286101812222518}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:00:22,683] Trial 16 finished with value: 63926.48258223401 and parameters: {'eta': 0.02885022176856517, 'max_depth': 8, 'subsample': 0.9130583637471172, 'colsample_bytree': 0.711519859987635, 'lambda': 0.5773815550839202, 'alpha': 0.548303528243865}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:00:30,817] Trial 17 finished with value: 63837.175567116494 and parameters: {'eta': 0.023039920635358507, 'max_depth': 7, 'subsample': 0.8385361087176418, 'colsample_bytree': 0.7162967149535927, 'lambda': 0.4762437957361963, 'alpha': 0.503376241136745}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:00:59,996] Trial 18 finished with value: 63850.712900750565 and parameters: {'eta': 0.010599536492466293, 'max_depth': 8, 'subsample': 0.9573476663604308, 'colsample_bytree': 0.7284619854968657, 'lambda': 0.4876155753540968, 'alpha': 0.5613366863596986}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:01:11,402] Trial 19 finished with value: 64132.43502278309 and parameters: {'eta': 0.030790278778935032, 'max_depth': 9, 'subsample': 0.8554604356024537, 'colsample_bytree': 0.7022072552099091, 'lambda': 0.3962134327251923, 'alpha': 0.6071773641585239}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:01:23,644] Trial 20 finished with value: 63882.53705418201 and parameters: {'eta': 0.023096828114672806, 'max_depth': 7, 'subsample': 0.9132678695094183, 'colsample_bytree': 0.7284520878198883, 'lambda': 0.5670434423330296, 'alpha': 0.5484111253720679}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:01:38,124] Trial 21 finished with value: 63905.39778906483 and parameters: {'eta': 0.023884813631669614, 'max_depth': 7, 'subsample': 0.8344425639333336, 'colsample_bytree': 0.7166717139992567, 'lambda': 0.4836434291024735, 'alpha': 0.5021606192324226}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:01:53,621] Trial 22 finished with value: 63890.958907753884 and parameters: {'eta': 0.02047744851656022, 'max_depth': 7, 'subsample': 0.8460928622391342, 'colsample_bytree': 0.7163798956943067, 'lambda': 0.49024808156735067, 'alpha': 0.5041108507505067}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:02:06,252] Trial 23 finished with value: 63924.24829362123 and parameters: {'eta': 0.018504841007687695, 'max_depth': 8, 'subsample': 0.8961089127168865, 'colsample_bytree': 0.705775703369664, 'lambda': 0.5490720391991615, 'alpha': 0.49835938496855753}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:02:19,485] Trial 24 finished with value: 64001.289103514624 and parameters: {'eta': 0.024379102520145822, 'max_depth': 8, 'subsample': 0.8239234090472416, 'colsample_bytree': 0.716445514649352, 'lambda': 0.4696309153798104, 'alpha': 0.5395740984081362}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:02:28,934] Trial 25 finished with value: 63975.043932014785 and parameters: {'eta': 0.03247179357885216, 'max_depth': 7, 'subsample': 0.8665209204296888, 'colsample_bytree': 0.7026513507930137, 'lambda': 0.4181511810346343, 'alpha': 0.4947059860396006}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:02:57,245] Trial 26 finished with value: 63867.381055511374 and parameters: {'eta': 0.013743936546006003, 'max_depth': 8, 'subsample': 0.9222758128745626, 'colsample_bytree': 0.7250948192710551, 'lambda': 0.38272201376565246, 'alpha': 0.5640286372970744}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:03:12,776] Trial 27 finished with value: 63822.985644653556 and parameters: {'eta': 0.02651949178789074, 'max_depth': 7, 'subsample': 0.9505037649479536, 'colsample_bytree': 0.7137520961572416, 'lambda': 0.6570364153769778, 'alpha': 0.515768064032395}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:03:55,780] Trial 28 finished with value: 63973.28322435027 and parameters: {'eta': 0.02686092075629133, 'max_depth': 8, 'subsample': 0.9540504689250712, 'colsample_bytree': 0.713787562597486, 'lambda': 0.7467981266345022, 'alpha': 0.4552965561801786}. Best is trial 14 with value: 63747.01515096744.\n",
      "[I 2025-07-08 22:04:13,718] Trial 29 finished with value: 64143.08517260071 and parameters: {'eta': 0.030908407255964276, 'max_depth': 9, 'subsample': 0.9841116667638453, 'colsample_bytree': 0.7495830281369327, 'lambda': 0.6405404312402985, 'alpha': 0.5314425540482025}. Best is trial 14 with value: 63747.01515096744.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Error Model Tuning Complete. Best Validation RMSE: $63,747.02\n",
      "\n",
      "# STAGE 2, PART 2: K-Fold Training of Error Model...\n",
      "  Error Model - Fold 1/5...\n",
      "  Error Model - Fold 2/5...\n",
      "  Error Model - Fold 3/5...\n",
      "  Error Model - Fold 4/5...\n",
      "  Error Model - Fold 5/5...\n",
      "\n",
      "# Error model K-Fold training complete.\n",
      "# Final OOF RMSE for Error Model: $63,560.57\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STAGE 2: ERROR MODEL FEATURE ENGINEERING, TUNING, AND TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "# --- Define the Error Target ---\n",
    "error_target = np.abs(y_true - oof_mean_preds)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- PART 1A: FEATURE ENGINEERING FOR THE ERROR MODEL ---\n",
    "# =============================================================================\n",
    "print(\"\\n# Performing feature engineering for the error model...\")\n",
    "\n",
    "# Start with copies of the original data\n",
    "X_for_error = X.copy()\n",
    "X_test_for_error = X_test.copy()\n",
    "\n",
    "# Feature 1: The mean prediction itself\n",
    "print(\"  Adding 'mean_pred_oof' feature...\")\n",
    "X_for_error['mean_pred_oof'] = oof_mean_preds\n",
    "X_test_for_error['mean_pred_oof'] = test_mean_preds\n",
    "\n",
    "# Feature 2: Deviation from the average prediction\n",
    "print(\"  Adding 'pred_deviation' feature...\")\n",
    "avg_oof_pred = oof_mean_preds.mean()\n",
    "X_for_error['pred_deviation'] = np.abs(X_for_error['mean_pred_oof'] - avg_oof_pred)\n",
    "X_test_for_error['pred_deviation'] = np.abs(X_test_for_error['mean_pred_oof'] - avg_oof_pred)\n",
    "\n",
    "# Feature 3: Binned predictions\n",
    "print(\"  Adding 'pred_bin' feature...\")\n",
    "X_for_error['pred_bin'], bin_edges = pd.qcut(X_for_error['mean_pred_oof'], \n",
    "                                             q=10, \n",
    "                                             labels=False, \n",
    "                                             retbins=True, \n",
    "                                             duplicates='drop')\n",
    "X_test_for_error['pred_bin'] = pd.cut(X_test_for_error['mean_pred_oof'], \n",
    "                                      bins=bin_edges, \n",
    "                                      labels=False, \n",
    "                                      include_lowest=True).fillna(-1).astype(int)\n",
    "\n",
    "print(f\"\\n# Feature engineering complete. Total features for error model: {X_for_error.shape[1]}\")\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# --- PART 1B: Tuning the Error Model on the NEW Feature Set ---\n",
    "print(\"\\n# STAGE 2, PART 1: Tuning Error Prediction Model (on new features)...\")\n",
    "\n",
    "def objective_error(trial):\n",
    "    # This split now correctly uses X_for_error which has the new features\n",
    "    train_x, val_x, train_y, val_y = train_test_split(X_for_error, error_target, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    \n",
    "    params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "            'tree_method': 'hist',\n",
    "            'eta': trial.suggest_float('eta', 0.01, 0.043),\n",
    "            'max_depth': trial.suggest_int('max_depth', 7, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.8, 0.99),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.692, 0.75),\n",
    "            'lambda': trial.suggest_float('lambda', 0.345, 0.765, log=True),\n",
    "            'alpha': trial.suggest_float('alpha', 0.455, 0.69),\n",
    "        }\n",
    "\n",
    "    model = xgb.XGBRegressor(**params, n_estimators=2500, random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(train_x, train_y, eval_set=[(val_x, val_y)], verbose=False)\n",
    "    preds = model.predict(val_x)\n",
    "    return np.sqrt(mean_squared_error(val_y, preds))\n",
    "\n",
    "study_error = optuna.create_study(direction='minimize')\n",
    "study_error.optimize(objective_error, n_trials=N_OPTUNA_TRIALS)\n",
    "best_params_error = study_error.best_params\n",
    "print(f\"\\n# Error Model Tuning Complete. Best Validation RMSE: ${study_error.best_value:,.2f}\")\n",
    "\n",
    "\n",
    "# --- PART 2: K-Fold Training of Error Model with Best Params ---\n",
    "print(\"\\n# STAGE 2, PART 2: K-Fold Training of Error Model...\")\n",
    "oof_error_preds = np.zeros(len(X))\n",
    "test_error_preds = np.zeros(len(X_test))\n",
    "\n",
    "# Add the other required XGBoost parameters\n",
    "final_params_error = {\n",
    "    'objective': 'reg:squarederror', \n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'hist', \n",
    "    'random_state': RANDOM_STATE, \n",
    "    'n_jobs': -1, \n",
    "    **best_params_error\n",
    "}\n",
    "\n",
    "# This loop now uses X_for_error, which has the correct features\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_for_error, grade_for_stratify)):\n",
    "    print(f\"  Error Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**final_params_error, n_estimators=2500, early_stopping_rounds=100)\n",
    "    \n",
    "    X_train, X_val = X_for_error.iloc[train_idx], X_for_error.iloc[val_idx]\n",
    "    y_train, y_val = error_target.iloc[train_idx], error_target.iloc[val_idx]\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    oof_error_preds[val_idx] = model.predict(X_val)\n",
    "    test_error_preds += model.predict(X_test_for_error) / N_SPLITS\n",
    "\n",
    "# --- Calculate and Print Final OOF RMSE ---\n",
    "final_error_rmse = np.sqrt(mean_squared_error(error_target, oof_error_preds))\n",
    "print(f\"\\n# Error model K-Fold training complete.\")\n",
    "print(f\"# Final OOF RMSE for Error Model: ${final_error_rmse:,.2f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ca9aff-1f95-4968-aac8-fc2ccf0b0b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Asymmetric Calibration ---\n",
      "New Best! a=1.90, b=2.10, Score=305,861.00, Cov=88.89%\n",
      "New Best! a=1.90, b=2.11, Score=305,815.11, Cov=88.95%\n",
      "New Best! a=1.90, b=2.12, Score=305,775.35, Cov=89.01%\n",
      "New Best! a=1.90, b=2.13, Score=305,742.42, Cov=89.09%\n",
      "New Best! a=1.90, b=2.14, Score=305,716.01, Cov=89.15%\n",
      "New Best! a=1.90, b=2.15, Score=305,696.73, Cov=89.21%\n",
      "New Best! a=1.90, b=2.16, Score=305,683.98, Cov=89.27%\n",
      "New Best! a=1.90, b=2.17, Score=305,677.79, Cov=89.33%\n",
      "New Best! a=1.91, b=2.14, Score=305,666.67, Cov=89.24%\n",
      "New Best! a=1.91, b=2.15, Score=305,647.39, Cov=89.30%\n",
      "New Best! a=1.91, b=2.16, Score=305,634.64, Cov=89.36%\n",
      "New Best! a=1.91, b=2.17, Score=305,628.45, Cov=89.42%\n",
      "New Best! a=1.92, b=2.14, Score=305,627.43, Cov=89.33%\n",
      "New Best! a=1.92, b=2.15, Score=305,608.15, Cov=89.39%\n",
      "New Best! a=1.92, b=2.16, Score=305,595.40, Cov=89.45%\n",
      "New Best! a=1.92, b=2.17, Score=305,589.21, Cov=89.51%\n",
      "New Best! a=1.93, b=2.15, Score=305,578.97, Cov=89.49%\n",
      "New Best! a=1.93, b=2.16, Score=305,566.22, Cov=89.55%\n",
      "New Best! a=1.93, b=2.17, Score=305,560.03, Cov=89.61%\n",
      "New Best! a=1.94, b=2.15, Score=305,559.59, Cov=89.57%\n",
      "New Best! a=1.94, b=2.16, Score=305,546.84, Cov=89.63%\n",
      "New Best! a=1.94, b=2.17, Score=305,540.65, Cov=89.69%\n",
      "New Best! a=1.95, b=2.16, Score=305,536.95, Cov=89.73%\n",
      "New Best! a=1.95, b=2.17, Score=305,530.75, Cov=89.79%\n",
      "\n",
      "Grid search complete. Final OOF Score: 305,530.75. Best multipliers: a=1.95, b=2.17\n",
      "\n",
      "Creating final submission file...\n",
      "\n",
      "'submission_final_v6.csv' created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>842937.644678</td>\n",
       "      <td>1.058807e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>583226.265234</td>\n",
       "      <td>8.498752e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>459273.298828</td>\n",
       "      <td>6.861471e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>283237.266577</td>\n",
       "      <td>4.086617e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>370852.202930</td>\n",
       "      <td>8.742853e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       pi_lower      pi_upper\n",
       "0  200000  842937.644678  1.058807e+06\n",
       "1  200001  583226.265234  8.498752e+05\n",
       "2  200002  459273.298828  6.861471e+05\n",
       "3  200003  283237.266577  4.086617e+05\n",
       "4  200004  370852.202930  8.742853e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL ASYMMETRIC CALIBRATION AND SUBMISSION (ULTIMATE ROBUST VERSION)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Final Asymmetric Calibration ---\")\n",
    "\n",
    "# --- Safely reload y_true to ensure it's available ---\n",
    "y_true = pd.read_csv('./dataset.csv')['sale_price']\n",
    "\n",
    "# --- Your existing correct code ---\n",
    "oof_error_final = np.clip(oof_error_preds, 0, None) \n",
    "best_a, best_b, best_metric = 2.0, 2.0, float('inf')\n",
    "\n",
    "for a in np.arange(1.90, 2.31, 0.01):\n",
    "    for b in np.arange(2.10, 2.51, 0.01):\n",
    "        low = oof_mean_preds - oof_error_final * a\n",
    "        high = oof_mean_preds + oof_error_final * b\n",
    "        # We need the winkler_score function defined here or in a previous cell\n",
    "        metric, coverage = winkler_score(y_true, low, high, alpha=COMPETITION_ALPHA, return_coverage=True)\n",
    "        if metric < best_metric:\n",
    "            best_metric = metric\n",
    "            best_a, best_b = a, b\n",
    "            print(f\"New Best! a={best_a:.2f}, b={best_b:.2f}, Score={best_metric:,.2f}, Cov={coverage:.2%}\")\n",
    "print(f\"\\nGrid search complete. Final OOF Score: {best_metric:,.2f}. Best multipliers: a={best_a:.2f}, b={best_b:.2f}\")\n",
    "\n",
    "\n",
    "# --- Create Final Submission ---\n",
    "print(\"\\nCreating final submission file...\")\n",
    "test_error_final = np.clip(test_error_preds, 0, None)\n",
    "final_lower = test_mean_preds - test_error_final * best_a\n",
    "final_upper = test_mean_preds + test_error_final * best_b\n",
    "final_upper = np.maximum(final_lower, final_upper)\n",
    "\n",
    "# Your excellent, robust fix for the IDs\n",
    "test_ids = pd.read_csv('./test.csv', usecols=['id'])['id']\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids, \n",
    "    'pi_lower': final_lower, \n",
    "    'pi_upper': final_upper\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_final_v6.csv', index=False)\n",
    "print(\"\\n'submission_final_v6.csv' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa457b-3766-47ce-a5cc-749a5d3c21dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a498e92-5949-4a29-99f3-9223904f1462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
