{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c8f6cd7-09f6-4211-8378-179a9a8546c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "print(\"Libraries imported successfully.\")\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm','view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4841131-708c-4403-b959-ab8b2cfb51ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Block 2: Synthesized Feature Engineering ---\n",
      "Creating brute-force numerical interaction features...\n",
      "Creating TF-IDF features for text columns...\n",
      "Finalizing feature set...\n",
      "\n",
      "Synthesized FE complete. Total features: 111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 2: SYNTHESIZED FEATURE ENGINEERING (CORRECTED)\n",
    "# =============================================================================\n",
    "print(\"--- Starting Block 2: Synthesized Feature Engineering ---\")\n",
    "def create_synthesized_features(df_train, df_test):\n",
    "    # Combine for consistent processing and reset the index\n",
    "    df_train['is_train'] = 1\n",
    "    df_test['is_train'] = 0\n",
    "    # Store the original id for later, as reset_index will remove it\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    all_data = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # --- A) Brute-Force Numerical Interactions ---\n",
    "    print(\"Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1','grade', 'year_built']\n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] *all_data[NUMS[j]]\n",
    "    \n",
    "    # --- B) Date Features ---\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['year'] = all_data['sale_date'].dt.year\n",
    "    all_data['month'] = all_data['sale_date'].dt.month\n",
    "    all_data['year_diff'] = all_data['year'] - all_data['year_built']\n",
    "    \n",
    "    # --- C) TF-IDF Text Features ---\n",
    "    print(\"Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning','join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5),max_features=128, binary=True)\n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        \n",
    "        # This concat will now work because both have a simple 0-based index\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "    \n",
    "    # --- D) Log transform some of the new interaction features ---\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            # Add a small constant to avoid log(0)\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "    \n",
    "    # --- E) Final Cleanup ---\n",
    "    print(\"Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city','sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "    all_data.fillna(0, inplace=True)\n",
    "    \n",
    "    # Separate final datasets\n",
    "    X = all_data[all_data['is_train'] == 1].drop(columns=['is_train','sale_price'])\n",
    "    X_test = all_data[all_data['is_train'] == 0].drop(columns=['is_train','sale_price'])\n",
    "    \n",
    "    # Restore the original 'id' as the index\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    X_test = X_test[X.columns]\n",
    "    return X, X_test\n",
    "\n",
    "# We need to re-run this from the original dataframes\n",
    "X, X_test = create_synthesized_features(df_train, df_test)\n",
    "print(f\"\\nSynthesized FE complete. Total features: {X.shape[1]}\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5eaf832-1819-4647-8f24-ea9bbe9e02a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 1, PART 1: K-Fold Aware Tuning of Mean Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-08 22:06:59,556] A new study created in memory with name: no-name-4e11d064-15c5-4f5e-aed2-fd6f7d9d2776\n",
      "[I 2025-07-08 22:09:55,588] Trial 0 finished with value: 98938.76009298983 and parameters: {'eta': 0.04281475119303402, 'max_depth': 8, 'subsample': 0.8008724938098551, 'colsample_bytree': 0.8282533499349177, 'lambda': 4.582727733350719, 'alpha': 0.00026503269830849513, 'min_child_weight': 3}. Best is trial 0 with value: 98938.76009298983.\n",
      "[I 2025-07-08 22:12:38,572] Trial 1 finished with value: 99067.04272329391 and parameters: {'eta': 0.04474614584127113, 'max_depth': 8, 'subsample': 0.8698095567011409, 'colsample_bytree': 0.8541165482843458, 'lambda': 4.5231751148301225, 'alpha': 1.6013231735378886e-05, 'min_child_weight': 3}. Best is trial 0 with value: 98938.76009298983.\n",
      "[I 2025-07-08 22:14:57,408] Trial 2 finished with value: 99148.80405718597 and parameters: {'eta': 0.042785180070228294, 'max_depth': 7, 'subsample': 0.8026216965286678, 'colsample_bytree': 0.8200600991698459, 'lambda': 3.065550180293547, 'alpha': 2.0495589322939413e-05, 'min_child_weight': 3}. Best is trial 0 with value: 98938.76009298983.\n",
      "[I 2025-07-08 22:17:40,383] Trial 3 finished with value: 98799.5695412467 and parameters: {'eta': 0.04142820275059558, 'max_depth': 8, 'subsample': 0.856106512254891, 'colsample_bytree': 0.8695456058966013, 'lambda': 4.438899584496487, 'alpha': 0.0007521396017589885, 'min_child_weight': 3}. Best is trial 3 with value: 98799.5695412467.\n",
      "[I 2025-07-08 22:19:56,758] Trial 4 finished with value: 99178.89256084581 and parameters: {'eta': 0.041462461424829365, 'max_depth': 7, 'subsample': 0.8283359269106536, 'colsample_bytree': 0.8674241286113501, 'lambda': 3.1703838578268684, 'alpha': 9.630380771886077e-05, 'min_child_weight': 1}. Best is trial 3 with value: 98799.5695412467.\n",
      "[I 2025-07-08 22:23:20,578] Trial 5 finished with value: 99430.67819153584 and parameters: {'eta': 0.040162318598110584, 'max_depth': 9, 'subsample': 0.8640460455898282, 'colsample_bytree': 0.8054067423764159, 'lambda': 4.652471288951934, 'alpha': 0.00011883428129485928, 'min_child_weight': 3}. Best is trial 3 with value: 98799.5695412467.\n",
      "[I 2025-07-08 22:26:13,925] Trial 6 finished with value: 98953.83809735895 and parameters: {'eta': 0.04277138931558555, 'max_depth': 8, 'subsample': 0.8076795482059492, 'colsample_bytree': 0.8091098967462629, 'lambda': 4.19462467903197, 'alpha': 4.421552381634501e-05, 'min_child_weight': 2}. Best is trial 3 with value: 98799.5695412467.\n",
      "[I 2025-07-08 22:28:30,540] Trial 7 finished with value: 99272.15917502125 and parameters: {'eta': 0.03845143864634896, 'max_depth': 7, 'subsample': 0.8407862552024993, 'colsample_bytree': 0.8236103367467089, 'lambda': 4.005807458675662, 'alpha': 8.565338659443138e-05, 'min_child_weight': 3}. Best is trial 3 with value: 98799.5695412467.\n",
      "[I 2025-07-08 22:31:27,068] Trial 8 finished with value: 98880.16746297888 and parameters: {'eta': 0.0393831795021813, 'max_depth': 8, 'subsample': 0.8683619063746334, 'colsample_bytree': 0.8292729643479821, 'lambda': 3.732444897858989, 'alpha': 0.0003129634306276942, 'min_child_weight': 1}. Best is trial 3 with value: 98799.5695412467.\n",
      "[I 2025-07-08 22:34:14,788] Trial 9 finished with value: 98770.55194728417 and parameters: {'eta': 0.0418686383967202, 'max_depth': 8, 'subsample': 0.8384479831175183, 'colsample_bytree': 0.8677852528903391, 'lambda': 4.182603608312375, 'alpha': 1.921000042773171e-05, 'min_child_weight': 3}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 22:37:49,266] Trial 10 finished with value: 99446.19798540678 and parameters: {'eta': 0.03530681449221218, 'max_depth': 9, 'subsample': 0.8276885851252845, 'colsample_bytree': 0.8493422326352719, 'lambda': 3.6158536285399876, 'alpha': 1.0880853599378116e-05, 'min_child_weight': 2}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 22:41:24,190] Trial 11 finished with value: 99405.020826109 and parameters: {'eta': 0.03759928249810151, 'max_depth': 9, 'subsample': 0.8499694197433048, 'colsample_bytree': 0.8799983744706174, 'lambda': 4.945373499107991, 'alpha': 0.000895884943817953, 'min_child_weight': 2}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 22:44:04,274] Trial 12 finished with value: 98976.8214015913 and parameters: {'eta': 0.04100037921328195, 'max_depth': 8, 'subsample': 0.851737418382437, 'colsample_bytree': 0.8680963879274322, 'lambda': 4.263991305024088, 'alpha': 0.0007263969374770905, 'min_child_weight': 3}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 22:46:47,893] Trial 13 finished with value: 99171.29010057625 and parameters: {'eta': 0.043880592748969445, 'max_depth': 8, 'subsample': 0.8277455869036754, 'colsample_bytree': 0.8787452320703838, 'lambda': 3.7418706385186984, 'alpha': 3.494425020059219e-05, 'min_child_weight': 2}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 22:49:32,416] Trial 14 finished with value: 98958.3618997887 and parameters: {'eta': 0.0414367225468227, 'max_depth': 8, 'subsample': 0.854975701883237, 'colsample_bytree': 0.85860455659199, 'lambda': 4.298082026608348, 'alpha': 0.0003303369888151251, 'min_child_weight': 2}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 22:53:01,684] Trial 15 finished with value: 99378.63362581759 and parameters: {'eta': 0.036830512741407945, 'max_depth': 9, 'subsample': 0.8392876817643421, 'colsample_bytree': 0.8435328584588961, 'lambda': 4.9720895494161095, 'alpha': 4.254894789405681e-05, 'min_child_weight': 3}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 22:55:32,301] Trial 16 finished with value: 99120.4838407089 and parameters: {'eta': 0.04015577435958881, 'max_depth': 7, 'subsample': 0.8798668562684148, 'colsample_bytree': 0.8618512349872987, 'lambda': 4.001050128515959, 'alpha': 0.0001907694494891333, 'min_child_weight': 3}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 22:58:21,890] Trial 17 finished with value: 99028.58607020814 and parameters: {'eta': 0.042088970213788934, 'max_depth': 8, 'subsample': 0.814775612015575, 'colsample_bytree': 0.8707944977030322, 'lambda': 3.3653280639271146, 'alpha': 0.0004873694942641055, 'min_child_weight': 2}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 23:00:39,285] Trial 18 finished with value: 99112.93278625645 and parameters: {'eta': 0.043901605592089596, 'max_depth': 7, 'subsample': 0.8388065700207841, 'colsample_bytree': 0.8371853841106249, 'lambda': 4.4613158726268205, 'alpha': 6.700073008537795e-05, 'min_child_weight': 1}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 23:04:10,515] Trial 19 finished with value: 99285.74157152665 and parameters: {'eta': 0.039211774569484145, 'max_depth': 9, 'subsample': 0.8605566285024556, 'colsample_bytree': 0.8469914252644418, 'lambda': 4.720690310912461, 'alpha': 0.00015735009018275357, 'min_child_weight': 3}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 23:06:55,663] Trial 20 finished with value: 98831.72627975151 and parameters: {'eta': 0.041000875591645676, 'max_depth': 8, 'subsample': 0.8442348477183542, 'colsample_bytree': 0.8734541577486823, 'lambda': 4.156829453971179, 'alpha': 2.2644493609864194e-05, 'min_child_weight': 2}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 23:09:42,546] Trial 21 finished with value: 98996.54336860165 and parameters: {'eta': 0.04109874007281074, 'max_depth': 8, 'subsample': 0.8463885459774475, 'colsample_bytree': 0.8719833450481091, 'lambda': 4.130888897797768, 'alpha': 2.5487502817801466e-05, 'min_child_weight': 2}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 23:12:26,268] Trial 22 finished with value: 99039.01833916956 and parameters: {'eta': 0.04216159400123763, 'max_depth': 8, 'subsample': 0.8322217747919535, 'colsample_bytree': 0.8606755755731301, 'lambda': 4.371379309826259, 'alpha': 1.0880836659650267e-05, 'min_child_weight': 3}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 23:15:09,488] Trial 23 finished with value: 99102.88882256234 and parameters: {'eta': 0.04053683518052403, 'max_depth': 8, 'subsample': 0.8581266700514425, 'colsample_bytree': 0.8741211177573838, 'lambda': 3.819054658233084, 'alpha': 5.773774407166684e-05, 'min_child_weight': 2}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 23:17:59,149] Trial 24 finished with value: 98948.61111263097 and parameters: {'eta': 0.03896299455298019, 'max_depth': 8, 'subsample': 0.8448284459431081, 'colsample_bytree': 0.864317331434609, 'lambda': 4.789980773021938, 'alpha': 2.478228671198882e-05, 'min_child_weight': 1}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 23:20:44,496] Trial 25 finished with value: 99070.87271691667 and parameters: {'eta': 0.042005589131654204, 'max_depth': 8, 'subsample': 0.822118567583412, 'colsample_bytree': 0.8542993037982787, 'lambda': 4.080185467718197, 'alpha': 1.783663846727891e-05, 'min_child_weight': 3}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 23:24:16,667] Trial 26 finished with value: 99561.98479606936 and parameters: {'eta': 0.043426992946247946, 'max_depth': 9, 'subsample': 0.8347232303062938, 'colsample_bytree': 0.876004178186617, 'lambda': 3.866280783741838, 'alpha': 2.943636868690186e-05, 'min_child_weight': 2}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 23:26:30,696] Trial 27 finished with value: 99196.30800319428 and parameters: {'eta': 0.04045451308883999, 'max_depth': 7, 'subsample': 0.8213222049459411, 'colsample_bytree': 0.8553085679734783, 'lambda': 3.5057002710758733, 'alpha': 1.3594383262504458e-05, 'min_child_weight': 3}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 23:29:18,288] Trial 28 finished with value: 98958.39373666723 and parameters: {'eta': 0.03959837217845169, 'max_depth': 8, 'subsample': 0.8481622702113317, 'colsample_bytree': 0.8666279770685769, 'lambda': 4.401911138198309, 'alpha': 0.0005085801399416878, 'min_child_weight': 2}. Best is trial 9 with value: 98770.55194728417.\n",
      "[I 2025-07-08 23:31:58,631] Trial 29 finished with value: 99091.32522391932 and parameters: {'eta': 0.04309902765908997, 'max_depth': 8, 'subsample': 0.8799613210952212, 'colsample_bytree': 0.8352354962639817, 'lambda': 4.576003772415359, 'alpha': 0.00018456130261037997, 'min_child_weight': 3}. Best is trial 9 with value: 98770.55194728417.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Mean Model K-Fold Aware Tuning Complete. Best Avg RMSE: $98,770.55\n",
      "\n",
      "# STAGE 1, PART 2: Final K-Fold Training of Mean Model...\n",
      "  Mean Model - Fold 1/5...\n",
      "  Mean Model - Fold 2/5...\n",
      "  Mean Model - Fold 3/5...\n",
      "  Mean Model - Fold 4/5...\n",
      "  Mean Model - Fold 5/5...\n",
      "\n",
      "# Mean model K-Fold training complete. Final OOF RMSE: $98,772.21\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: K-FOLD AWARE TUNING & TRAINING OF MEAN MODEL\n",
    "# =============================================================================\n",
    "print(\"\\n--- STAGE 1, PART 1: K-Fold Aware Tuning of Mean Model ---\")\n",
    "\n",
    "# Reload 'grade' for stratification as it's a reliable column\n",
    "grade_for_stratify = pd.read_csv(DATA_PATH + 'dataset.csv')['grade']\n",
    "\n",
    "def objective_mean_kfold(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method':'hist',\n",
    "        'random_state': RANDOM_STATE, 'n_jobs': -1,\n",
    "        'eta': trial.suggest_float('eta', 0.035, 0.045),\n",
    "        'max_depth': trial.suggest_int('max_depth', 7, 9),\n",
    "        'subsample': trial.suggest_float('subsample', 0.80, 0.88),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.80, 0.88),\n",
    "        'lambda': trial.suggest_float('lambda', 3.0, 5.0),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-5, 9e-4, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 3)\n",
    "    }\n",
    "    fold_scores = []\n",
    "    skf_inner = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    for train_idx, val_idx in skf_inner.split(X, grade_for_stratify):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y_true.iloc[train_idx], y_true.iloc[val_idx]\n",
    "        model = xgb.XGBRegressor(**params, n_estimators=2500, early_stopping_rounds=100)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        preds = model.predict(X_val)\n",
    "        fold_scores.append(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "    return np.mean(fold_scores)\n",
    "\n",
    "N_OPTUNA_TRIALS_KFOLD = 30 # Adjust as needed (30 is a good start)\n",
    "study_mean = optuna.create_study(direction='minimize')\n",
    "study_mean.optimize(objective_mean_kfold, n_trials=N_OPTUNA_TRIALS_KFOLD)\n",
    "best_params_mean = study_mean.best_params\n",
    "print(f\"\\n# Mean Model K-Fold Aware Tuning Complete. Best Avg RMSE: ${study_mean.best_value:,.2f}\")\n",
    "\n",
    "# --- STAGE 1, PART 2: Final K-Fold Training of Mean Model ---\n",
    "print(\"\\n# STAGE 1, PART 2: Final K-Fold Training of Mean Model...\")\n",
    "oof_mean_preds = np.zeros(len(X))\n",
    "test_mean_preds = np.zeros(len(X_test))\n",
    "final_params_mean = {'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist', 'random_state': RANDOM_STATE, 'n_jobs': -1, **best_params_mean}\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, grade_for_stratify)):\n",
    "    print(f\"  Mean Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**final_params_mean, n_estimators=2500, early_stopping_rounds=100)\n",
    "    model.fit(X.iloc[train_idx], y_true.iloc[train_idx], eval_set=[(X.iloc[val_idx], y_true.iloc[val_idx])], verbose=False)\n",
    "    oof_mean_preds[val_idx] = model.predict(X.iloc[val_idx])\n",
    "    test_mean_preds += model.predict(X_test) / N_SPLITS\n",
    "\n",
    "final_mean_rmse = np.sqrt(mean_squared_error(y_true, oof_mean_preds))\n",
    "print(f\"\\n# Mean model K-Fold training complete. Final OOF RMSE: ${final_mean_rmse:,.2f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be68af94-fcf6-463d-83ea-7643e6c04035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f40b4cfc-1fc7-4916-9ee5-e69fffadda3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-08 23:40:11,084] A new study created in memory with name: no-name-a9e43e31-ad43-40c1-a24b-b7c71fdd43e2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 2, PART 1: K-Fold Aware Tuning of Error Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-08 23:41:18,646] Trial 0 finished with value: 62651.05659003237 and parameters: {'eta': 0.016004741548190267, 'max_depth': 8, 'subsample': 0.9054208257067495, 'colsample_bytree': 0.7229972362079995, 'lambda': 0.5528965029036256, 'alpha': 0.5679242127910673, 'min_child_weight': 2}. Best is trial 0 with value: 62651.05659003237.\n",
      "[I 2025-07-08 23:42:35,336] Trial 1 finished with value: 62726.50669290988 and parameters: {'eta': 0.018848903525894523, 'max_depth': 9, 'subsample': 0.942429587879134, 'colsample_bytree': 0.7246729432183672, 'lambda': 0.5027653340251114, 'alpha': 0.5640422618502338, 'min_child_weight': 2}. Best is trial 0 with value: 62651.05659003237.\n",
      "[I 2025-07-08 23:43:42,711] Trial 2 finished with value: 62743.61984062765 and parameters: {'eta': 0.019318463201290985, 'max_depth': 9, 'subsample': 0.9483914277571206, 'colsample_bytree': 0.7407761878489196, 'lambda': 0.5038826664505884, 'alpha': 0.6346248151809579, 'min_child_weight': 3}. Best is trial 0 with value: 62651.05659003237.\n",
      "[I 2025-07-08 23:45:20,448] Trial 3 finished with value: 62680.98042716131 and parameters: {'eta': 0.012855986411998324, 'max_depth': 9, 'subsample': 0.9234806510363288, 'colsample_bytree': 0.7434114296688472, 'lambda': 0.66629725079286, 'alpha': 0.6145939279450058, 'min_child_weight': 2}. Best is trial 0 with value: 62651.05659003237.\n",
      "[I 2025-07-08 23:46:46,167] Trial 4 finished with value: 62703.35876391609 and parameters: {'eta': 0.016220023746247036, 'max_depth': 9, 'subsample': 0.9401732280349518, 'colsample_bytree': 0.7417674199751144, 'lambda': 0.6703310271629969, 'alpha': 0.5512538259520682, 'min_child_weight': 2}. Best is trial 0 with value: 62651.05659003237.\n",
      "[I 2025-07-08 23:48:06,631] Trial 5 finished with value: 62627.293529286966 and parameters: {'eta': 0.016063633642360547, 'max_depth': 9, 'subsample': 0.9003024365396008, 'colsample_bytree': 0.7288514499178521, 'lambda': 0.5861670673182893, 'alpha': 0.5162564991962343, 'min_child_weight': 2}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-08 23:49:22,839] Trial 6 finished with value: 62638.796185860665 and parameters: {'eta': 0.014785988403157459, 'max_depth': 8, 'subsample': 0.9049367821642481, 'colsample_bytree': 0.7229090517109371, 'lambda': 0.5716094895511555, 'alpha': 0.615067985650602, 'min_child_weight': 1}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-08 23:50:37,303] Trial 7 finished with value: 62652.22919957832 and parameters: {'eta': 0.016617432750397898, 'max_depth': 9, 'subsample': 0.9233740149587165, 'colsample_bytree': 0.7490553025016791, 'lambda': 0.5841497160702479, 'alpha': 0.6254495989063829, 'min_child_weight': 1}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-08 23:52:05,644] Trial 8 finished with value: 62672.265611381255 and parameters: {'eta': 0.01213477399268446, 'max_depth': 8, 'subsample': 0.9438288162813384, 'colsample_bytree': 0.7265033852486998, 'lambda': 0.5106823985827644, 'alpha': 0.6265004303711905, 'min_child_weight': 3}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-08 23:53:33,474] Trial 9 finished with value: 62632.49901362714 and parameters: {'eta': 0.01349997187588466, 'max_depth': 9, 'subsample': 0.9102738873163472, 'colsample_bytree': 0.7300425674349711, 'lambda': 0.5411630847254463, 'alpha': 0.6419731519264885, 'min_child_weight': 1}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-08 23:55:48,196] Trial 10 finished with value: 62681.31154381884 and parameters: {'eta': 0.010006411996121772, 'max_depth': 10, 'subsample': 0.9143119870362489, 'colsample_bytree': 0.7344501603542078, 'lambda': 0.6241552477468704, 'alpha': 0.5031135173881095, 'min_child_weight': 3}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-08 23:57:34,448] Trial 11 finished with value: 62629.25701160946 and parameters: {'eta': 0.01389289851219689, 'max_depth': 10, 'subsample': 0.9007167976748138, 'colsample_bytree': 0.7319092057665021, 'lambda': 0.5449881719780805, 'alpha': 0.5152920136968173, 'min_child_weight': 1}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-08 23:58:55,811] Trial 12 finished with value: 62668.28062142497 and parameters: {'eta': 0.017694276529981522, 'max_depth': 10, 'subsample': 0.9001675945428331, 'colsample_bytree': 0.7321047506216395, 'lambda': 0.6198144997548186, 'alpha': 0.508009319772848, 'min_child_weight': 1}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-09 00:00:29,975] Trial 13 finished with value: 62699.85583463195 and parameters: {'eta': 0.01446134667186535, 'max_depth': 10, 'subsample': 0.9174403241190323, 'colsample_bytree': 0.7294697387463435, 'lambda': 0.5386486243669123, 'alpha': 0.535507278096922, 'min_child_weight': 1}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-09 00:02:13,447] Trial 14 finished with value: 62746.864783106095 and parameters: {'eta': 0.011365957570101858, 'max_depth': 10, 'subsample': 0.9333140865719636, 'colsample_bytree': 0.7349102757092197, 'lambda': 0.6128563821305085, 'alpha': 0.5265361736788793, 'min_child_weight': 2}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-09 00:03:54,820] Trial 15 finished with value: 62712.37530371593 and parameters: {'eta': 0.013797547133414271, 'max_depth': 10, 'subsample': 0.9000268180724444, 'colsample_bytree': 0.7386803056370077, 'lambda': 0.5728227788967996, 'alpha': 0.5908454387649128, 'min_child_weight': 2}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-09 00:05:20,935] Trial 16 finished with value: 62715.771672282295 and parameters: {'eta': 0.017696760357084926, 'max_depth': 10, 'subsample': 0.9110627646071151, 'colsample_bytree': 0.7281722781899599, 'lambda': 0.5281779471274035, 'alpha': 0.5263131282375784, 'min_child_weight': 1}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-09 00:06:29,670] Trial 17 finished with value: 62674.649586275205 and parameters: {'eta': 0.015527361535223138, 'max_depth': 8, 'subsample': 0.928633547894294, 'colsample_bytree': 0.7205330562804285, 'lambda': 0.5983033926527548, 'alpha': 0.5873633539803003, 'min_child_weight': 3}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-09 00:07:43,626] Trial 18 finished with value: 62670.25427898031 and parameters: {'eta': 0.017006172246817906, 'max_depth': 9, 'subsample': 0.9180282136894785, 'colsample_bytree': 0.7373565146960205, 'lambda': 0.559700982796856, 'alpha': 0.542745839521297, 'min_child_weight': 1}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-09 00:09:05,949] Trial 19 finished with value: 62677.815759223406 and parameters: {'eta': 0.014859653665458352, 'max_depth': 9, 'subsample': 0.9066721024478928, 'colsample_bytree': 0.7331669659405087, 'lambda': 0.6368080661311124, 'alpha': 0.5178662479328363, 'min_child_weight': 2}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-09 00:10:15,933] Trial 20 finished with value: 62720.426026749854 and parameters: {'eta': 0.0199711075867781, 'max_depth': 10, 'subsample': 0.9029036328277701, 'colsample_bytree': 0.7459748902743408, 'lambda': 0.699603046050832, 'alpha': 0.5554159639878539, 'min_child_weight': 2}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-09 00:11:35,695] Trial 21 finished with value: 62652.46746071598 and parameters: {'eta': 0.013495215614776705, 'max_depth': 9, 'subsample': 0.9098879340289695, 'colsample_bytree': 0.7304457102198492, 'lambda': 0.5339796034577186, 'alpha': 0.6470871110816224, 'min_child_weight': 1}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-09 00:13:21,805] Trial 22 finished with value: 62678.71861709689 and parameters: {'eta': 0.01251180480774753, 'max_depth': 9, 'subsample': 0.9096904865545796, 'colsample_bytree': 0.7263113663750651, 'lambda': 0.5511132637780679, 'alpha': 0.5946903523053533, 'min_child_weight': 1}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-09 00:14:44,428] Trial 23 finished with value: 62650.35581663125 and parameters: {'eta': 0.013657836865568913, 'max_depth': 9, 'subsample': 0.9143412883854187, 'colsample_bytree': 0.7316671672141535, 'lambda': 0.5175659620494906, 'alpha': 0.5001413117680024, 'min_child_weight': 1}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-09 00:16:10,538] Trial 24 finished with value: 62667.66007286422 and parameters: {'eta': 0.011636970251750032, 'max_depth': 9, 'subsample': 0.9037850108238198, 'colsample_bytree': 0.7369395256793251, 'lambda': 0.5940096269672933, 'alpha': 0.5160025083463435, 'min_child_weight': 1}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-09 00:17:28,656] Trial 25 finished with value: 62631.90735434894 and parameters: {'eta': 0.015424265980980722, 'max_depth': 8, 'subsample': 0.9082770014291921, 'colsample_bytree': 0.7277446290965861, 'lambda': 0.5428603550274519, 'alpha': 0.5793764843204626, 'min_child_weight': 1}. Best is trial 5 with value: 62627.293529286966.\n",
      "[I 2025-07-09 00:18:41,788] Trial 26 finished with value: 62621.95033344147 and parameters: {'eta': 0.015478052389422032, 'max_depth': 8, 'subsample': 0.9014087963108629, 'colsample_bytree': 0.7276647963944527, 'lambda': 0.5667395594849141, 'alpha': 0.5809043777063232, 'min_child_weight': 1}. Best is trial 26 with value: 62621.95033344147.\n",
      "[I 2025-07-09 00:19:44,116] Trial 27 finished with value: 62647.7533779368 and parameters: {'eta': 0.01751078567830952, 'max_depth': 8, 'subsample': 0.9008383961900533, 'colsample_bytree': 0.7200823781406596, 'lambda': 0.5670383605359015, 'alpha': 0.6005597055269409, 'min_child_weight': 2}. Best is trial 26 with value: 62621.95033344147.\n",
      "[I 2025-07-09 00:20:57,857] Trial 28 finished with value: 62684.387570666244 and parameters: {'eta': 0.014206209546012875, 'max_depth': 8, 'subsample': 0.9150606285385879, 'colsample_bytree': 0.7256652094832453, 'lambda': 0.589847751658803, 'alpha': 0.5402633760564455, 'min_child_weight': 2}. Best is trial 26 with value: 62621.95033344147.\n",
      "[I 2025-07-09 00:22:03,550] Trial 29 finished with value: 62650.82477095972 and parameters: {'eta': 0.015907746967554082, 'max_depth': 8, 'subsample': 0.9047996750746844, 'colsample_bytree': 0.7231136854981014, 'lambda': 0.5577306051485874, 'alpha': 0.5775328549539371, 'min_child_weight': 3}. Best is trial 26 with value: 62621.95033344147.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Error Model K-Fold Aware Tuning Complete. Best Avg RMSE: $62,621.95\n",
      "\n",
      "# STAGE 2, PART 2: Final K-Fold Training of Error Model...\n",
      "  Error Model - Fold 1/5...\n",
      "  Error Model - Fold 2/5...\n",
      "  Error Model - Fold 3/5...\n",
      "  Error Model - Fold 4/5...\n",
      "  Error Model - Fold 5/5...\n",
      "\n",
      "# Error model K-Fold training complete. Final OOF RMSE: $62,628.62\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 4: K-FOLD AWARE TUNING & TRAINING OF ERROR MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# --- Define the Error Target ---\n",
    "error_target = np.abs(y_true - oof_mean_preds)\n",
    "\n",
    "# --- Create the Error Model's Feature Set ---\n",
    "X_for_error = X.copy()\n",
    "X_for_error['mean_pred_oof'] = oof_mean_preds\n",
    "X_test_for_error = X_test.copy()\n",
    "X_test_for_error['mean_pred_oof'] = test_mean_preds\n",
    "\n",
    "# --- STAGE 2, PART 1: K-Fold Aware Tuning of Error Model ---\n",
    "print(\"\\n--- STAGE 2, PART 1: K-Fold Aware Tuning of Error Model ---\")\n",
    "\n",
    "def objective_error_kfold(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method':'hist',\n",
    "        'random_state': RANDOM_STATE, 'n_jobs': -1,\n",
    "        # Use the search space from your most successful error model run (v1)\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.02),\n",
    "        'max_depth': trial.suggest_int('max_depth', 8, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.9, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.72, 0.75),\n",
    "        'lambda': trial.suggest_float('lambda', 0.5, 0.7, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 0.5, 0.65),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 3)\n",
    "    }\n",
    "    fold_scores = []\n",
    "    skf_inner = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    # Use X_for_error in this split\n",
    "    for train_idx, val_idx in skf_inner.split(X_for_error, grade_for_stratify):\n",
    "        X_train, X_val = X_for_error.iloc[train_idx], X_for_error.iloc[val_idx]\n",
    "        y_train, y_val = error_target.iloc[train_idx], error_target.iloc[val_idx]\n",
    "        model = xgb.XGBRegressor(**params, n_estimators=2500, early_stopping_rounds=100)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        preds = model.predict(X_val)\n",
    "        fold_scores.append(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "    return np.mean(fold_scores)\n",
    "\n",
    "study_error = optuna.create_study(direction='minimize')\n",
    "study_error.optimize(objective_error_kfold, n_trials=N_OPTUNA_TRIALS_KFOLD)\n",
    "best_params_error = study_error.best_params\n",
    "print(f\"\\n# Error Model K-Fold Aware Tuning Complete. Best Avg RMSE: ${study_error.best_value:,.2f}\")\n",
    "\n",
    "# --- STAGE 2, PART 2: Final K-Fold Training of Error Model ---\n",
    "print(\"\\n# STAGE 2, PART 2: Final K-Fold Training of Error Model...\")\n",
    "oof_error_preds = np.zeros(len(X))\n",
    "test_error_preds = np.zeros(len(X_test))\n",
    "final_params_error = {'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist', 'random_state': RANDOM_STATE, 'n_jobs': -1, **best_params_error}\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_for_error, grade_for_stratify)):\n",
    "    print(f\"  Error Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**final_params_error, n_estimators=2500, early_stopping_rounds=100)\n",
    "    model.fit(X_for_error.iloc[train_idx], error_target.iloc[train_idx], eval_set=[(X_for_error.iloc[val_idx], error_target.iloc[val_idx])], verbose=False)\n",
    "    oof_error_preds[val_idx] = model.predict(X_for_error.iloc[val_idx])\n",
    "    test_error_preds += model.predict(X_test_for_error) / N_SPLITS\n",
    "\n",
    "final_error_rmse = np.sqrt(mean_squared_error(error_target, oof_error_preds))\n",
    "print(f\"\\n# Error model K-Fold training complete. Final OOF RMSE: ${final_error_rmse:,.2f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17013e94-d877-428b-b671-0005e3b8f999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a41b9b7-8b2b-41cb-8802-2dbe962e5ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Asymmetric Calibration ---\n",
      "\n",
      "Grid search complete. Final OOF Score: 302,021.33. Best multipliers: a=1.96, b=2.17\n",
      "\n",
      "Creating final submission file...\n",
      "\n",
      "'submission_final_v6.csv' created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>838794.670312</td>\n",
       "      <td>1.060866e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>593587.770781</td>\n",
       "      <td>8.215377e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>464649.988477</td>\n",
       "      <td>6.743832e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>281573.330977</td>\n",
       "      <td>4.045716e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>331295.164844</td>\n",
       "      <td>8.423343e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       pi_lower      pi_upper\n",
       "0  200000  838794.670312  1.060866e+06\n",
       "1  200001  593587.770781  8.215377e+05\n",
       "2  200002  464649.988477  6.743832e+05\n",
       "3  200003  281573.330977  4.045716e+05\n",
       "4  200004  331295.164844  8.423343e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL ASYMMETRIC CALIBRATION AND SUBMISSION (ULTIMATE ROBUST VERSION)\n",
    "# =============================================================================\n",
    "print(\"\\n--- Final Asymmetric Calibration ---\")\n",
    "\n",
    "# --- Safely reload y_true to ensure it's available ---\n",
    "y_true = pd.read_csv('./dataset.csv')['sale_price']\n",
    "# --- Your existing correct code ---\n",
    "oof_error_final = np.clip(oof_error_preds, 0, None)\n",
    "best_a, best_b, best_metric = 2.0, 2.0, float('inf')\n",
    "for a in np.arange(1.90, 2.31, 0.01):\n",
    "    for b in np.arange(2.10, 2.51, 0.01):\n",
    "        low = oof_mean_preds - oof_error_final * a\n",
    "        high = oof_mean_preds + oof_error_final * b\n",
    "        # We need the winkler_score function defined here or in a previous cell\n",
    "        metric, coverage = winkler_score(y_true, low, high, alpha=COMPETITION_ALPHA, return_coverage=True)\n",
    "        if metric < best_metric:\n",
    "            best_metric = metric\n",
    "            best_a, best_b = a, b\n",
    "print(f\"\\nGrid search complete. Final OOF Score: {best_metric:,.2f}. Best multipliers: a={best_a:.2f}, b={best_b:.2f}\")    \n",
    "\n",
    "# --- Create Final Submission ---\n",
    "print(\"\\nCreating final submission file...\")\n",
    "test_error_final = np.clip(test_error_preds, 0, None)\n",
    "final_lower = test_mean_preds - test_error_final * best_a\n",
    "final_upper = test_mean_preds + test_error_final * best_b\n",
    "final_upper = np.maximum(final_lower, final_upper)\n",
    "\n",
    "# Your excellent, robust fix for the IDs\n",
    "test_ids = pd.read_csv('./test.csv', usecols=['id'])['id']\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'pi_lower': final_lower,\n",
    "    'pi_upper': final_upper\n",
    "    })\n",
    "\n",
    "submission_df.to_csv('submission_winner_v1_301.csv', index=False)\n",
    "print(\"\\n'submission_final_v6.csv' created successfully!\")\n",
    "display(submission_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3415b5-7f91-4556-8668-0674e99d72b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
