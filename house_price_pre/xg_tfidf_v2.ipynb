{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d873fc5-79e3-41d0-9907-da9cd99a809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm', 'view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bddfc431-ff04-4dc2-bf67-dd8271d45837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Block 2: Synthesized Feature Engineering ---\n",
      "Creating brute-force numerical interaction features...\n",
      "Creating TF-IDF features for text columns...\n",
      "Finalizing feature set...\n",
      "\n",
      "Synthesized FE complete. Total features: 111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 2: SYNTHESIZED FEATURE ENGINEERING (CORRECTED)\n",
    "# =============================================================================\n",
    "print(\"--- Starting Block 2: Synthesized Feature Engineering ---\")\n",
    "\n",
    "def create_synthesized_features(df_train, df_test):\n",
    "    # Combine for consistent processing and reset the index\n",
    "    df_train['is_train'] = 1\n",
    "    df_test['is_train'] = 0\n",
    "    # Store the original id for later, as reset_index will remove it\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    all_data = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # --- A) Brute-Force Numerical Interactions ---\n",
    "    print(\"Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # --- B) Date Features ---\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['year'] = all_data['sale_date'].dt.year\n",
    "    all_data['month'] = all_data['sale_date'].dt.month\n",
    "    all_data['year_diff'] = all_data['year'] - all_data['year_built']\n",
    "\n",
    "    # --- C) TF-IDF Text Features ---\n",
    "    print(\"Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "\n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        # This concat will now work because both have a simple 0-based index\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # --- D) Log transform some of the new interaction features ---\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            # Add a small constant to avoid log(0)\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "            \n",
    "    # --- E) Final Cleanup ---\n",
    "    print(\"Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate final datasets\n",
    "    X = all_data[all_data['is_train'] == 1].drop(columns=['is_train', 'sale_price'])\n",
    "    X_test = all_data[all_data['is_train'] == 0].drop(columns=['is_train', 'sale_price'])\n",
    "    \n",
    "    # Restore the original 'id' as the index\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    \n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    return X, X_test\n",
    "\n",
    "# We need to re-run this from the original dataframes\n",
    "X, X_test = create_synthesized_features(df_train, df_test)\n",
    "\n",
    "print(f\"\\nSynthesized FE complete. Total features: {X.shape[1]}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d57205-46ad-4d8a-a11b-8c6933567a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-05 17:25:12,826] A new study created in memory with name: no-name-66ffed7c-33e9-4194-a1dd-6332ff7e74ad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 3: Two-Stage Modeling Pipeline ---\n",
      "\n",
      "--- STAGE 1, PART 1: Tuning Mean Prediction Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-05 17:25:54,475] Trial 0 finished with value: 100611.20605578685 and parameters: {'eta': 0.04431460410068728, 'max_depth': 10, 'subsample': 0.8080436066986578, 'colsample_bytree': 0.9376283520404535, 'lambda': 1.570836888733768, 'alpha': 3.0256373277931843}. Best is trial 0 with value: 100611.20605578685.\n",
      "[I 2025-07-05 17:26:42,932] Trial 1 finished with value: 101986.75113954753 and parameters: {'eta': 0.03526548125713587, 'max_depth': 11, 'subsample': 0.7170174340601567, 'colsample_bytree': 0.9118744847079123, 'lambda': 0.023682374322150763, 'alpha': 0.0001037231941308145}. Best is trial 0 with value: 100611.20605578685.\n",
      "[I 2025-07-05 17:28:46,683] Trial 2 finished with value: 107213.89283110655 and parameters: {'eta': 0.042941914173944974, 'max_depth': 14, 'subsample': 0.8181208676248621, 'colsample_bytree': 0.9732596169156393, 'lambda': 0.0019841785577542678, 'alpha': 0.3121172690472878}. Best is trial 0 with value: 100611.20605578685.\n",
      "[I 2025-07-05 17:29:39,408] Trial 3 finished with value: 102404.01992109489 and parameters: {'eta': 0.020974059512145517, 'max_depth': 11, 'subsample': 0.8509553725696224, 'colsample_bytree': 0.787555275515627, 'lambda': 0.0001976432683230606, 'alpha': 0.005090274238835147}. Best is trial 0 with value: 100611.20605578685.\n",
      "[I 2025-07-05 17:30:18,812] Trial 4 finished with value: 100875.50695783392 and parameters: {'eta': 0.020077349161374144, 'max_depth': 10, 'subsample': 0.8579997881643564, 'colsample_bytree': 0.98256343580438, 'lambda': 2.02350107794135, 'alpha': 0.03717657483063921}. Best is trial 0 with value: 100611.20605578685.\n",
      "[I 2025-07-05 17:31:14,101] Trial 5 finished with value: 101921.72306235801 and parameters: {'eta': 0.016208608059369835, 'max_depth': 11, 'subsample': 0.7595069530051342, 'colsample_bytree': 0.7921920126112301, 'lambda': 0.00044582399984475525, 'alpha': 0.06469209600107409}. Best is trial 0 with value: 100611.20605578685.\n",
      "[I 2025-07-05 17:32:07,302] Trial 6 finished with value: 102301.93804615825 and parameters: {'eta': 0.02545214156980748, 'max_depth': 11, 'subsample': 0.9465056209455703, 'colsample_bytree': 0.8104478041035088, 'lambda': 0.000628892283494256, 'alpha': 0.41681774913917446}. Best is trial 0 with value: 100611.20605578685.\n",
      "[I 2025-07-05 17:32:30,423] Trial 7 finished with value: 100824.74362972613 and parameters: {'eta': 0.018869761647522798, 'max_depth': 8, 'subsample': 0.7441243118024985, 'colsample_bytree': 0.924293920115025, 'lambda': 0.015034775822514975, 'alpha': 0.0007117445294746982}. Best is trial 0 with value: 100611.20605578685.\n",
      "[I 2025-07-05 17:32:48,462] Trial 8 finished with value: 100815.71940922705 and parameters: {'eta': 0.03937345807187433, 'max_depth': 7, 'subsample': 0.8049207081298912, 'colsample_bytree': 0.8996124413402447, 'lambda': 0.002923457103236055, 'alpha': 0.006541496198419795}. Best is trial 0 with value: 100611.20605578685.\n",
      "[I 2025-07-05 17:33:12,015] Trial 9 finished with value: 100930.58715770952 and parameters: {'eta': 0.06548946442624133, 'max_depth': 9, 'subsample': 0.951006094575481, 'colsample_bytree': 0.781534026495125, 'lambda': 0.006872314465428365, 'alpha': 6.5160029370749735}. Best is trial 0 with value: 100611.20605578685.\n",
      "[I 2025-07-05 17:35:55,467] Trial 10 finished with value: 105453.65859940565 and parameters: {'eta': 0.01054557206115197, 'max_depth': 14, 'subsample': 0.9015481840209884, 'colsample_bytree': 0.7115785799651532, 'lambda': 1.1395860527146424, 'alpha': 6.126507212687441}. Best is trial 0 with value: 100611.20605578685.\n",
      "[I 2025-07-05 17:36:12,681] Trial 11 finished with value: 101316.2753361966 and parameters: {'eta': 0.05606038512682087, 'max_depth': 6, 'subsample': 0.793199785748325, 'colsample_bytree': 0.8951281122580175, 'lambda': 0.2685321615151213, 'alpha': 0.0027447713582693456}. Best is trial 0 with value: 100611.20605578685.\n",
      "[I 2025-07-05 17:36:27,441] Trial 12 finished with value: 101529.00070423228 and parameters: {'eta': 0.04426117133133479, 'max_depth': 6, 'subsample': 0.795169492561681, 'colsample_bytree': 0.869197849901407, 'lambda': 0.17533372701367858, 'alpha': 0.5925210473967494}. Best is trial 0 with value: 100611.20605578685.\n",
      "[I 2025-07-05 17:36:49,279] Trial 13 finished with value: 100051.95962098893 and parameters: {'eta': 0.07811343007557489, 'max_depth': 8, 'subsample': 0.8830049149236417, 'colsample_bytree': 0.9412498707777459, 'lambda': 9.726741762523657, 'alpha': 0.0076783725483656065}. Best is trial 13 with value: 100051.95962098893.\n",
      "[I 2025-07-05 17:37:14,327] Trial 14 finished with value: 100241.74555543215 and parameters: {'eta': 0.07260266978090589, 'max_depth': 9, 'subsample': 0.8965318791838385, 'colsample_bytree': 0.9531023535363621, 'lambda': 9.36823622592545, 'alpha': 1.4923832875090848}. Best is trial 13 with value: 100051.95962098893.\n",
      "[I 2025-07-05 17:37:34,358] Trial 15 finished with value: 99848.19453550475 and parameters: {'eta': 0.0771009951681633, 'max_depth': 8, 'subsample': 0.9028389468081272, 'colsample_bytree': 0.9928280041642248, 'lambda': 8.840991994295145, 'alpha': 0.11882283503740804}. Best is trial 15 with value: 99848.19453550475.\n",
      "[I 2025-07-05 17:37:54,026] Trial 16 finished with value: 100156.33986922645 and parameters: {'eta': 0.0792181658337364, 'max_depth': 8, 'subsample': 0.9150095664607546, 'colsample_bytree': 0.9986396843900642, 'lambda': 8.072214186883148, 'alpha': 0.08311287495171638}. Best is trial 15 with value: 99848.19453550475.\n",
      "[I 2025-07-05 17:38:09,522] Trial 17 finished with value: 100433.07439285128 and parameters: {'eta': 0.05794753469894902, 'max_depth': 7, 'subsample': 0.9878987128233344, 'colsample_bytree': 0.8513608444780755, 'lambda': 0.14102646150408374, 'alpha': 0.014202380823518854}. Best is trial 15 with value: 99848.19453550475.\n",
      "[I 2025-07-05 17:38:29,165] Trial 18 finished with value: 100374.81883420762 and parameters: {'eta': 0.053862947107701904, 'max_depth': 8, 'subsample': 0.8799733132710709, 'colsample_bytree': 0.970882383839652, 'lambda': 0.6127492942042373, 'alpha': 0.0010950313066160017}. Best is trial 15 with value: 99848.19453550475.\n",
      "[I 2025-07-05 17:38:45,631] Trial 19 finished with value: 100511.28684879126 and parameters: {'eta': 0.03090144903920814, 'max_depth': 7, 'subsample': 0.9384146642462211, 'colsample_bytree': 0.9477030638468493, 'lambda': 4.049577321426417, 'alpha': 0.13037226195065327}. Best is trial 15 with value: 99848.19453550475.\n",
      "[I 2025-07-05 17:39:06,714] Trial 20 finished with value: 101764.33198326416 and parameters: {'eta': 0.06457474230082713, 'max_depth': 9, 'subsample': 0.9916679444317734, 'colsample_bytree': 0.9945152447779525, 'lambda': 0.07371387392423547, 'alpha': 0.019480267200154867}. Best is trial 15 with value: 99848.19453550475.\n",
      "[I 2025-07-05 17:39:27,479] Trial 21 finished with value: 100241.91410782219 and parameters: {'eta': 0.07885343923965286, 'max_depth': 8, 'subsample': 0.9124213192782548, 'colsample_bytree': 0.9974530889069674, 'lambda': 9.665803057867441, 'alpha': 0.10822592155682773}. Best is trial 15 with value: 99848.19453550475.\n",
      "[I 2025-07-05 17:39:46,274] Trial 22 finished with value: 100174.5880750203 and parameters: {'eta': 0.07780736533207377, 'max_depth': 8, 'subsample': 0.9270635212728391, 'colsample_bytree': 0.9647176346139834, 'lambda': 4.268770959537739, 'alpha': 0.1523669219847646}. Best is trial 15 with value: 99848.19453550475.\n",
      "[I 2025-07-05 17:40:11,994] Trial 23 finished with value: 100560.54860629988 and parameters: {'eta': 0.05185777813969995, 'max_depth': 9, 'subsample': 0.8664501440559683, 'colsample_bytree': 0.9989875486610283, 'lambda': 0.5135029567331129, 'alpha': 0.039026862124841535}. Best is trial 15 with value: 99848.19453550475.\n",
      "[I 2025-07-05 17:40:28,008] Trial 24 finished with value: 99605.0363385306 and parameters: {'eta': 0.06686624525334232, 'max_depth': 7, 'subsample': 0.9692418227186756, 'colsample_bytree': 0.9365308438034243, 'lambda': 3.66471625245591, 'alpha': 0.009989912763032578}. Best is trial 24 with value: 99605.0363385306.\n",
      "[I 2025-07-05 17:40:41,008] Trial 25 finished with value: 101209.96504297391 and parameters: {'eta': 0.0654454635603015, 'max_depth': 6, 'subsample': 0.9675906460788072, 'colsample_bytree': 0.8776705661879679, 'lambda': 3.2090639809648307, 'alpha': 0.0012792222306605254}. Best is trial 24 with value: 99605.0363385306.\n",
      "[I 2025-07-05 17:40:58,234] Trial 26 finished with value: 100061.9277047969 and parameters: {'eta': 0.05204722450195543, 'max_depth': 7, 'subsample': 0.8297424965970552, 'colsample_bytree': 0.926815439975976, 'lambda': 0.5999336641453112, 'alpha': 0.009552561491692204}. Best is trial 24 with value: 99605.0363385306.\n",
      "[I 2025-07-05 17:41:14,825] Trial 27 finished with value: 99761.96950742301 and parameters: {'eta': 0.0657483883745489, 'max_depth': 7, 'subsample': 0.8839479148108442, 'colsample_bytree': 0.9459978061150607, 'lambda': 1.22572635029498, 'alpha': 0.00434597592365517}. Best is trial 24 with value: 99605.0363385306.\n",
      "[I 2025-07-05 17:41:28,857] Trial 28 finished with value: 101127.80729354316 and parameters: {'eta': 0.06225502054969394, 'max_depth': 6, 'subsample': 0.9684317982649195, 'colsample_bytree': 0.8201133231894607, 'lambda': 0.051755955399511736, 'alpha': 0.0028251575577782503}. Best is trial 24 with value: 99605.0363385306.\n",
      "[I 2025-07-05 17:41:58,441] Trial 29 finished with value: 101569.30055878105 and parameters: {'eta': 0.04908580187855549, 'max_depth': 10, 'subsample': 0.9999868388419045, 'colsample_bytree': 0.9560404434716524, 'lambda': 1.4208927151801296, 'alpha': 0.0002889537717530402}. Best is trial 24 with value: 99605.0363385306.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Model Tuning Complete. Best Validation RMSE: $99,605.04\n",
      "\n",
      "--- STAGE 1, PART 2: K-Fold Training of Mean Model ---\n",
      "  Mean Model - Fold 1/5...\n",
      "  Mean Model - Fold 2/5...\n",
      "  Mean Model - Fold 3/5...\n",
      "  Mean Model - Fold 4/5...\n",
      "  Mean Model - Fold 5/5...\n",
      "\n",
      "--- STAGE 2: K-Fold Training of Error Model ---\n",
      "  Error Model - Fold 1/5...\n",
      "  Error Model - Fold 2/5...\n",
      "  Error Model - Fold 3/5...\n",
      "  Error Model - Fold 4/5...\n",
      "  Error Model - Fold 5/5...\n",
      "\n",
      "--- Final Asymmetric Calibration ---\n",
      "New Best! a=1.90, b=2.10, Score=307,712.47, Cov=88.75%\n",
      "New Best! a=1.90, b=2.11, Score=307,656.71, Cov=88.82%\n",
      "New Best! a=1.90, b=2.12, Score=307,607.80, Cov=88.89%\n",
      "New Best! a=1.90, b=2.13, Score=307,566.14, Cov=88.95%\n",
      "New Best! a=1.90, b=2.14, Score=307,530.18, Cov=89.02%\n",
      "New Best! a=1.90, b=2.15, Score=307,501.82, Cov=89.09%\n",
      "New Best! a=1.90, b=2.16, Score=307,480.49, Cov=89.16%\n",
      "New Best! a=1.90, b=2.17, Score=307,466.32, Cov=89.23%\n",
      "New Best! a=1.90, b=2.18, Score=307,459.89, Cov=89.30%\n",
      "New Best! a=1.91, b=2.15, Score=307,442.90, Cov=89.18%\n",
      "New Best! a=1.91, b=2.16, Score=307,421.57, Cov=89.25%\n",
      "New Best! a=1.91, b=2.17, Score=307,407.40, Cov=89.32%\n",
      "New Best! a=1.91, b=2.18, Score=307,400.96, Cov=89.39%\n",
      "New Best! a=1.92, b=2.15, Score=307,393.76, Cov=89.27%\n",
      "New Best! a=1.92, b=2.16, Score=307,372.44, Cov=89.34%\n",
      "New Best! a=1.92, b=2.17, Score=307,358.27, Cov=89.40%\n",
      "New Best! a=1.92, b=2.18, Score=307,351.83, Cov=89.47%\n",
      "New Best! a=1.93, b=2.16, Score=307,333.20, Cov=89.42%\n",
      "New Best! a=1.93, b=2.17, Score=307,319.03, Cov=89.48%\n",
      "New Best! a=1.93, b=2.18, Score=307,312.59, Cov=89.55%\n",
      "New Best! a=1.94, b=2.16, Score=307,303.18, Cov=89.50%\n",
      "New Best! a=1.94, b=2.17, Score=307,289.02, Cov=89.56%\n",
      "New Best! a=1.94, b=2.18, Score=307,282.58, Cov=89.63%\n",
      "New Best! a=1.95, b=2.16, Score=307,281.44, Cov=89.57%\n",
      "New Best! a=1.95, b=2.17, Score=307,267.27, Cov=89.64%\n",
      "New Best! a=1.95, b=2.18, Score=307,260.83, Cov=89.71%\n",
      "New Best! a=1.96, b=2.17, Score=307,254.73, Cov=89.72%\n",
      "New Best! a=1.96, b=2.18, Score=307,248.29, Cov=89.79%\n",
      "New Best! a=1.97, b=2.18, Score=307,245.33, Cov=89.87%\n",
      "\n",
      "Grid search complete. Final OOF Score: 307,245.33. Best multipliers: a=1.97, b=2.18\n",
      "\n",
      "Creating final submission file...\n",
      "\n",
      "'submission_ultimate_synthesis.csv' created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>824916.976963</td>\n",
       "      <td>1.048226e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>610220.406826</td>\n",
       "      <td>8.243456e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>441503.063955</td>\n",
       "      <td>6.811817e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>275279.479692</td>\n",
       "      <td>3.994264e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>266776.459473</td>\n",
       "      <td>7.608646e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       pi_lower      pi_upper\n",
       "0   0  824916.976963  1.048226e+06\n",
       "1   1  610220.406826  8.243456e+05\n",
       "2   2  441503.063955  6.811817e+05\n",
       "3   3  275279.479692  3.994264e+05\n",
       "4   4  266776.459473  7.608646e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: TWO-STAGE TUNING, TRAINING, AND SUBMISSION\n",
    "# =============================================================================\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"\\n--- Starting Block 3: Two-Stage Modeling Pipeline ---\")\n",
    "\n",
    "# --- STAGE 1: TUNE AND TRAIN THE MEAN MODEL ---\n",
    "def objective_mean(trial):\n",
    "    train_x, val_x, train_y, val_y = train_test_split(X, y_true, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.08, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 14),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-4, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-4, 10.0, log=True),\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params, n_estimators=1500, random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=50)\n",
    "    model.fit(train_x, train_y, eval_set=[(val_x, val_y)], verbose=False)\n",
    "    preds = model.predict(val_x)\n",
    "    return np.sqrt(mean_squared_error(val_y, preds))\n",
    "\n",
    "print(\"\\n--- STAGE 1, PART 1: Tuning Mean Prediction Model ---\")\n",
    "study_mean = optuna.create_study(direction='minimize')\n",
    "study_mean.optimize(objective_mean, n_trials=N_OPTUNA_TRIALS)\n",
    "best_params_mean = study_mean.best_params\n",
    "print(f\"Mean Model Tuning Complete. Best Validation RMSE: ${study_mean.best_value:,.2f}\")\n",
    "\n",
    "print(\"\\n--- STAGE 1, PART 2: K-Fold Training of Mean Model ---\")\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_mean_preds = np.zeros(len(X))\n",
    "test_mean_preds = np.zeros(len(X_test))\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, df_train['grade'])):\n",
    "    print(f\"  Mean Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**best_params_mean, n_estimators=2000, objective='reg:squarederror', eval_metric='rmse', tree_method='hist', random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(X.iloc[train_idx], y_true.iloc[train_idx], eval_set=[(X.iloc[val_idx], y_true.iloc[val_idx])], verbose=False)\n",
    "    oof_mean_preds[val_idx] = model.predict(X.iloc[val_idx])\n",
    "    test_mean_preds += model.predict(X_test) / N_SPLITS\n",
    "\n",
    "# --- STAGE 2: TRAIN THE ERROR MODEL (No tuning for speed, using good defaults) ---\n",
    "print(\"\\n--- STAGE 2: K-Fold Training of Error Model ---\")\n",
    "error_target = np.abs(y_true - oof_mean_preds)\n",
    "X_for_error = X.copy()\n",
    "X_for_error['mean_pred_oof'] = oof_mean_preds\n",
    "X_test_for_error = X_test.copy()\n",
    "X_test_for_error['mean_pred_oof'] = test_mean_preds\n",
    "params_error = {'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist', 'eta': 0.03, 'max_depth': 7, 'random_state': RANDOM_STATE}\n",
    "oof_error_preds = np.zeros(len(X))\n",
    "test_error_preds = np.zeros(len(X_test))\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_for_error, df_train['grade'])):\n",
    "    print(f\"  Error Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**params_error, n_estimators=1500, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(X_for_error.iloc[train_idx], error_target.iloc[train_idx], eval_set=[(X_for_error.iloc[val_idx], error_target.iloc[val_idx])], verbose=False)\n",
    "    oof_error_preds[val_idx] = model.predict(X_for_error.iloc[val_idx])\n",
    "    test_error_preds += model.predict(X_test_for_error) / N_SPLITS\n",
    "\n",
    "# --- FINAL ASYMMETRIC CALIBRATION AND SUBMISSION ---\n",
    "print(\"\\n--- Final Asymmetric Calibration ---\")\n",
    "oof_error_final = np.clip(oof_error_preds, 0, None) # Ensure error is not negative\n",
    "best_a, best_b, best_metric = 2.0, 2.0, float('inf')\n",
    "for a in np.arange(1.90, 2.31, 0.01):\n",
    "    for b in np.arange(2.10, 2.51, 0.01):\n",
    "        low = oof_mean_preds - oof_error_final * a\n",
    "        high = oof_mean_preds + oof_error_final * b\n",
    "        metric, coverage = winkler_score(y_true, low, high, alpha=COMPETITION_ALPHA, return_coverage=True)\n",
    "        if metric < best_metric:\n",
    "            best_metric = metric\n",
    "            best_a, best_b = a, b\n",
    "            print(f\"New Best! a={best_a:.2f}, b={best_b:.2f}, Score={best_metric:,.2f}, Cov={coverage:.2%}\")\n",
    "print(f\"\\nGrid search complete. Final OOF Score: {best_metric:,.2f}. Best multipliers: a={best_a:.2f}, b={best_b:.2f}\")\n",
    "\n",
    "# --- Create Final Submission ---\n",
    "print(\"\\nCreating final submission file...\")\n",
    "test_error_final = np.clip(test_error_preds, 0, None)\n",
    "final_lower = test_mean_preds - test_error_final * best_a\n",
    "final_upper = test_mean_preds + test_error_final * best_b\n",
    "final_upper = np.maximum(final_lower, final_upper)\n",
    "submission_df = pd.DataFrame({'id': X_test.index, 'pi_lower': final_lower, 'pi_upper': final_upper})\n",
    "submission_df.to_csv('submission_ultimate_synthesis.csv', index=False)\n",
    "print(\"\\n'submission_ultimate_synthesis.csv' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dbcb61-58dd-4089-96af-81258985b476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b1f4d-300d-4e32-87f2-e28b20397f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
