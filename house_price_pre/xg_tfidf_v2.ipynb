{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d873fc5-79e3-41d0-9907-da9cd99a809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Raw data loaded successfully.\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 1: SETUP, IMPORTS, AND DATA LOADING\n",
    "# =============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "# --- Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# --- Helper Function for Winkler Score ---\n",
    "def winkler_score(y_true, lower, upper, alpha=0.1, return_coverage=False):\n",
    "    width = upper - lower\n",
    "    penalty_lower = np.where(y_true < lower, (2 / alpha) * (lower - y_true), 0)\n",
    "    penalty_upper = np.where(y_true > upper, (2 / alpha) * (y_true - upper), 0)\n",
    "    score = width + penalty_lower + penalty_upper\n",
    "    if return_coverage:\n",
    "        coverage = np.mean((y_true >= lower) & (y_true <= upper))\n",
    "        return np.mean(score), coverage\n",
    "    return np.mean(score)\n",
    "\n",
    "# --- Global Constants ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = './'\n",
    "N_OPTUNA_TRIALS = 30 # A strong number for a comprehensive search\n",
    "COMPETITION_ALPHA = 0.1\n",
    "\n",
    "# --- Load Raw Data ---\n",
    "try:\n",
    "    # We drop the low-variance columns they identified right away\n",
    "    drop_cols=['id', 'golf', 'view_rainier', 'view_skyline', 'view_lakesamm', 'view_otherwater', 'view_other']\n",
    "    df_train = pd.read_csv(DATA_PATH + 'dataset.csv').drop(columns=drop_cols)\n",
    "    df_test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=drop_cols)\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find 'dataset.csv' or 'test.csv'.\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Target Variable ---\n",
    "y_true = df_train['sale_price'].copy()\n",
    "# The mean-error model works best when predicting the raw price directly\n",
    "# So, we will NOT log-transform the target this time.\n",
    "# df_train.drop('sale_price', axis=1, inplace=True) # We keep sale_price for FE\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bddfc431-ff04-4dc2-bf67-dd8271d45837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Block 2: Synthesized Feature Engineering ---\n",
      "Creating brute-force numerical interaction features...\n",
      "Creating TF-IDF features for text columns...\n",
      "Finalizing feature set...\n",
      "\n",
      "Synthesized FE complete. Total features: 111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 2: SYNTHESIZED FEATURE ENGINEERING (CORRECTED)\n",
    "# =============================================================================\n",
    "print(\"--- Starting Block 2: Synthesized Feature Engineering ---\")\n",
    "\n",
    "def create_synthesized_features(df_train, df_test):\n",
    "    # Combine for consistent processing and reset the index\n",
    "    df_train['is_train'] = 1\n",
    "    df_test['is_train'] = 0\n",
    "    # Store the original id for later, as reset_index will remove it\n",
    "    train_ids = df_train.index\n",
    "    test_ids = df_test.index\n",
    "    all_data = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # --- A) Brute-Force Numerical Interactions ---\n",
    "    print(\"Creating brute-force numerical interaction features...\")\n",
    "    NUMS = ['area', 'land_val', 'imp_val', 'sqft_lot', 'sqft', 'sqft_1', 'grade', 'year_built']\n",
    "    for i in range(len(NUMS)):\n",
    "        for j in range(i + 1, len(NUMS)):\n",
    "            all_data[f'{NUMS[i]}_x_{NUMS[j]}'] = all_data[NUMS[i]] * all_data[NUMS[j]]\n",
    "\n",
    "    # --- B) Date Features ---\n",
    "    all_data['sale_date'] = pd.to_datetime(all_data['sale_date'])\n",
    "    all_data['year'] = all_data['sale_date'].dt.year\n",
    "    all_data['month'] = all_data['sale_date'].dt.month\n",
    "    all_data['year_diff'] = all_data['year'] - all_data['year_built']\n",
    "\n",
    "    # --- C) TF-IDF Text Features ---\n",
    "    print(\"Creating TF-IDF features for text columns...\")\n",
    "    text_cols = ['subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data[text_cols] = all_data[text_cols].fillna('missing').astype(str)\n",
    "\n",
    "    for col in text_cols:\n",
    "        tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=128, binary=True)\n",
    "        tfidf_matrix = tfidf.fit_transform(all_data[col])\n",
    "        svd = TruncatedSVD(n_components=8, random_state=RANDOM_STATE)\n",
    "        tfidf_svd = svd.fit_transform(tfidf_matrix)\n",
    "        tfidf_df = pd.DataFrame(tfidf_svd, columns=[f'{col}_tfidf_svd_{i}' for i in range(8)])\n",
    "        # This concat will now work because both have a simple 0-based index\n",
    "        all_data = pd.concat([all_data, tfidf_df], axis=1)\n",
    "\n",
    "    # --- D) Log transform some of the new interaction features ---\n",
    "    for c in ['land_val_x_imp_val', 'land_val_x_sqft', 'imp_val_x_sqft']:\n",
    "        if c in all_data.columns:\n",
    "            # Add a small constant to avoid log(0)\n",
    "            all_data[c] = np.log1p(all_data[c].fillna(0))\n",
    "            \n",
    "    # --- E) Final Cleanup ---\n",
    "    print(\"Finalizing feature set...\")\n",
    "    cols_to_drop = ['sale_date', 'subdivision', 'zoning', 'city', 'sale_warning', 'join_status', 'submarket']\n",
    "    all_data = all_data.drop(columns=cols_to_drop)\n",
    "    all_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate final datasets\n",
    "    X = all_data[all_data['is_train'] == 1].drop(columns=['is_train', 'sale_price'])\n",
    "    X_test = all_data[all_data['is_train'] == 0].drop(columns=['is_train', 'sale_price'])\n",
    "    \n",
    "    # Restore the original 'id' as the index\n",
    "    X.index = train_ids\n",
    "    X_test.index = test_ids\n",
    "    \n",
    "    X_test = X_test[X.columns]\n",
    "    \n",
    "    return X, X_test\n",
    "\n",
    "# We need to re-run this from the original dataframes\n",
    "X, X_test = create_synthesized_features(df_train, df_test)\n",
    "\n",
    "print(f\"\\nSynthesized FE complete. Total features: {X.shape[1]}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d57205-46ad-4d8a-a11b-8c6933567a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 18:47:59,844] A new study created in memory with name: no-name-b04a7426-01fc-40d9-be0d-37229afb49b1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Block 3: Two-Stage Modeling Pipeline ---\n",
      "\n",
      "--- STAGE 1, PART 1: Tuning Mean Prediction Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 18:49:23,902] Trial 0 finished with value: 108788.92178893952 and parameters: {'eta': 0.06390775093802926, 'max_depth': 14, 'subsample': 0.9801996480974612, 'colsample_bytree': 0.8955994797575587, 'lambda': 0.0014640381248415897, 'alpha': 8.476133865702552}. Best is trial 0 with value: 108788.92178893952.\n",
      "[I 2025-07-06 18:49:38,858] Trial 1 finished with value: 101225.98496433611 and parameters: {'eta': 0.04747122154060738, 'max_depth': 6, 'subsample': 0.9606143808140577, 'colsample_bytree': 0.8791188529638809, 'lambda': 0.00011325690288851288, 'alpha': 4.605557557546353}. Best is trial 1 with value: 101225.98496433611.\n",
      "[I 2025-07-06 18:51:21,459] Trial 2 finished with value: 105649.01650275785 and parameters: {'eta': 0.023345559921624893, 'max_depth': 13, 'subsample': 0.9617487529268152, 'colsample_bytree': 0.8804904884526665, 'lambda': 0.17496762093974744, 'alpha': 0.14063427122774758}. Best is trial 1 with value: 101225.98496433611.\n",
      "[I 2025-07-06 18:52:38,223] Trial 3 finished with value: 103890.95575650461 and parameters: {'eta': 0.047886011264406586, 'max_depth': 13, 'subsample': 0.9106189413762433, 'colsample_bytree': 0.989767625365243, 'lambda': 5.183926279746687, 'alpha': 0.17364285585779082}. Best is trial 1 with value: 101225.98496433611.\n",
      "[I 2025-07-06 18:54:07,550] Trial 4 finished with value: 106075.39537517642 and parameters: {'eta': 0.04092544524427054, 'max_depth': 13, 'subsample': 0.9760318921354161, 'colsample_bytree': 0.7406986473400196, 'lambda': 0.3286394995662939, 'alpha': 0.10638661555887607}. Best is trial 1 with value: 101225.98496433611.\n",
      "[I 2025-07-06 18:54:46,286] Trial 5 finished with value: 102183.78172684743 and parameters: {'eta': 0.04822616013939768, 'max_depth': 11, 'subsample': 0.9417766339419962, 'colsample_bytree': 0.7169740505674642, 'lambda': 1.1096925573737813, 'alpha': 0.07418130135528649}. Best is trial 1 with value: 101225.98496433611.\n",
      "[I 2025-07-06 18:55:05,170] Trial 6 finished with value: 103836.42520811278 and parameters: {'eta': 0.011258095408937374, 'max_depth': 7, 'subsample': 0.8735188852205118, 'colsample_bytree': 0.9430543098228378, 'lambda': 0.00019396794523173392, 'alpha': 0.00405596414492399}. Best is trial 1 with value: 101225.98496433611.\n",
      "[I 2025-07-06 18:55:52,540] Trial 7 finished with value: 101911.30386762795 and parameters: {'eta': 0.021271633460834233, 'max_depth': 11, 'subsample': 0.9548465664270155, 'colsample_bytree': 0.7891537489733838, 'lambda': 0.7579033586274493, 'alpha': 3.1380391108584544}. Best is trial 1 with value: 101225.98496433611.\n",
      "[I 2025-07-06 18:56:29,608] Trial 8 finished with value: 100881.9273408275 and parameters: {'eta': 0.013583466423124448, 'max_depth': 10, 'subsample': 0.8230966685428741, 'colsample_bytree': 0.712080051261473, 'lambda': 0.04675492936121426, 'alpha': 0.009899444483635288}. Best is trial 8 with value: 100881.9273408275.\n",
      "[I 2025-07-06 18:57:01,177] Trial 9 finished with value: 103603.18636026597 and parameters: {'eta': 0.058207249686360166, 'max_depth': 11, 'subsample': 0.7796073230497832, 'colsample_bytree': 0.8355240692686824, 'lambda': 0.003545452074614568, 'alpha': 0.0001840912743587255}. Best is trial 8 with value: 100881.9273408275.\n",
      "[I 2025-07-06 18:57:23,639] Trial 10 finished with value: 102068.0068189832 and parameters: {'eta': 0.011433255492396035, 'max_depth': 8, 'subsample': 0.7004379549448861, 'colsample_bytree': 0.7811218786286678, 'lambda': 0.025017339049408845, 'alpha': 0.0036630784402125463}. Best is trial 8 with value: 100881.9273408275.\n",
      "[I 2025-07-06 18:57:37,627] Trial 11 finished with value: 102761.1282149043 and parameters: {'eta': 0.030750973464577978, 'max_depth': 6, 'subsample': 0.804371278111851, 'colsample_bytree': 0.8349401651452176, 'lambda': 0.021682338848466372, 'alpha': 0.0062580048207264575}. Best is trial 8 with value: 100881.9273408275.\n",
      "[I 2025-07-06 18:58:05,674] Trial 12 finished with value: 101033.05122582412 and parameters: {'eta': 0.01624872479949244, 'max_depth': 9, 'subsample': 0.8246609732870823, 'colsample_bytree': 0.9121598832225097, 'lambda': 0.0001339780761833835, 'alpha': 1.0061808553267524}. Best is trial 8 with value: 100881.9273408275.\n",
      "[I 2025-07-06 18:58:33,079] Trial 13 finished with value: 100909.7053806025 and parameters: {'eta': 0.0159792367789417, 'max_depth': 9, 'subsample': 0.8140783653385786, 'colsample_bytree': 0.9399740903658242, 'lambda': 0.0019806501126662323, 'alpha': 0.7991000453929545}. Best is trial 8 with value: 100881.9273408275.\n",
      "[I 2025-07-06 18:59:06,335] Trial 14 finished with value: 101161.0296507504 and parameters: {'eta': 0.01599231083533947, 'max_depth': 9, 'subsample': 0.737497520522429, 'colsample_bytree': 0.9800722313406816, 'lambda': 0.003420855272602836, 'alpha': 0.01197818403481134}. Best is trial 8 with value: 100881.9273408275.\n",
      "[I 2025-07-06 18:59:51,432] Trial 15 finished with value: 101527.25080489473 and parameters: {'eta': 0.014716654468194045, 'max_depth': 10, 'subsample': 0.8697281289200655, 'colsample_bytree': 0.9509843278978418, 'lambda': 0.09288986757462214, 'alpha': 0.00030312084984313}. Best is trial 8 with value: 100881.9273408275.\n",
      "[I 2025-07-06 19:00:25,909] Trial 16 finished with value: 100406.9033483256 and parameters: {'eta': 0.022440097212148312, 'max_depth': 9, 'subsample': 0.7705602147492955, 'colsample_bytree': 0.7880031124602239, 'lambda': 0.0008656777573046573, 'alpha': 0.5102671883987259}. Best is trial 16 with value: 100406.9033483256.\n",
      "[I 2025-07-06 19:00:49,868] Trial 17 finished with value: 99830.56365662772 and parameters: {'eta': 0.030368340461792535, 'max_depth': 8, 'subsample': 0.765498257049984, 'colsample_bytree': 0.7663755931415053, 'lambda': 0.0006217296769339828, 'alpha': 0.0009706864899267193}. Best is trial 17 with value: 99830.56365662772.\n",
      "[I 2025-07-06 19:01:15,081] Trial 18 finished with value: 100198.16269772615 and parameters: {'eta': 0.03185501561915672, 'max_depth': 8, 'subsample': 0.7551768789641409, 'colsample_bytree': 0.7836692463376219, 'lambda': 0.0009063068942713728, 'alpha': 0.0008420824997831356}. Best is trial 17 with value: 99830.56365662772.\n",
      "[I 2025-07-06 19:01:33,644] Trial 19 finished with value: 100808.07078800784 and parameters: {'eta': 0.03281793615151723, 'max_depth': 7, 'subsample': 0.7344461326812641, 'colsample_bytree': 0.7571722800479184, 'lambda': 0.00043772288266445014, 'alpha': 0.0007643472643953118}. Best is trial 17 with value: 99830.56365662772.\n",
      "[I 2025-07-06 19:01:51,740] Trial 20 finished with value: 101490.75827877138 and parameters: {'eta': 0.07564430326033475, 'max_depth': 7, 'subsample': 0.7480612706114627, 'colsample_bytree': 0.8046233957106319, 'lambda': 0.010980657440703721, 'alpha': 0.0010100959845112908}. Best is trial 17 with value: 99830.56365662772.\n",
      "[I 2025-07-06 19:02:13,005] Trial 21 finished with value: 100029.77348769715 and parameters: {'eta': 0.023418570304782147, 'max_depth': 8, 'subsample': 0.7748142009173121, 'colsample_bytree': 0.7496148963092684, 'lambda': 0.0006583160108336488, 'alpha': 0.0010656521091506708}. Best is trial 17 with value: 99830.56365662772.\n",
      "[I 2025-07-06 19:02:32,954] Trial 22 finished with value: 100093.67260721329 and parameters: {'eta': 0.02646752995427017, 'max_depth': 8, 'subsample': 0.7012352069580774, 'colsample_bytree': 0.7503401131663296, 'lambda': 0.0004734846840413471, 'alpha': 0.001258629437412626}. Best is trial 17 with value: 99830.56365662772.\n",
      "[I 2025-07-06 19:02:54,295] Trial 23 finished with value: 100085.5339796916 and parameters: {'eta': 0.02535413764410559, 'max_depth': 8, 'subsample': 0.702226373443501, 'colsample_bytree': 0.7489269837532762, 'lambda': 0.0004222630357940538, 'alpha': 0.00011599997307863717}. Best is trial 17 with value: 99830.56365662772.\n",
      "[I 2025-07-06 19:03:14,366] Trial 24 finished with value: 100661.04962695352 and parameters: {'eta': 0.019684881938694988, 'max_depth': 8, 'subsample': 0.7205147049478658, 'colsample_bytree': 0.7022999226260608, 'lambda': 0.006597798425064227, 'alpha': 0.00010117627344357919}. Best is trial 17 with value: 99830.56365662772.\n",
      "[I 2025-07-06 19:03:31,787] Trial 25 finished with value: 100958.31123785698 and parameters: {'eta': 0.0377041489298073, 'max_depth': 7, 'subsample': 0.7769199573374863, 'colsample_bytree': 0.8152759023478299, 'lambda': 0.0003246117659665712, 'alpha': 0.0002971485766266463}. Best is trial 17 with value: 99830.56365662772.\n",
      "[I 2025-07-06 19:03:45,673] Trial 26 finished with value: 102864.82998576335 and parameters: {'eta': 0.02563267226622926, 'max_depth': 6, 'subsample': 0.7961088378556715, 'colsample_bytree': 0.7320306511042478, 'lambda': 0.0007733516945250431, 'alpha': 0.0023458690906484145}. Best is trial 17 with value: 99830.56365662772.\n",
      "[I 2025-07-06 19:04:22,707] Trial 27 finished with value: 100713.80187442037 and parameters: {'eta': 0.019483715505541677, 'max_depth': 10, 'subsample': 0.8439655054546787, 'colsample_bytree': 0.7626442895032595, 'lambda': 0.006332148464395949, 'alpha': 0.0004232853567466794}. Best is trial 17 with value: 99830.56365662772.\n",
      "[I 2025-07-06 19:04:45,872] Trial 28 finished with value: 100055.18349391 and parameters: {'eta': 0.03549387470110669, 'max_depth': 8, 'subsample': 0.7225621181135319, 'colsample_bytree': 0.72980967524525, 'lambda': 0.0019211829558217015, 'alpha': 0.021138884721159594}. Best is trial 17 with value: 99830.56365662772.\n",
      "[I 2025-07-06 19:05:05,157] Trial 29 finished with value: 100879.77034073778 and parameters: {'eta': 0.036913763102981396, 'max_depth': 7, 'subsample': 0.7614697877387266, 'colsample_bytree': 0.7278375945771134, 'lambda': 0.0018770928093738847, 'alpha': 0.022628921216990054}. Best is trial 17 with value: 99830.56365662772.\n",
      "[I 2025-07-06 19:05:37,198] Trial 30 finished with value: 99891.7353137886 and parameters: {'eta': 0.029133573564654937, 'max_depth': 9, 'subsample': 0.7901535715388027, 'colsample_bytree': 0.768990338921499, 'lambda': 0.0018721280989633302, 'alpha': 0.04499473331854104}. Best is trial 17 with value: 99830.56365662772.\n",
      "[I 2025-07-06 19:06:09,837] Trial 31 finished with value: 99711.33247530092 and parameters: {'eta': 0.02923798693903738, 'max_depth': 9, 'subsample': 0.7912732271970704, 'colsample_bytree': 0.7728524748804043, 'lambda': 0.002965163281426026, 'alpha': 0.03592701790401984}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:06:40,925] Trial 32 finished with value: 99762.09268053673 and parameters: {'eta': 0.02947252724464211, 'max_depth': 9, 'subsample': 0.7900164742249243, 'colsample_bytree': 0.7664812898210535, 'lambda': 0.011125502894707144, 'alpha': 0.07588038904202651}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:07:22,244] Trial 33 finished with value: 100925.84905761259 and parameters: {'eta': 0.028483997645949888, 'max_depth': 10, 'subsample': 0.7931648316651991, 'colsample_bytree': 0.8600122746911636, 'lambda': 0.013423209592124404, 'alpha': 0.045865138410117696}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:07:54,595] Trial 34 finished with value: 100446.18029571856 and parameters: {'eta': 0.04103174715514506, 'max_depth': 9, 'subsample': 0.7908694559569278, 'colsample_bytree': 0.7708934243609737, 'lambda': 0.004323955111716323, 'alpha': 0.29587964917142556}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:09:17,205] Trial 35 finished with value: 103802.20945625387 and parameters: {'eta': 0.027815840217011514, 'max_depth': 12, 'subsample': 0.8424428743273236, 'colsample_bytree': 0.809476932335311, 'lambda': 0.0014073273539431743, 'alpha': 0.037240605227119306}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:09:49,296] Trial 36 finished with value: 100755.12397888258 and parameters: {'eta': 0.041583897316135356, 'max_depth': 9, 'subsample': 0.9994161021957068, 'colsample_bytree': 0.8274569372727181, 'lambda': 0.008222861231654586, 'alpha': 0.07240230086919}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:12:37,275] Trial 37 finished with value: 107165.3340964325 and parameters: {'eta': 0.04723558188026718, 'max_depth': 14, 'subsample': 0.8561014854465151, 'colsample_bytree': 0.8695111504776633, 'lambda': 0.0406586485712769, 'alpha': 0.20555510057524948}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:14:06,202] Trial 38 finished with value: 100786.42702268991 and parameters: {'eta': 0.030308530444059793, 'max_depth': 10, 'subsample': 0.9008328556963625, 'colsample_bytree': 0.7960006351785437, 'lambda': 0.00022429720384699005, 'alpha': 0.09203312685545433}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:14:45,074] Trial 39 finished with value: 101468.38719522451 and parameters: {'eta': 0.055105915050410056, 'max_depth': 11, 'subsample': 0.8292709358218733, 'colsample_bytree': 0.7693909651289066, 'lambda': 4.1650015262865905, 'alpha': 0.013035261226416284}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:16:17,780] Trial 40 finished with value: 103479.45542956825 and parameters: {'eta': 0.034392783518694146, 'max_depth': 12, 'subsample': 0.8011076428847685, 'colsample_bytree': 0.848525682670853, 'lambda': 0.07759208605988517, 'alpha': 0.06249152142877304}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:17:09,624] Trial 41 finished with value: 100058.27933759405 and parameters: {'eta': 0.02426298227078566, 'max_depth': 9, 'subsample': 0.777495833334094, 'colsample_bytree': 0.7431379014891821, 'lambda': 0.001263175025888993, 'alpha': 0.0018996467449786593}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:17:38,100] Trial 42 finished with value: 100258.81898366846 and parameters: {'eta': 0.02054711112056514, 'max_depth': 8, 'subsample': 0.7547500984499663, 'colsample_bytree': 0.7729852916706885, 'lambda': 0.0040514162923710436, 'alpha': 0.00644629179702062}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:18:06,704] Trial 43 finished with value: 100120.10962838585 and parameters: {'eta': 0.01862161943355914, 'max_depth': 9, 'subsample': 0.8112528891149182, 'colsample_bytree': 0.712724874726148, 'lambda': 0.002776372571522036, 'alpha': 0.022872451899637044}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:18:33,705] Trial 44 finished with value: 100171.71560874856 and parameters: {'eta': 0.028273108367527423, 'max_depth': 8, 'subsample': 0.7860881383814887, 'colsample_bytree': 0.7611851888822712, 'lambda': 0.0001094752798302, 'alpha': 0.1338021044916765}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:19:22,176] Trial 45 finished with value: 100835.91487163688 and parameters: {'eta': 0.02427132606573023, 'max_depth': 10, 'subsample': 0.7609875267418655, 'colsample_bytree': 0.8195428317259089, 'lambda': 0.0006440733934143148, 'alpha': 0.2948018943323201}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:19:55,200] Trial 46 finished with value: 100321.49536365573 and parameters: {'eta': 0.017925748153858927, 'max_depth': 9, 'subsample': 0.7421629173545105, 'colsample_bytree': 0.7381961722772936, 'lambda': 0.014372260635172406, 'alpha': 3.263755570022704}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:20:20,099] Trial 47 finished with value: 100470.25685246356 and parameters: {'eta': 0.022571298034295195, 'max_depth': 8, 'subsample': 0.7689656475583764, 'colsample_bytree': 0.8018534093059438, 'lambda': 0.00020878409901426776, 'alpha': 0.007213537914876771}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:20:35,938] Trial 48 finished with value: 102729.69424660038 and parameters: {'eta': 0.02964518898708907, 'max_depth': 6, 'subsample': 0.8106861147871735, 'colsample_bytree': 0.7201375807375018, 'lambda': 0.022114815292239826, 'alpha': 0.0036645100705357597}. Best is trial 31 with value: 99711.33247530092.\n",
      "[I 2025-07-06 19:20:55,121] Trial 49 finished with value: 100413.01716411074 and parameters: {'eta': 0.03870385563039345, 'max_depth': 7, 'subsample': 0.8307379171425087, 'colsample_bytree': 0.7832179929983385, 'lambda': 0.27791989014873864, 'alpha': 8.359399957261175}. Best is trial 31 with value: 99711.33247530092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Model Tuning Complete. Best Validation RMSE: $99,711.33\n",
      "\n",
      "--- STAGE 1, PART 2: K-Fold Training of Mean Model ---\n",
      "  Mean Model - Fold 1/5...\n",
      "  Mean Model - Fold 2/5...\n",
      "  Mean Model - Fold 3/5...\n",
      "  Mean Model - Fold 4/5...\n",
      "  Mean Model - Fold 5/5...\n",
      "\n",
      "--- STAGE 2: K-Fold Training of Error Model ---\n",
      "  Error Model - Fold 1/5...\n",
      "  Error Model - Fold 2/5...\n",
      "  Error Model - Fold 3/5...\n",
      "  Error Model - Fold 4/5...\n",
      "  Error Model - Fold 5/5...\n",
      "\n",
      "--- Final Asymmetric Calibration ---\n",
      "New Best! a=1.90, b=2.10, Score=305,995.06, Cov=88.76%\n",
      "New Best! a=1.90, b=2.11, Score=305,944.31, Cov=88.83%\n",
      "New Best! a=1.90, b=2.12, Score=305,900.40, Cov=88.89%\n",
      "New Best! a=1.90, b=2.13, Score=305,862.05, Cov=88.94%\n",
      "New Best! a=1.90, b=2.14, Score=305,830.10, Cov=89.00%\n",
      "New Best! a=1.90, b=2.15, Score=305,804.18, Cov=89.06%\n",
      "New Best! a=1.90, b=2.16, Score=305,784.71, Cov=89.12%\n",
      "New Best! a=1.90, b=2.17, Score=305,772.83, Cov=89.19%\n",
      "New Best! a=1.90, b=2.18, Score=305,766.56, Cov=89.24%\n",
      "New Best! a=1.91, b=2.14, Score=305,752.56, Cov=89.08%\n",
      "New Best! a=1.91, b=2.15, Score=305,726.64, Cov=89.14%\n",
      "New Best! a=1.91, b=2.16, Score=305,707.17, Cov=89.21%\n",
      "New Best! a=1.91, b=2.17, Score=305,695.29, Cov=89.27%\n",
      "New Best! a=1.91, b=2.18, Score=305,689.02, Cov=89.32%\n",
      "New Best! a=1.92, b=2.14, Score=305,683.81, Cov=89.17%\n",
      "New Best! a=1.92, b=2.15, Score=305,657.89, Cov=89.23%\n",
      "New Best! a=1.92, b=2.16, Score=305,638.42, Cov=89.29%\n",
      "New Best! a=1.92, b=2.17, Score=305,626.54, Cov=89.35%\n",
      "New Best! a=1.92, b=2.18, Score=305,620.27, Cov=89.41%\n",
      "New Best! a=1.93, b=2.15, Score=305,598.57, Cov=89.32%\n",
      "New Best! a=1.93, b=2.16, Score=305,579.11, Cov=89.38%\n",
      "New Best! a=1.93, b=2.17, Score=305,567.23, Cov=89.44%\n",
      "New Best! a=1.93, b=2.18, Score=305,560.95, Cov=89.50%\n",
      "New Best! a=1.94, b=2.15, Score=305,546.87, Cov=89.39%\n",
      "New Best! a=1.94, b=2.16, Score=305,527.41, Cov=89.45%\n",
      "New Best! a=1.94, b=2.17, Score=305,515.53, Cov=89.52%\n",
      "New Best! a=1.94, b=2.18, Score=305,509.25, Cov=89.57%\n",
      "New Best! a=1.95, b=2.15, Score=305,503.75, Cov=89.47%\n",
      "New Best! a=1.95, b=2.16, Score=305,484.28, Cov=89.53%\n",
      "New Best! a=1.95, b=2.17, Score=305,472.40, Cov=89.59%\n",
      "New Best! a=1.95, b=2.18, Score=305,466.13, Cov=89.65%\n",
      "New Best! a=1.96, b=2.16, Score=305,449.69, Cov=89.62%\n",
      "New Best! a=1.96, b=2.17, Score=305,437.81, Cov=89.68%\n",
      "New Best! a=1.96, b=2.18, Score=305,431.54, Cov=89.74%\n",
      "New Best! a=1.97, b=2.16, Score=305,424.64, Cov=89.70%\n",
      "New Best! a=1.97, b=2.17, Score=305,412.76, Cov=89.76%\n",
      "New Best! a=1.97, b=2.18, Score=305,406.48, Cov=89.81%\n",
      "New Best! a=1.98, b=2.17, Score=305,397.21, Cov=89.85%\n",
      "New Best! a=1.98, b=2.18, Score=305,390.94, Cov=89.91%\n",
      "New Best! a=1.99, b=2.17, Score=305,390.84, Cov=89.93%\n",
      "New Best! a=1.99, b=2.18, Score=305,384.56, Cov=89.98%\n",
      "\n",
      "Grid search complete. Final OOF Score: 305,384.56. Best multipliers: a=1.99, b=2.18\n",
      "\n",
      "Creating final submission file...\n",
      "\n",
      "'submission_ultimate_synthesis.csv' created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>822391.902314</td>\n",
       "      <td>1.038016e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>603598.815254</td>\n",
       "      <td>8.239731e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>442467.346729</td>\n",
       "      <td>6.716598e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>279592.376924</td>\n",
       "      <td>4.055703e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>344854.317754</td>\n",
       "      <td>7.795774e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       pi_lower      pi_upper\n",
       "0   0  822391.902314  1.038016e+06\n",
       "1   1  603598.815254  8.239731e+05\n",
       "2   2  442467.346729  6.716598e+05\n",
       "3   3  279592.376924  4.055703e+05\n",
       "4   4  344854.317754  7.795774e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BLOCK 3: TWO-STAGE TUNING, TRAINING, AND SUBMISSION\n",
    "# =============================================================================\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"\\n--- Starting Block 3: Two-Stage Modeling Pipeline ---\")\n",
    "\n",
    "# --- STAGE 1: TUNE AND TRAIN THE MEAN MODEL ---\n",
    "def objective_mean(trial):\n",
    "    train_x, val_x, train_y, val_y = train_test_split(X, y_true, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.08, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 14),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-4, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-4, 10.0, log=True),\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params, n_estimators=1500, random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=50)\n",
    "    model.fit(train_x, train_y, eval_set=[(val_x, val_y)], verbose=False)\n",
    "    preds = model.predict(val_x)\n",
    "    return np.sqrt(mean_squared_error(val_y, preds))\n",
    "\n",
    "print(\"\\n--- STAGE 1, PART 1: Tuning Mean Prediction Model ---\")\n",
    "study_mean = optuna.create_study(direction='minimize')\n",
    "study_mean.optimize(objective_mean, n_trials=N_OPTUNA_TRIALS)\n",
    "best_params_mean = study_mean.best_params\n",
    "print(f\"Mean Model Tuning Complete. Best Validation RMSE: ${study_mean.best_value:,.2f}\")\n",
    "\n",
    "print(\"\\n--- STAGE 1, PART 2: K-Fold Training of Mean Model ---\")\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_mean_preds = np.zeros(len(X))\n",
    "test_mean_preds = np.zeros(len(X_test))\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, df_train['grade'])):\n",
    "    print(f\"  Mean Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**best_params_mean, n_estimators=2000, objective='reg:squarederror', eval_metric='rmse', tree_method='hist', random_state=RANDOM_STATE, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(X.iloc[train_idx], y_true.iloc[train_idx], eval_set=[(X.iloc[val_idx], y_true.iloc[val_idx])], verbose=False)\n",
    "    oof_mean_preds[val_idx] = model.predict(X.iloc[val_idx])\n",
    "    test_mean_preds += model.predict(X_test) / N_SPLITS\n",
    "\n",
    "# --- STAGE 2: TRAIN THE ERROR MODEL (No tuning for speed, using good defaults) ---\n",
    "print(\"\\n--- STAGE 2: K-Fold Training of Error Model ---\")\n",
    "error_target = np.abs(y_true - oof_mean_preds)\n",
    "X_for_error = X.copy()\n",
    "X_for_error['mean_pred_oof'] = oof_mean_preds\n",
    "X_test_for_error = X_test.copy()\n",
    "X_test_for_error['mean_pred_oof'] = test_mean_preds\n",
    "params_error = {'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist', 'eta': 0.03, 'max_depth': 7, 'random_state': RANDOM_STATE}\n",
    "oof_error_preds = np.zeros(len(X))\n",
    "test_error_preds = np.zeros(len(X_test))\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_for_error, df_train['grade'])):\n",
    "    print(f\"  Error Model - Fold {fold+1}/{N_SPLITS}...\")\n",
    "    model = xgb.XGBRegressor(**params_error, n_estimators=1500, n_jobs=-1, early_stopping_rounds=100)\n",
    "    model.fit(X_for_error.iloc[train_idx], error_target.iloc[train_idx], eval_set=[(X_for_error.iloc[val_idx], error_target.iloc[val_idx])], verbose=False)\n",
    "    oof_error_preds[val_idx] = model.predict(X_for_error.iloc[val_idx])\n",
    "    test_error_preds += model.predict(X_test_for_error) / N_SPLITS\n",
    "\n",
    "# --- FINAL ASYMMETRIC CALIBRATION AND SUBMISSION ---\n",
    "print(\"\\n--- Final Asymmetric Calibration ---\")\n",
    "oof_error_final = np.clip(oof_error_preds, 0, None) # Ensure error is not negative\n",
    "best_a, best_b, best_metric = 2.0, 2.0, float('inf')\n",
    "for a in np.arange(1.90, 2.31, 0.01):\n",
    "    for b in np.arange(2.10, 2.51, 0.01):\n",
    "        low = oof_mean_preds - oof_error_final * a\n",
    "        high = oof_mean_preds + oof_error_final * b\n",
    "        metric, coverage = winkler_score(y_true, low, high, alpha=COMPETITION_ALPHA, return_coverage=True)\n",
    "        if metric < best_metric:\n",
    "            best_metric = metric\n",
    "            best_a, best_b = a, b\n",
    "            print(f\"New Best! a={best_a:.2f}, b={best_b:.2f}, Score={best_metric:,.2f}, Cov={coverage:.2%}\")\n",
    "print(f\"\\nGrid search complete. Final OOF Score: {best_metric:,.2f}. Best multipliers: a={best_a:.2f}, b={best_b:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e223ddf-dc30-478f-a090-01215061b8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Asymmetric Calibration ---\n",
      "New Best! a=1.90, b=2.10, Score=305,995.06, Cov=88.76%\n",
      "New Best! a=1.90, b=2.11, Score=305,944.31, Cov=88.83%\n",
      "New Best! a=1.90, b=2.12, Score=305,900.40, Cov=88.89%\n",
      "New Best! a=1.90, b=2.13, Score=305,862.05, Cov=88.94%\n",
      "New Best! a=1.90, b=2.14, Score=305,830.10, Cov=89.00%\n",
      "New Best! a=1.90, b=2.15, Score=305,804.18, Cov=89.06%\n",
      "New Best! a=1.90, b=2.16, Score=305,784.71, Cov=89.12%\n",
      "New Best! a=1.90, b=2.17, Score=305,772.83, Cov=89.19%\n",
      "New Best! a=1.90, b=2.18, Score=305,766.56, Cov=89.24%\n",
      "New Best! a=1.91, b=2.14, Score=305,752.56, Cov=89.08%\n",
      "New Best! a=1.91, b=2.15, Score=305,726.64, Cov=89.14%\n",
      "New Best! a=1.91, b=2.16, Score=305,707.17, Cov=89.21%\n",
      "New Best! a=1.91, b=2.17, Score=305,695.29, Cov=89.27%\n",
      "New Best! a=1.91, b=2.18, Score=305,689.02, Cov=89.32%\n",
      "New Best! a=1.92, b=2.14, Score=305,683.81, Cov=89.17%\n",
      "New Best! a=1.92, b=2.15, Score=305,657.89, Cov=89.23%\n",
      "New Best! a=1.92, b=2.16, Score=305,638.42, Cov=89.29%\n",
      "New Best! a=1.92, b=2.17, Score=305,626.54, Cov=89.35%\n",
      "New Best! a=1.92, b=2.18, Score=305,620.27, Cov=89.41%\n",
      "New Best! a=1.93, b=2.15, Score=305,598.57, Cov=89.32%\n",
      "New Best! a=1.93, b=2.16, Score=305,579.11, Cov=89.38%\n",
      "New Best! a=1.93, b=2.17, Score=305,567.23, Cov=89.44%\n",
      "New Best! a=1.93, b=2.18, Score=305,560.95, Cov=89.50%\n",
      "New Best! a=1.94, b=2.15, Score=305,546.87, Cov=89.39%\n",
      "New Best! a=1.94, b=2.16, Score=305,527.41, Cov=89.45%\n",
      "New Best! a=1.94, b=2.17, Score=305,515.53, Cov=89.52%\n",
      "New Best! a=1.94, b=2.18, Score=305,509.25, Cov=89.57%\n",
      "New Best! a=1.95, b=2.15, Score=305,503.75, Cov=89.47%\n",
      "New Best! a=1.95, b=2.16, Score=305,484.28, Cov=89.53%\n",
      "New Best! a=1.95, b=2.17, Score=305,472.40, Cov=89.59%\n",
      "New Best! a=1.95, b=2.18, Score=305,466.13, Cov=89.65%\n",
      "New Best! a=1.96, b=2.16, Score=305,449.69, Cov=89.62%\n",
      "New Best! a=1.96, b=2.17, Score=305,437.81, Cov=89.68%\n",
      "New Best! a=1.96, b=2.18, Score=305,431.54, Cov=89.74%\n",
      "New Best! a=1.97, b=2.16, Score=305,424.64, Cov=89.70%\n",
      "New Best! a=1.97, b=2.17, Score=305,412.76, Cov=89.76%\n",
      "New Best! a=1.97, b=2.18, Score=305,406.48, Cov=89.81%\n",
      "New Best! a=1.98, b=2.17, Score=305,397.21, Cov=89.85%\n",
      "New Best! a=1.98, b=2.18, Score=305,390.94, Cov=89.91%\n",
      "New Best! a=1.99, b=2.17, Score=305,390.84, Cov=89.93%\n",
      "New Best! a=1.99, b=2.18, Score=305,384.56, Cov=89.98%\n",
      "\n",
      "Grid search complete. Final OOF Score: 305,384.56. Best multipliers: a=1.99, b=2.18\n",
      "\n",
      "Creating final submission file...\n",
      "\n",
      "'submission_ultimate_synthesis_final.csv' created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pi_lower</th>\n",
       "      <th>pi_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>822391.902314</td>\n",
       "      <td>1.038016e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>603598.815254</td>\n",
       "      <td>8.239731e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>442467.346729</td>\n",
       "      <td>6.716598e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>279592.376924</td>\n",
       "      <td>4.055703e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>344854.317754</td>\n",
       "      <td>7.795774e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       pi_lower      pi_upper\n",
       "0  200000  822391.902314  1.038016e+06\n",
       "1  200001  603598.815254  8.239731e+05\n",
       "2  200002  442467.346729  6.716598e+05\n",
       "3  200003  279592.376924  4.055703e+05\n",
       "4  200004  344854.317754  7.795774e+05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL ASYMMETRIC CALIBRATION AND SUBMISSION (CORRECTED)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Final Asymmetric Calibration ---\")\n",
    "oof_error_final = np.clip(oof_error_preds, 0, None) # Ensure error is not negative\n",
    "best_a, best_b, best_metric = 2.0, 2.0, float('inf')\n",
    "\n",
    "# This grid search finds the best multipliers based on your OOF predictions\n",
    "for a in np.arange(1.90, 2.31, 0.01):\n",
    "    for b in np.arange(2.10, 2.51, 0.01):\n",
    "        low = oof_mean_preds - oof_error_final * a\n",
    "        high = oof_mean_preds + oof_error_final * b\n",
    "        metric, coverage = winkler_score(y_true, low, high, alpha=COMPETITION_ALPHA, return_coverage=True)\n",
    "        if metric < best_metric:\n",
    "            best_metric = metric\n",
    "            best_a, best_b = a, b\n",
    "            # This print is optional but good for seeing progress\n",
    "            print(f\"New Best! a={best_a:.2f}, b={best_b:.2f}, Score={best_metric:,.2f}, Cov={coverage:.2%}\")\n",
    "\n",
    "print(f\"\\nGrid search complete. Final OOF Score: {best_metric:,.2f}. Best multipliers: a={best_a:.2f}, b={best_b:.2f}\")\n",
    "\n",
    "\n",
    "# --- Create Final Submission ---\n",
    "print(\"\\nCreating final submission file...\")\n",
    "test_error_final = np.clip(test_error_preds, 0, None)\n",
    "final_lower = test_mean_preds - test_error_final * best_a\n",
    "final_upper = test_mean_preds + test_error_final * best_b\n",
    "final_upper = np.maximum(final_lower, final_upper)\n",
    "\n",
    "# --- THE FIX IS HERE ---\n",
    "# Instead of using X_test.index, which was reset, we use the 'test_ids'\n",
    "# variable that we saved in the feature engineering block.\n",
    "test_ids = pd.read_csv('./test.csv', usecols=['id'])['id']\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids, \n",
    "    'pi_lower': final_lower, \n",
    "    'pi_upper': final_upper\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_ultimate_synthesis_final.csv', index=False)\n",
    "print(\"\\n'submission_ultimate_synthesis_final.csv' created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fcfcff-e8ce-4ddd-a536-c9a957b3bee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dbcb61-58dd-4089-96af-81258985b476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b1f4d-300d-4e32-87f2-e28b20397f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaggle Comp)",
   "language": "python",
   "name": "kaggle-comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
